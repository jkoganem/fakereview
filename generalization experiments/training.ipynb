{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d32934f3-3d0e-453a-9567-6fe9ec62a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import random\n",
    "import os\n",
    "import IPython.display as ipd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import trange\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "\n",
    "# SBERT imports \n",
    "from datasets import Dataset, load_dataset\n",
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer, SentenceTransformerTrainingArguments, CrossEncoder, LoggingHandler\n",
    "from sentence_transformers.cross_encoder.evaluation import CEBinaryAccuracyEvaluator, CEBinaryClassificationEvaluator\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9b88192-11d5-48cf-8da3-8636a0778fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = None\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "print(f\"Using {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c024073-4dec-4076-8afc-88b3f471c131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience and saving flags\n",
    "ABRIDGED_RUN = False\n",
    "FREEZE_EMBEDDING = True\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 16 # Number of samples per batch while training our network\n",
    "NUM_EPOCHS = 10 # Number of epochs to train our network\n",
    "WARMUP_PROPORTION = 0.5 # Number of epochs to use as warmup steps\n",
    "\n",
    "if ABRIDGED_RUN == True:\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_EPOCHS = 50\n",
    "\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "TRAINING_DATA_SOURCES = ['wiki', 'grover'] # ['reviews', 'wiki', 'grover']\n",
    "\n",
    "MODEL_NAME = \"SBERT_CrossEncoder_all-mpnet-base-v2_\"\n",
    "\n",
    "# Directories\n",
    "OUTPUT_DIR = Path(f\"training_results/{\"baseline_\" if FREEZE_EMBEDDING else \"\"}{\"_\".join(TRAINING_DATA_SOURCES)}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}\") # Checkpoints, models, and statistics will be saved here\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATA_DIR = Path(\"../raw data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5abc9f7d-8ecc-463c-a759-a593868774a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../raw data/combined_data.csv\")\n",
    "data = data[data['Original dataset'] != 'essays'] # Throw out essays data\n",
    "data['Stratify'] = data['Label'] + \" \" + data['Original dataset']\n",
    "\n",
    "if ABRIDGED_RUN == True:\n",
    "    data = data.sample(320)\n",
    "\n",
    "# Set integer labels\n",
    "# Human = 0, 1 = Machine\n",
    "data['Label'] = data['Label'].apply(lambda x: 0 if x == 'Human' else 1)\n",
    "\n",
    "# Train test split, stratified by binary labels\n",
    "data_train, data_test = train_test_split(data, test_size = 0.2, stratify=data['Stratify'], random_state = 406)\n",
    "train_tt, train_vv = train_test_split(data_train, test_size = 0.2, stratify=data_train['Stratify'], random_state = 406)\n",
    "train_tt = train_tt[train_tt['Original dataset'].isin(TRAINING_DATA_SOURCES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e516e3ae-95a4-40c0-95df-641dcacf4c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict({\n",
    "    \"text\": train_tt['Text'],\n",
    "    \"label\": train_tt['Label'],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c12ae7c4-f22d-446e-b75a-87ead3d78af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c35151b2-955a-49b3-b5b8-d9119e1e7faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from https://github.com/UKPLab/sentence-transformers/blob/7290448809cb73f08f63c955550815775434beb4/sentence_transformers/cross_encoder/CrossEncoder.py#L143\n",
    "def smart_batching_collate(batch):\n",
    "        texts = []\n",
    "        labels = []\n",
    "\n",
    "        for example in batch:\n",
    "            texts.append(example['text'])\n",
    "            labels.append(example['label'])\n",
    "\n",
    "        tokenized = tokenizer(\n",
    "            texts, padding=True, truncation=\"longest_first\", return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = torch.tensor(labels, dtype=torch.long).to(DEVICE)\n",
    "\n",
    "        for name in tokenized:\n",
    "            tokenized[name] = tokenized[name].to(DEVICE)\n",
    "\n",
    "        return tokenized, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ae2ec68-f244-4049-b46d-706a154aa69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader.collate_fn = smart_batching_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d148741-73fe-468e-bb79-3cd45bcf67e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-mpnet-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\",\n",
    "                                            num_labels = 1).to(DEVICE)\n",
    "if FREEZE_EMBEDDING:\n",
    "    for p in model.mpnet.parameters():\n",
    "        p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f42ae4ab-e6d0-4926-9b14-62b8a8dfe3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amzi/installs/miniforge3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88966dc9-6e59-4495-88ba-72cd0de28e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.classifier.parameters() if FREEZE_EMBEDDING else model.parameters(), lr = LEARNING_RATE)\n",
    "scheduler = SentenceTransformer._get_scheduler(optimizer, scheduler = 'WarmupLinear', warmup_steps = WARMUP_PROPORTION * len(train_dataloader), t_total = len(train_dataloader)*NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cada7fc5-11e1-4b5f-ba43-5096423be314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_accuracy(model, tokenizer, validation_samples):\n",
    "    strata = ['reviews', 'wiki', 'grover']\n",
    "    accuracies = {strat: 0 for strat in strata}\n",
    "    for strat in strata:\n",
    "        strat_validation_samples = validation_samples[validation_samples['Original dataset'] == strat]\n",
    "        for index, sample in strat_validation_samples.iterrows():\n",
    "            features = tokenizer(sample['Text'], padding=True, truncation=\"longest_first\", return_tensors=\"pt\")\n",
    "            for key in features:\n",
    "                features[key] = features[key].to(DEVICE)\n",
    "            logits = model(**features, return_dict = True).logits\n",
    "            prob = torch.nn.Sigmoid()(logits).item()\n",
    "            label = sample['Label']\n",
    "            accuracies[strat] += int(round(prob) == label)\n",
    "    for strat in strata:\n",
    "        accuracies[strat] = accuracies[strat]/(validation_samples['Original dataset'] == strat).sum()\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73b91a55-3f94-4e6e-a112-035da16b4d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01bfc0e50d443df86d883486791af68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(OUTPUT_DIR/'accuracies.csv', 'w') as f:\n",
    "    f.write('epoch, grover, wiki, reviews\\n')\n",
    "    with torch.no_grad():\n",
    "        acc = validation_accuracy(model, tokenizer, train_vv)\n",
    "        f.write(f'-1, {acc['grover']}, {acc['wiki']}, {acc['reviews']}\\n')\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCHS), position = 0):\n",
    "    for features, labels in tqdm(train_dataloader, position = 1, leave = False):\n",
    "        logits = model(**features, return_dict=True).logits\n",
    "        loss = criterion(logits, labels.unsqueeze(1).float())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        acc = validation_accuracy(model, tokenizer, train_vv)\n",
    "    with open(OUTPUT_DIR/'accuracies.csv', 'a') as f:\n",
    "        f.write(f'{epoch}, {acc['grover']}, {acc['wiki']}, {acc['reviews']}\\n')\n",
    "        \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ed23158-b4db-47a0-948a-0abe343bbfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies on held out test set:\n",
      "    grover: 51.10\n",
      "    wiki: 54.87\n",
      "    reviews: 52.95\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    acc = validation_accuracy(model, tokenizer, data_test)\n",
    "with open(OUTPUT_DIR/'accuracies.csv', 'a') as f:\n",
    "    f.write(f'TEST SET, {acc['grover']}, {acc['wiki']}, {acc['reviews']}\\n')\n",
    "print(f\"\"\"Accuracies on held out test set:\n",
    "    grover: {100*acc['grover']:.2f}\n",
    "    wiki: {100*acc['wiki']:.2f}\n",
    "    reviews: {100*acc['reviews']:.2f}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3642d7-fb17-4dea-85e9-7a52316133b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
