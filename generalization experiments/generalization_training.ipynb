{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a38fb737-1858-4dee-82cc-e1f3f04befc6",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd06790f-1535-4aa1-bb00-feb2ebf24026",
   "metadata": {},
   "source": [
    "This notebook fine-tunes an `all-mpnet-base-v2` model to classify human-generated versus AI-generated text. The fine-tuning data comes from three contexts:\n",
    " * `grover`: news articles,\n",
    " * `wiki`: Wikipedia intro paragraphs, and\n",
    " * `reviews`: product reviews.\n",
    "\n",
    "Our dataset consists of 10,000 human-generated and 10,000 AI-generated samples in each context (prior to our train / validation / test split). For citations and more information on our data, see the readme in our `raw data` folder. \n",
    "\n",
    "We can restrict our model to only train on data from a subset of the contexts (see the `TRAIN_DATA_CONTEXTS` variable below). After every epoch, we validate our model on data from *all* contexts, allowing us to test how well it generalizes to unseen contexts. For results and discussion, see the readme in the base folder of the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e739c7-a6d6-4399-ab3d-703bb62325eb",
   "metadata": {},
   "source": [
    "## Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32934f3-3d0e-453a-9567-6fe9ec62a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "\n",
    "# Transformer imports\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc0b26d-ff42-431e-8972-008480c9ab90",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c03484b-1c14-4d00-b184-1f4582367fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which contexts training data should come from\n",
    "TRAINING_DATA_CONTEXTS = ['grover', 'wiki', 'reviews'] # Any nonempty subset of ['grover', 'wiki', 'reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b88192-11d5-48cf-8da3-8636a0778fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training device\n",
    "DEVICE = None\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "print(f\"Using {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594cdb7c-9e98-41bc-a4df-b912807d43db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc flags\n",
    "ABRIDGED_RUN = False # If True, downsamples our training data for quick test runs\n",
    "FREEZE_EMBEDDING = False # If True, we only train the classification head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c024073-4dec-4076-8afc-88b3f471c131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "BATCH_SIZE = 16 \n",
    "NUM_EPOCHS = 10\n",
    "WARMUP_PROPORTION = 0.5 # Number of epochs to use as warmup steps\n",
    "LEARNING_RATE = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8d3a9b-55db-4afc-a45d-01f5f6c1e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "OUTPUT_DIR = Path(f\"training_results/{\"baseline_\" if FREEZE_EMBEDDING else \"\"}{\"_\".join(TRAINING_DATA_CONTEXTS)}_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATA_DIR = Path(\"../raw data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4094ea1a-9e80-4bef-a066-c677e34e8c6f",
   "metadata": {},
   "source": [
    "## Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abc9f7d-8ecc-463c-a759-a593868774a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(DATA_DIR / \"combined_data.csv\")\n",
    "\n",
    "# Throw out essays data\n",
    "filt = (data['Original dataset'] != 'essays')\n",
    "data = data[filt]\n",
    "\n",
    "# Downsample if abridged run\n",
    "if ABRIDGED_RUN:\n",
    "    data = data.sample(320)\n",
    "\n",
    "# Create a column to stratify our train test split by, accounting for both label and context\n",
    "data['Stratify'] = data['Label'] + \" \" + data['Original dataset']\n",
    "\n",
    "# Set integer labels for training: Human -> 0, Machine -> 1\n",
    "data['Label'] = data['Label'].apply(lambda x: 0 if x == 'Human' else 1)\n",
    "\n",
    "# Train test split, stratified by labels and original dataset\n",
    "data_train, data_test = train_test_split(data, test_size = 0.2, stratify=data['Stratify'], random_state = 406)\n",
    "train_tt, train_vv = train_test_split(data_train, test_size = 0.2, stratify=data_train['Stratify'], random_state = 406)\n",
    "\n",
    "# Restrict training set to only the specified contexts\n",
    "filt = (train_tt['Original dataset'].isin(TRAINING_DATA_CONTEXTS))\n",
    "train_tt = train_tt[filt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12ae7c4-f22d-446e-b75a-87ead3d78af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put training data into Pytorch dataset format\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"text\": train_tt['Text'],\n",
    "    \"label\": train_tt['Label'],\n",
    "})\n",
    "\n",
    "# Create dataloader for training\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a647d65f-e8ff-4c22-83d2-8baf4aada084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35151b2-955a-49b3-b5b8-d9119e1e7faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_batching_collate(batch):\n",
    "    \"\"\"Collate a batch of inputs for training.\n",
    "    \n",
    "    Adapted from https://github.com/UKPLab/sentence-transformers/blob/7290448809cb73f08f63c955550815775434beb4/sentence_transformers/cross_encoder/CrossEncoder.py#L143\n",
    "    Returns tokenized texts stacked into a batch, and a tensor with labels for each text.\"\"\"\n",
    "    \n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    for example in batch:\n",
    "        texts.append(example['text'])\n",
    "        labels.append(example['label'])\n",
    "\n",
    "    texts_tokenized = tokenizer(texts, padding=True, truncation=\"longest_first\", return_tensors=\"pt\")\n",
    "    for key in texts_tokenized:\n",
    "        texts_tokenized[key] = texts_tokenized[key].to(DEVICE)\n",
    "\n",
    "    labels = torch.tensor(labels, dtype=torch.long).to(DEVICE)\n",
    "    \n",
    "    return texts_tokenized, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae2ec68-f244-4049-b46d-706a154aa69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader.collate_fn = smart_batching_collate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb041d5e-114f-4072-967b-e8dc9aadc012",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d148741-73fe-468e-bb79-3cd45bcf67e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\", num_labels = 1)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "if FREEZE_EMBEDDING:\n",
    "    for p in model.mpnet.parameters():\n",
    "        p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbcfe6c-4b22-4c51-9027-b92ad47cb02d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88966dc9-6e59-4495-88ba-72cd0de28e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.classifier.parameters() if FREEZE_EMBEDDING else model.parameters(), lr = LEARNING_RATE)\n",
    "scheduler = SentenceTransformer._get_scheduler(optimizer, scheduler = 'WarmupLinear', warmup_steps = WARMUP_PROPORTION * len(train_dataloader), t_total = len(train_dataloader)*NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cada7fc5-11e1-4b5f-ba43-5096423be314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_accuracy(model, tokenizer, validation_samples):\n",
    "    \"\"\"Compute model accuracy stratified over our three data contexts.\n",
    "\n",
    "    Returns dictionary with keys 'grover', 'wiki', and 'reviews' and values\n",
    "    equal to model accuracy on each data context from validation_samples.\"\"\"\n",
    "\n",
    "    contexts = ['grover', 'wiki', 'reviews']\n",
    "    accuracies = {c:0 for c in contexts}\n",
    "    \n",
    "    for context in contexts:\n",
    "        context_validation_samples = validation_samples[validation_samples['Original dataset'] == context]\n",
    "        for index, sample in context_validation_samples.iterrows():\n",
    "            features = tokenizer(sample['Text'], padding=True, truncation=\"longest_first\", return_tensors=\"pt\")\n",
    "            for key in features:\n",
    "                features[key] = features[key].to(DEVICE)\n",
    "            logits = model(**features, return_dict = True).logits\n",
    "            prob = nn.Sigmoid()(logits).item()\n",
    "            label = sample['Label']\n",
    "            accuracies[context] += int(round(prob) == label)\n",
    "            \n",
    "    for context in contexts:\n",
    "        accuracies[context] = accuracies[context]/(validation_samples['Original dataset'] == context).sum()\n",
    "        \n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade51fb5-96db-43c8-9d47-2466dda76eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write accuracies before training\n",
    "with open(OUTPUT_DIR/'accuracies.csv', 'w') as f:\n",
    "    f.write('epoch, grover, wiki, reviews\\n')\n",
    "    with torch.no_grad():\n",
    "        acc = validation_accuracy(model, tokenizer, train_vv)\n",
    "        f.write(f'-1, {acc['grover']}, {acc['wiki']}, {acc['reviews']}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b91a55-3f94-4e6e-a112-035da16b4d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in tqdm(range(NUM_EPOCHS), position = 0):\n",
    "    \n",
    "    for features, labels in tqdm(train_dataloader, position = 1, leave = False):\n",
    "        logits = model(**features, return_dict=True).logits\n",
    "        loss = criterion(logits, labels.unsqueeze(1).float())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Write accuracies for this epoch\n",
    "    with torch.no_grad():\n",
    "        acc = validation_accuracy(model, tokenizer, train_vv)\n",
    "    with open(OUTPUT_DIR/'accuracies.csv', 'a') as f:\n",
    "        f.write(f'{epoch}, {acc['grover']}, {acc['wiki']}, {acc['reviews']}\\n')\n",
    "\n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed23158-b4db-47a0-948a-0abe343bbfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write accuracies on final test set\n",
    "with torch.no_grad():\n",
    "    acc = validation_accuracy(model, tokenizer, data_test)\n",
    "with open(OUTPUT_DIR/'accuracies.csv', 'a') as f:\n",
    "    f.write(f'TEST SET, {acc['grover']}, {acc['wiki']}, {acc['reviews']}\\n')\n",
    "\n",
    "print(f\"\"\"Accuracies on held out test set:\n",
    "    grover: {100*acc['grover']:.2f}\n",
    "    wiki: {100*acc['wiki']:.2f}\n",
    "    reviews: {100*acc['reviews']:.2f}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3642d7-fb17-4dea-85e9-7a52316133b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
