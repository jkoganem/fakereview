{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5ca90f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f25a6a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Original dataset</th>\n",
       "      <th>Row in original dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The idea of graduating high school in three ye...</td>\n",
       "      <td>1</td>\n",
       "      <td>essays</td>\n",
       "      <td>26613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey, I'm so excited to write this essay about ...</td>\n",
       "      <td>1</td>\n",
       "      <td>essays</td>\n",
       "      <td>26326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Introduction\\n\\nSelf-reliance is a concept tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>essays</td>\n",
       "      <td>30579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sure, here's my attempt at writing an essay as...</td>\n",
       "      <td>1</td>\n",
       "      <td>essays</td>\n",
       "      <td>33547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The legalization of marijuana is a highly deba...</td>\n",
       "      <td>1</td>\n",
       "      <td>essays</td>\n",
       "      <td>33768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label Original dataset  \\\n",
       "0  The idea of graduating high school in three ye...      1           essays   \n",
       "1  Hey, I'm so excited to write this essay about ...      1           essays   \n",
       "2  Introduction\\n\\nSelf-reliance is a concept tha...      1           essays   \n",
       "3  Sure, here's my attempt at writing an essay as...      1           essays   \n",
       "4  The legalization of marijuana is a highly deba...      1           essays   \n",
       "\n",
       "   Row in original dataset  \n",
       "0                    26613  \n",
       "1                    26326  \n",
       "2                    30579  \n",
       "3                    33547  \n",
       "4                    33768  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../raw data/combined_data.csv')\n",
    "data['Label'] = data['Label'].apply(lambda x: 0 if x == 'Human' else 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d8883d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = data.sample(n=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cffa203e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X shape: (7000,)\n",
      "Test X shape: (3000,)\n",
      "Train Y shape: (7000,)\n",
      "Test Y shape: (3000,)\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(\n",
    "    sampled_data['Text'], \n",
    "    sampled_data['Label'], \n",
    "    train_size=0.7, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Print the shapes to verify\n",
    "print(f\"Train X shape: {train_X.shape}\")\n",
    "print(f\"Test X shape: {test_X.shape}\")\n",
    "print(f\"Train Y shape: {train_Y.shape}\")\n",
    "print(f\"Test Y shape: {test_Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e39d4577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7278afbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = tokenizer(list(train_X), padding = True, truncation=True)\n",
    "test_tokens = tokenizer(list(test_X), padding = True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afec4909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fa57178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 16501, 4386, 5132, 113, 107, 1632, 1493, 107, 114, 1110, 1126, 8256, 1705, 1104, 1415, 117, 186, 18413, 20910, 14604, 1233, 1610, 2605, 12198, 2285, 23570, 1115, 2077, 1219, 1103, 19605, 16477, 119, 1109, 1148, 13104, 1104, 1142, 1372, 1127, 1276, 1107, 6411, 1118, 2594, 1987, 10223, 1200, 26673, 117, 1105, 1152, 1127, 1549, 1147, 1271, 1118, 152, 1582, 5213, 1233, 1889, 11463, 1107, 5825, 119, 16501, 4386, 5132, 1127, 1103, 2026, 1657, 3551, 1106, 1518, 2647, 1103, 2746, 117, 1114, 1199, 9985, 16584, 1146, 1106, 1160, 2937, 10992, 1105, 2288, 1120, 1166, 1565, 2759, 3543, 1120, 1103, 2342, 119, 1220, 1127, 18532, 1118, 4672, 24387, 24891, 4832, 1105, 1493, 1115, 1127, 1146, 1106, 1160, 1105, 170, 1544, 2759, 1263, 117, 1543, 1172, 1621, 1103, 2026, 1105, 11112, 1657, 3551, 1104, 1147, 1159, 119, 16501, 4386, 5132, 7672, 1113, 1415, 117, 22245, 15435, 13149, 23570, 117, 1105, 1112, 1216, 1132, 4485, 1112, 6631, 4793, 15334, 1107, 1103, 16969, 2094, 5127, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[CLS] Megacheira ( \" great hands \" ) is an extinct class of large, quadrupedal carnivorous dinosaurs that lived during the Cretaceous Period. The first fossils of this group were found in 1881 by Edward Drinker Cope, and they were given their name by Othniel Charles Marsh in 1889. Megacheira were the largest land animals to ever walk the Earth, with some specimens weighing up to two hundred tonnes and standing at over six metres tall at the shoulder. They were characterised by massive forelimbs and hands that were up to two and a half metres long, making them among the largest and strongest land animals of their time. Megacheira fed on large, herbivorous dinosaurs, and as such are regarded as keystone predators in the prehistoric food web. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(train_tokens['input_ids'][0])\n",
    "print(tokenizer.decode(train_tokens['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b80a80f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenData(Dataset):\n",
    "    def __init__(self, train = False):\n",
    "        if train:\n",
    "            self.text_data = train_X\n",
    "            self.tokens = train_tokens\n",
    "            self.labels = list(train_Y)\n",
    "        else:\n",
    "            self.text_data = test_X\n",
    "            self.tokens = test_tokens\n",
    "            self.labels = list(test_Y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        for k, v in self.tokens.items():\n",
    "            sample[k] = torch.tensor(v[idx])\n",
    "        sample['labels'] = torch.tensor(self.labels[idx])\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab6ba20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_dataset = TokenData(train = True)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_dataset = TokenData(train = False)\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36f72ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-cased') # Pre-trained model\n",
    "optimizer = AdamW(bert_model.parameters(), lr=1e-5) # Optimization function\n",
    "loss_fn = torch.nn.CrossEntropyLoss() # Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e2f2256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "bert_model.to(device) # Transfer model to GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e823c785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "Training batch 1 last loss: 0.18734529614448547\n",
      "Training batch 2 last loss: 0.1866150051355362\n",
      "Training batch 3 last loss: 0.14533594250679016\n",
      "Training batch 4 last loss: 0.147320955991745\n",
      "Training batch 5 last loss: 0.18233612179756165\n",
      "Training batch 6 last loss: 0.187950000166893\n",
      "Training batch 7 last loss: 0.15937501192092896\n",
      "Training batch 8 last loss: 0.17763705551624298\n",
      "Training batch 9 last loss: 0.16404762864112854\n",
      "Training batch 10 last loss: 0.16660204529762268\n",
      "Training batch 11 last loss: 0.1807909607887268\n",
      "Training batch 12 last loss: 0.19744455814361572\n",
      "Training batch 13 last loss: 0.15474480390548706\n",
      "Training batch 14 last loss: 0.16219180822372437\n",
      "Training batch 15 last loss: 0.13685107231140137\n",
      "Training batch 16 last loss: 0.20292365550994873\n",
      "Training batch 17 last loss: 0.20930646359920502\n",
      "Training batch 18 last loss: 0.16511917114257812\n",
      "Training batch 19 last loss: 0.19898006319999695\n",
      "Training batch 20 last loss: 0.188750222325325\n",
      "Training batch 21 last loss: 0.1598225235939026\n",
      "Training batch 22 last loss: 0.22738347947597504\n",
      "Training batch 23 last loss: 0.16326263546943665\n",
      "Training batch 24 last loss: 0.171452596783638\n",
      "Training batch 25 last loss: 0.21073150634765625\n",
      "Training batch 26 last loss: 0.17922751605510712\n",
      "Training batch 27 last loss: 0.1412876397371292\n",
      "Training batch 28 last loss: 0.2056911289691925\n",
      "Training batch 29 last loss: 0.12844449281692505\n",
      "Training batch 30 last loss: 0.1734333634376526\n",
      "Training batch 31 last loss: 0.16305458545684814\n",
      "Training batch 32 last loss: 0.18185877799987793\n",
      "Training batch 33 last loss: 0.1884135603904724\n",
      "Training batch 34 last loss: 0.19044730067253113\n",
      "Training batch 35 last loss: 0.18023765087127686\n",
      "Training batch 36 last loss: 0.21264046430587769\n",
      "Training batch 37 last loss: 0.20498602092266083\n",
      "Training batch 38 last loss: 0.19099050760269165\n",
      "Training batch 39 last loss: 0.17143914103507996\n",
      "Training batch 40 last loss: 0.1800728291273117\n",
      "Training batch 41 last loss: 0.15945619344711304\n",
      "Training batch 42 last loss: 0.1358063519001007\n",
      "Training batch 43 last loss: 0.19330760836601257\n",
      "Training batch 44 last loss: 0.1557212769985199\n",
      "Training batch 45 last loss: 0.14960181713104248\n",
      "Training batch 46 last loss: 0.16934870183467865\n",
      "Training batch 47 last loss: 0.15330643951892853\n",
      "Training batch 48 last loss: 0.18358148634433746\n",
      "Training batch 49 last loss: 0.14445671439170837\n",
      "Training batch 50 last loss: 0.12543682754039764\n",
      "Training batch 51 last loss: 0.2004956752061844\n",
      "Training batch 52 last loss: 0.17005619406700134\n",
      "Training batch 53 last loss: 0.17062130570411682\n",
      "Training batch 54 last loss: 0.14228050410747528\n",
      "Training batch 55 last loss: 0.19860194623470306\n",
      "Training batch 56 last loss: 0.15778540074825287\n",
      "Training batch 57 last loss: 0.12365046888589859\n",
      "Training batch 58 last loss: 0.1544838696718216\n",
      "Training batch 59 last loss: 0.1731337159872055\n",
      "Training batch 60 last loss: 0.1784602403640747\n",
      "Training batch 61 last loss: 0.17492373287677765\n",
      "Training batch 62 last loss: 0.1442718505859375\n",
      "Training batch 63 last loss: 0.19653597474098206\n",
      "Training batch 64 last loss: 0.15292450785636902\n",
      "Training batch 65 last loss: 0.129949688911438\n",
      "Training batch 66 last loss: 0.13924866914749146\n",
      "Training batch 67 last loss: 0.1289345920085907\n",
      "Training batch 68 last loss: 0.15860868990421295\n",
      "Training batch 69 last loss: 0.1357223391532898\n",
      "Training batch 70 last loss: 0.17688652873039246\n",
      "Training batch 71 last loss: 0.189367413520813\n",
      "Training batch 72 last loss: 0.15712594985961914\n",
      "Training batch 73 last loss: 0.14516964554786682\n",
      "Training batch 74 last loss: 0.10244869440793991\n",
      "Training batch 75 last loss: 0.2014554888010025\n",
      "Training batch 76 last loss: 0.17426292598247528\n",
      "Training batch 77 last loss: 0.15816783905029297\n",
      "Training batch 78 last loss: 0.1766432672739029\n",
      "Training batch 79 last loss: 0.16508331894874573\n",
      "Training batch 80 last loss: 0.13351862132549286\n",
      "Training batch 81 last loss: 0.11393622308969498\n",
      "Training batch 82 last loss: 0.14824458956718445\n",
      "Training batch 83 last loss: 0.14242491126060486\n",
      "Training batch 84 last loss: 0.201189786195755\n",
      "Training batch 85 last loss: 0.15661834180355072\n",
      "Training batch 86 last loss: 0.10804542899131775\n",
      "Training batch 87 last loss: 0.14811024069786072\n",
      "Training batch 88 last loss: 0.17239242792129517\n",
      "Training batch 89 last loss: 0.17270991206169128\n",
      "Training batch 90 last loss: 0.17018523812294006\n",
      "Training batch 91 last loss: 0.14591363072395325\n",
      "Training batch 92 last loss: 0.1810663640499115\n",
      "Training batch 93 last loss: 0.18611717224121094\n",
      "Training batch 94 last loss: 0.13456864655017853\n",
      "Training batch 95 last loss: 0.16844968497753143\n",
      "Training batch 96 last loss: 0.15178121626377106\n",
      "Training batch 97 last loss: 0.15346229076385498\n",
      "Training batch 98 last loss: 0.20864975452423096\n",
      "Training batch 99 last loss: 0.15739215910434723\n",
      "Training batch 100 last loss: 0.1679435670375824\n",
      "Training batch 101 last loss: 0.17023031413555145\n",
      "Training batch 102 last loss: 0.1704189032316208\n",
      "Training batch 103 last loss: 0.1701481193304062\n",
      "Training batch 104 last loss: 0.26472795009613037\n",
      "Training batch 105 last loss: 0.16227543354034424\n",
      "Training batch 106 last loss: 0.17646752297878265\n",
      "Training batch 107 last loss: 0.15195707976818085\n",
      "Training batch 108 last loss: 0.16568049788475037\n",
      "Training batch 109 last loss: 0.15908563137054443\n",
      "Training batch 110 last loss: 0.1669590324163437\n",
      "Training batch 111 last loss: 0.20773842930793762\n",
      "Training batch 112 last loss: 0.1618940681219101\n",
      "Training batch 113 last loss: 0.13848266005516052\n",
      "Training batch 114 last loss: 0.18429221212863922\n",
      "Training batch 115 last loss: 0.15625646710395813\n",
      "Training batch 116 last loss: 0.19091485440731049\n",
      "Training batch 117 last loss: 0.1598435640335083\n",
      "Training batch 118 last loss: 0.1438688337802887\n",
      "Training batch 119 last loss: 0.17419244349002838\n",
      "Training batch 120 last loss: 0.16467136144638062\n",
      "Training batch 121 last loss: 0.16603367030620575\n",
      "Training batch 122 last loss: 0.12309405207633972\n",
      "Training batch 123 last loss: 0.12821072340011597\n",
      "Training batch 124 last loss: 0.1661108136177063\n",
      "Training batch 125 last loss: 0.166702002286911\n",
      "Training batch 126 last loss: 0.13809426128864288\n",
      "Training batch 127 last loss: 0.1792338490486145\n",
      "Training batch 128 last loss: 0.13584111630916595\n",
      "Training batch 129 last loss: 0.19854456186294556\n",
      "Training batch 130 last loss: 0.17507700622081757\n",
      "Training batch 131 last loss: 0.17615118622779846\n",
      "Training batch 132 last loss: 0.12576916813850403\n",
      "Training batch 133 last loss: 0.1387098878622055\n",
      "Training batch 134 last loss: 0.11943859606981277\n",
      "Training batch 135 last loss: 0.12139614671468735\n",
      "Training batch 136 last loss: 0.1374075710773468\n",
      "Training batch 137 last loss: 0.14819933474063873\n",
      "Training batch 138 last loss: 0.1759791374206543\n",
      "Training batch 139 last loss: 0.14529159665107727\n",
      "Training batch 140 last loss: 0.16984133422374725\n",
      "Training batch 141 last loss: 0.1472228765487671\n",
      "Training batch 142 last loss: 0.1550692319869995\n",
      "Training batch 143 last loss: 0.1433825045824051\n",
      "Training batch 144 last loss: 0.14507684111595154\n",
      "Training batch 145 last loss: 0.1229841411113739\n",
      "Training batch 146 last loss: 0.17272323369979858\n",
      "Training batch 147 last loss: 0.17204701900482178\n",
      "Training batch 148 last loss: 0.07400694489479065\n",
      "Training batch 149 last loss: 0.13257402181625366\n",
      "Training batch 150 last loss: 0.1537734568119049\n",
      "Training batch 151 last loss: 0.2521778345108032\n",
      "Training batch 152 last loss: 0.15987573564052582\n",
      "Training batch 153 last loss: 0.10298183560371399\n",
      "Training batch 154 last loss: 0.14918935298919678\n",
      "Training batch 155 last loss: 0.2064511775970459\n",
      "Training batch 156 last loss: 0.24751906096935272\n",
      "Training batch 157 last loss: 0.17318028211593628\n",
      "Training batch 158 last loss: 0.12676765024662018\n",
      "Training batch 159 last loss: 0.1542852222919464\n",
      "Training batch 160 last loss: 0.20012491941452026\n",
      "Training batch 161 last loss: 0.11178935319185257\n",
      "Training batch 162 last loss: 0.17883723974227905\n",
      "Training batch 163 last loss: 0.12349183112382889\n",
      "Training batch 164 last loss: 0.24919915199279785\n",
      "Training batch 165 last loss: 0.13366064429283142\n",
      "Training batch 166 last loss: 0.10463292896747589\n",
      "Training batch 167 last loss: 0.17827944457530975\n",
      "Training batch 168 last loss: 0.11165306717157364\n",
      "Training batch 169 last loss: 0.13394565880298615\n",
      "Training batch 170 last loss: 0.18936967849731445\n",
      "Training batch 171 last loss: 0.08146049082279205\n",
      "Training batch 172 last loss: 0.18825595080852509\n",
      "Training batch 173 last loss: 0.2277194857597351\n",
      "Training batch 174 last loss: 0.12190990149974823\n",
      "Training batch 175 last loss: 0.10902754962444305\n",
      "Training batch 176 last loss: 0.1182541474699974\n",
      "Training batch 177 last loss: 0.10166043788194656\n",
      "Training batch 178 last loss: 0.17483748495578766\n",
      "Training batch 179 last loss: 0.1175415962934494\n",
      "Training batch 180 last loss: 0.11201745271682739\n",
      "Training batch 181 last loss: 0.12029942870140076\n",
      "Training batch 182 last loss: 0.16797620058059692\n",
      "Training batch 183 last loss: 0.14879313111305237\n",
      "Training batch 184 last loss: 0.09638290107250214\n",
      "Training batch 185 last loss: 0.1509585976600647\n",
      "Training batch 186 last loss: 0.1765827089548111\n",
      "Training batch 187 last loss: 0.1433488428592682\n",
      "Training batch 188 last loss: 0.1130182147026062\n",
      "Training batch 189 last loss: 0.1486721783876419\n",
      "Training batch 190 last loss: 0.1603037714958191\n",
      "Training batch 191 last loss: 0.11934417486190796\n",
      "Training batch 192 last loss: 0.1726715862751007\n",
      "Training batch 193 last loss: 0.1316179484128952\n",
      "Training batch 194 last loss: 0.18463683128356934\n",
      "Training batch 195 last loss: 0.16546490788459778\n",
      "Training batch 196 last loss: 0.1493581384420395\n",
      "Training batch 197 last loss: 0.13110089302062988\n",
      "Training batch 198 last loss: 0.15076199173927307\n",
      "Training batch 199 last loss: 0.09394953399896622\n",
      "Training batch 200 last loss: 0.13265755772590637\n",
      "Training batch 201 last loss: 0.14241504669189453\n",
      "Training batch 202 last loss: 0.07306547462940216\n",
      "Training batch 203 last loss: 0.11445313692092896\n",
      "Training batch 204 last loss: 0.0883968323469162\n",
      "Training batch 205 last loss: 0.1300671100616455\n",
      "Training batch 206 last loss: 0.13640142977237701\n",
      "Training batch 207 last loss: 0.20389360189437866\n",
      "Training batch 208 last loss: 0.08474283665418625\n",
      "Training batch 209 last loss: 0.16073434054851532\n",
      "Training batch 210 last loss: 0.07451867312192917\n",
      "Training batch 211 last loss: 0.18170256912708282\n",
      "Training batch 212 last loss: 0.1185804083943367\n",
      "Training batch 213 last loss: 0.14841598272323608\n",
      "Training batch 214 last loss: 0.09369811415672302\n",
      "Training batch 215 last loss: 0.3014465570449829\n",
      "Training batch 216 last loss: 0.04987826198339462\n",
      "Training batch 217 last loss: 0.19612570106983185\n",
      "Training batch 218 last loss: 0.16706129908561707\n",
      "Training batch 219 last loss: 0.1838030219078064\n",
      "Training batch 220 last loss: 0.10247081518173218\n",
      "Training batch 221 last loss: 0.10818116366863251\n",
      "Training batch 222 last loss: 0.1436733901500702\n",
      "Training batch 223 last loss: 0.14036744832992554\n",
      "Training batch 224 last loss: 0.0988961011171341\n",
      "Training batch 225 last loss: 0.16118472814559937\n",
      "Training batch 226 last loss: 0.2444559633731842\n",
      "Training batch 227 last loss: 0.1803370714187622\n",
      "Training batch 228 last loss: 0.2028164565563202\n",
      "Training batch 229 last loss: 0.11625482141971588\n",
      "Training batch 230 last loss: 0.1821458339691162\n",
      "Training batch 231 last loss: 0.08766066282987595\n",
      "Training batch 232 last loss: 0.22826001048088074\n",
      "Training batch 233 last loss: 0.24261826276779175\n",
      "Training batch 234 last loss: 0.06242886930704117\n",
      "Training batch 235 last loss: 0.19415009021759033\n",
      "Training batch 236 last loss: 0.2786788046360016\n",
      "Training batch 237 last loss: 0.15156777203083038\n",
      "Training batch 238 last loss: 0.159253790974617\n",
      "Training batch 239 last loss: 0.12230820208787918\n",
      "Training batch 240 last loss: 0.14196693897247314\n",
      "Training batch 241 last loss: 0.12852446734905243\n",
      "Training batch 242 last loss: 0.07848070561885834\n",
      "Training batch 243 last loss: 0.1991550624370575\n",
      "Training batch 244 last loss: 0.1329246461391449\n",
      "Training batch 245 last loss: 0.19992342591285706\n",
      "Training batch 246 last loss: 0.13840869069099426\n",
      "Training batch 247 last loss: 0.15464162826538086\n",
      "Training batch 248 last loss: 0.13139167428016663\n",
      "Training batch 249 last loss: 0.18173900246620178\n",
      "Training batch 250 last loss: 0.1986287236213684\n",
      "Training batch 251 last loss: 0.14091141521930695\n",
      "Training batch 252 last loss: 0.11437392979860306\n",
      "Training batch 253 last loss: 0.14830127358436584\n",
      "Training batch 254 last loss: 0.18782076239585876\n",
      "Training batch 255 last loss: 0.11105968803167343\n",
      "Training batch 256 last loss: 0.17732079327106476\n",
      "Training batch 257 last loss: 0.19122791290283203\n",
      "Training batch 258 last loss: 0.09838445484638214\n",
      "Training batch 259 last loss: 0.08237376809120178\n",
      "Training batch 260 last loss: 0.09030723571777344\n",
      "Training batch 261 last loss: 0.1001458689570427\n",
      "Training batch 262 last loss: 0.10126960277557373\n",
      "Training batch 263 last loss: 0.09781390428543091\n",
      "Training batch 264 last loss: 0.13115933537483215\n",
      "Training batch 265 last loss: 0.0975370779633522\n",
      "Training batch 266 last loss: 0.106697678565979\n",
      "Training batch 267 last loss: 0.1595204770565033\n",
      "Training batch 268 last loss: 0.20825885236263275\n",
      "Training batch 269 last loss: 0.19695697724819183\n",
      "Training batch 270 last loss: 0.06236723065376282\n",
      "Training batch 271 last loss: 0.08763530850410461\n",
      "Training batch 272 last loss: 0.15305772423744202\n",
      "Training batch 273 last loss: 0.09356740117073059\n",
      "Training batch 274 last loss: 0.09514385461807251\n",
      "Training batch 275 last loss: 0.19950611889362335\n",
      "Training batch 276 last loss: 0.03721547871828079\n",
      "Training batch 277 last loss: 0.11225607246160507\n",
      "Training batch 278 last loss: 0.08492320030927658\n",
      "Training batch 279 last loss: 0.15594100952148438\n",
      "Training batch 280 last loss: 0.1008157730102539\n",
      "Training batch 281 last loss: 0.12882277369499207\n",
      "Training batch 282 last loss: 0.09179943054914474\n",
      "Training batch 283 last loss: 0.21639814972877502\n",
      "Training batch 284 last loss: 0.12140409648418427\n",
      "Training batch 285 last loss: 0.1598953902721405\n",
      "Training batch 286 last loss: 0.07737742364406586\n",
      "Training batch 287 last loss: 0.09157273918390274\n",
      "Training batch 288 last loss: 0.07185392081737518\n",
      "Training batch 289 last loss: 0.1151122897863388\n",
      "Training batch 290 last loss: 0.11768516898155212\n",
      "Training batch 291 last loss: 0.2485891580581665\n",
      "Training batch 292 last loss: 0.2587818503379822\n",
      "Training batch 293 last loss: 0.10292206704616547\n",
      "Training batch 294 last loss: 0.11368890851736069\n",
      "Training batch 295 last loss: 0.10953667759895325\n",
      "Training batch 296 last loss: 0.14256028831005096\n",
      "Training batch 297 last loss: 0.08925595879554749\n",
      "Training batch 298 last loss: 0.23033154010772705\n",
      "Training batch 299 last loss: 0.13130638003349304\n",
      "Training batch 300 last loss: 0.06685356050729752\n",
      "Training batch 301 last loss: 0.1294412463903427\n",
      "Training batch 302 last loss: 0.09165871888399124\n",
      "Training batch 303 last loss: 0.12194506078958511\n",
      "Training batch 304 last loss: 0.10144533216953278\n",
      "Training batch 305 last loss: 0.10849539935588837\n",
      "Training batch 306 last loss: 0.10406161844730377\n",
      "Training batch 307 last loss: 0.21134135127067566\n",
      "Training batch 308 last loss: 0.052140846848487854\n",
      "Training batch 309 last loss: 0.15201278030872345\n",
      "Training batch 310 last loss: 0.0662146732211113\n",
      "Training batch 311 last loss: 0.06675797700881958\n",
      "Training batch 312 last loss: 0.0842394307255745\n",
      "Training batch 313 last loss: 0.09983441233634949\n",
      "Training batch 314 last loss: 0.1490849405527115\n",
      "Training batch 315 last loss: 0.11251764744520187\n",
      "Training batch 316 last loss: 0.07663794606924057\n",
      "Training batch 317 last loss: 0.12334340065717697\n",
      "Training batch 318 last loss: 0.05653238669037819\n",
      "Training batch 319 last loss: 0.10709045082330704\n",
      "Training batch 320 last loss: 0.10558561235666275\n",
      "Training batch 321 last loss: 0.11381811648607254\n",
      "Training batch 322 last loss: 0.08093136548995972\n",
      "Training batch 323 last loss: 0.050619229674339294\n",
      "Training batch 324 last loss: 0.12603212893009186\n",
      "Training batch 325 last loss: 0.1603049635887146\n",
      "Training batch 326 last loss: 0.08558204770088196\n",
      "Training batch 327 last loss: 0.1371990442276001\n",
      "Training batch 328 last loss: 0.07226632535457611\n",
      "Training batch 329 last loss: 0.203322172164917\n",
      "Training batch 330 last loss: 0.04703700169920921\n",
      "Training batch 331 last loss: 0.044116854667663574\n",
      "Training batch 332 last loss: 0.13987377285957336\n",
      "Training batch 333 last loss: 0.1218641847372055\n",
      "Training batch 334 last loss: 0.1392565220594406\n",
      "Training batch 335 last loss: 0.13977637887001038\n",
      "Training batch 336 last loss: 0.036998070776462555\n",
      "Training batch 337 last loss: 0.21237614750862122\n",
      "Training batch 338 last loss: 0.18819019198417664\n",
      "Training batch 339 last loss: 0.11531448364257812\n",
      "Training batch 340 last loss: 0.13289231061935425\n",
      "Training batch 341 last loss: 0.06747014075517654\n",
      "Training batch 342 last loss: 0.1588938683271408\n",
      "Training batch 343 last loss: 0.04284513741731644\n",
      "Training batch 344 last loss: 0.18296046555042267\n",
      "Training batch 345 last loss: 0.19063308835029602\n",
      "Training batch 346 last loss: 0.2627394497394562\n",
      "Training batch 347 last loss: 0.28955185413360596\n",
      "Training batch 348 last loss: 0.19768765568733215\n",
      "Training batch 349 last loss: 0.1628967523574829\n",
      "Training batch 350 last loss: 0.1735735833644867\n",
      "Training batch 351 last loss: 0.1462525725364685\n",
      "Training batch 352 last loss: 0.07218044996261597\n",
      "Training batch 353 last loss: 0.05836723744869232\n",
      "Training batch 354 last loss: 0.12014525383710861\n",
      "Training batch 355 last loss: 0.11722353100776672\n",
      "Training batch 356 last loss: 0.14055347442626953\n",
      "Training batch 357 last loss: 0.13202941417694092\n",
      "Training batch 358 last loss: 0.17797252535820007\n",
      "Training batch 359 last loss: 0.16041795909404755\n",
      "Training batch 360 last loss: 0.12579606473445892\n",
      "Training batch 361 last loss: 0.12894000113010406\n",
      "Training batch 362 last loss: 0.12688229978084564\n",
      "Training batch 363 last loss: 0.09940822422504425\n",
      "Training batch 364 last loss: 0.10425932705402374\n",
      "Training batch 365 last loss: 0.23759962618350983\n",
      "Training batch 366 last loss: 0.12023798376321793\n",
      "Training batch 367 last loss: 0.1079094409942627\n",
      "Training batch 368 last loss: 0.05332709848880768\n",
      "Training batch 369 last loss: 0.16929295659065247\n",
      "Training batch 370 last loss: 0.0696338564157486\n",
      "Training batch 371 last loss: 0.11329953372478485\n",
      "Training batch 372 last loss: 0.0840051919221878\n",
      "Training batch 373 last loss: 0.11108739674091339\n",
      "Training batch 374 last loss: 0.17489118874073029\n",
      "Training batch 375 last loss: 0.1001504436135292\n",
      "Training batch 376 last loss: 0.1407684087753296\n",
      "Training batch 377 last loss: 0.20124498009681702\n",
      "Training batch 378 last loss: 0.0974053293466568\n",
      "Training batch 379 last loss: 0.06048770993947983\n",
      "Training batch 380 last loss: 0.07575268298387527\n",
      "Training batch 381 last loss: 0.11257240921258926\n",
      "Training batch 382 last loss: 0.1093192771077156\n",
      "Training batch 383 last loss: 0.06294077634811401\n",
      "Training batch 384 last loss: 0.10431423783302307\n",
      "Training batch 385 last loss: 0.1373487114906311\n",
      "Training batch 386 last loss: 0.029722288250923157\n",
      "Training batch 387 last loss: 0.09570381045341492\n",
      "Training batch 388 last loss: 0.07030026614665985\n",
      "Training batch 389 last loss: 0.055383242666721344\n",
      "Training batch 390 last loss: 0.09581384062767029\n",
      "Training batch 391 last loss: 0.05739302560687065\n",
      "Training batch 392 last loss: 0.10767075419425964\n",
      "Training batch 393 last loss: 0.09367339313030243\n",
      "Training batch 394 last loss: 0.023380303755402565\n",
      "Training batch 395 last loss: 0.06654910743236542\n",
      "Training batch 396 last loss: 0.06677572429180145\n",
      "Training batch 397 last loss: 0.06334078311920166\n",
      "Training batch 398 last loss: 0.03764175623655319\n",
      "Training batch 399 last loss: 0.06501049548387527\n",
      "Training batch 400 last loss: 0.22411894798278809\n",
      "Training batch 401 last loss: 0.11157764494419098\n",
      "Training batch 402 last loss: 0.0693184956908226\n",
      "Training batch 403 last loss: 0.15351806581020355\n",
      "Training batch 404 last loss: 0.14982691407203674\n",
      "Training batch 405 last loss: 0.05805931240320206\n",
      "Training batch 406 last loss: 0.18407458066940308\n",
      "Training batch 407 last loss: 0.13339531421661377\n",
      "Training batch 408 last loss: 0.08600612729787827\n",
      "Training batch 409 last loss: 0.16145633161067963\n",
      "Training batch 410 last loss: 0.08717525750398636\n",
      "Training batch 411 last loss: 0.11092863976955414\n",
      "Training batch 412 last loss: 0.12029669433832169\n",
      "Training batch 413 last loss: 0.038829319179058075\n",
      "Training batch 414 last loss: 0.16713863611221313\n",
      "Training batch 415 last loss: 0.17363035678863525\n",
      "Training batch 416 last loss: 0.09794490784406662\n",
      "Training batch 417 last loss: 0.05588871240615845\n",
      "Training batch 418 last loss: 0.05684097483754158\n",
      "Training batch 419 last loss: 0.08281160145998001\n",
      "Training batch 420 last loss: 0.058403968811035156\n",
      "Training batch 421 last loss: 0.1546265184879303\n",
      "Training batch 422 last loss: 0.0759703665971756\n",
      "Training batch 423 last loss: 0.07274460047483444\n",
      "Training batch 424 last loss: 0.11963801085948944\n",
      "Training batch 425 last loss: 0.05035800486803055\n",
      "Training batch 426 last loss: 0.08523347228765488\n",
      "Training batch 427 last loss: 0.05503322556614876\n",
      "Training batch 428 last loss: 0.02067205309867859\n",
      "Training batch 429 last loss: 0.11448564380407333\n",
      "Training batch 430 last loss: 0.05691248178482056\n",
      "Training batch 431 last loss: 0.058044932782649994\n",
      "Training batch 432 last loss: 0.12110938131809235\n",
      "Training batch 433 last loss: 0.03172270581126213\n",
      "Training batch 434 last loss: 0.16762393712997437\n",
      "Training batch 435 last loss: 0.19401445984840393\n",
      "Training batch 436 last loss: 0.22702573239803314\n",
      "Training batch 437 last loss: 0.07282526046037674\n",
      "Training batch 438 last loss: 0.1554735153913498\n",
      "Training batch 439 last loss: 0.08267179131507874\n",
      "Training batch 440 last loss: 0.13744474947452545\n",
      "Training batch 441 last loss: 0.1625291407108307\n",
      "Training batch 442 last loss: 0.06034194678068161\n",
      "Training batch 443 last loss: 0.21186751127243042\n",
      "Training batch 444 last loss: 0.07614944875240326\n",
      "Training batch 445 last loss: 0.024704769253730774\n",
      "Training batch 446 last loss: 0.1374289095401764\n",
      "Training batch 447 last loss: 0.08462062478065491\n",
      "Training batch 448 last loss: 0.12483134865760803\n",
      "Training batch 449 last loss: 0.16568322479724884\n",
      "Training batch 450 last loss: 0.05602335184812546\n",
      "Training batch 451 last loss: 0.037176504731178284\n",
      "Training batch 452 last loss: 0.04635472968220711\n",
      "Training batch 453 last loss: 0.07841317355632782\n",
      "Training batch 454 last loss: 0.04245372116565704\n",
      "Training batch 455 last loss: 0.15698440372943878\n",
      "Training batch 456 last loss: 0.1086214929819107\n",
      "Training batch 457 last loss: 0.07110800594091415\n",
      "Training batch 458 last loss: 0.019014857709407806\n",
      "Training batch 459 last loss: 0.045229122042655945\n",
      "Training batch 460 last loss: 0.056557122617959976\n",
      "Training batch 461 last loss: 0.09155012667179108\n",
      "Training batch 462 last loss: 0.04492533951997757\n",
      "Training batch 463 last loss: 0.10885024815797806\n",
      "Training batch 464 last loss: 0.057937994599342346\n",
      "Training batch 465 last loss: 0.16217438876628876\n",
      "Training batch 466 last loss: 0.06792835891246796\n",
      "Training batch 467 last loss: 0.05652233213186264\n",
      "Training batch 468 last loss: 0.16952022910118103\n",
      "Training batch 469 last loss: 0.057347554713487625\n",
      "Training batch 470 last loss: 0.04010389745235443\n",
      "Training batch 471 last loss: 0.05714782327413559\n",
      "Training batch 472 last loss: 0.04416711628437042\n",
      "Training batch 473 last loss: 0.102203868329525\n",
      "Training batch 474 last loss: 0.1433916538953781\n",
      "Training batch 475 last loss: 0.16933691501617432\n",
      "Training batch 476 last loss: 0.09237627685070038\n",
      "Training batch 477 last loss: 0.18121644854545593\n",
      "Training batch 478 last loss: 0.18111152946949005\n",
      "Training batch 479 last loss: 0.12925942242145538\n",
      "Training batch 480 last loss: 0.08412602543830872\n",
      "Training batch 481 last loss: 0.06417372822761536\n",
      "Training batch 482 last loss: 0.052659109234809875\n",
      "Training batch 483 last loss: 0.05771833658218384\n",
      "Training batch 484 last loss: 0.11149609088897705\n",
      "Training batch 485 last loss: 0.028580550104379654\n",
      "Training batch 486 last loss: 0.18660755455493927\n",
      "Training batch 487 last loss: 0.04830460250377655\n",
      "Training batch 488 last loss: 0.14042091369628906\n",
      "Training batch 489 last loss: 0.06662946939468384\n",
      "Training batch 490 last loss: 0.20092490315437317\n",
      "Training batch 491 last loss: 0.11082477122545242\n",
      "Training batch 492 last loss: 0.17049452662467957\n",
      "Training batch 493 last loss: 0.022739140316843987\n",
      "Training batch 494 last loss: 0.19341452419757843\n",
      "Training batch 495 last loss: 0.08510731160640717\n",
      "Training batch 496 last loss: 0.12035632878541946\n",
      "Training batch 497 last loss: 0.11350870877504349\n",
      "Training batch 498 last loss: 0.1764528602361679\n",
      "Training batch 499 last loss: 0.16322900354862213\n",
      "Training batch 500 last loss: 0.14013853669166565\n",
      "Training batch 501 last loss: 0.07599005103111267\n",
      "Training batch 502 last loss: 0.040729932487010956\n",
      "Training batch 503 last loss: 0.08189086616039276\n",
      "Training batch 504 last loss: 0.09664509445428848\n",
      "Training batch 505 last loss: 0.14088955521583557\n",
      "Training batch 506 last loss: 0.11787442117929459\n",
      "Training batch 507 last loss: 0.10513467341661453\n",
      "Training batch 508 last loss: 0.11310350149869919\n",
      "Training batch 509 last loss: 0.056875236332416534\n",
      "Training batch 510 last loss: 0.09881249070167542\n",
      "Training batch 511 last loss: 0.14078229665756226\n",
      "Training batch 512 last loss: 0.05587539076805115\n",
      "Training batch 513 last loss: 0.0714375227689743\n",
      "Training batch 514 last loss: 0.024437349289655685\n",
      "Training batch 515 last loss: 0.11999544501304626\n",
      "Training batch 516 last loss: 0.15295380353927612\n",
      "Training batch 517 last loss: 0.07289724797010422\n",
      "Training batch 518 last loss: 0.07495442777872086\n",
      "Training batch 519 last loss: 0.14421898126602173\n",
      "Training batch 520 last loss: 0.08323054015636444\n",
      "Training batch 521 last loss: 0.027199912816286087\n",
      "Training batch 522 last loss: 0.03805464133620262\n",
      "Training batch 523 last loss: 0.025992009788751602\n",
      "Training batch 524 last loss: 0.04423752427101135\n",
      "Training batch 525 last loss: 0.10773687064647675\n",
      "Training batch 526 last loss: 0.10821491479873657\n",
      "Training batch 527 last loss: 0.11662967503070831\n",
      "Training batch 528 last loss: 0.015669092535972595\n",
      "Training batch 529 last loss: 0.1766025573015213\n",
      "Training batch 530 last loss: 0.06501471251249313\n",
      "Training batch 531 last loss: 0.1975887268781662\n",
      "Training batch 532 last loss: 0.08083769679069519\n",
      "Training batch 533 last loss: 0.01797781139612198\n",
      "Training batch 534 last loss: 0.04960507154464722\n",
      "Training batch 535 last loss: 0.04236261174082756\n",
      "Training batch 536 last loss: 0.17741701006889343\n",
      "Training batch 537 last loss: 0.08942477405071259\n",
      "Training batch 538 last loss: 0.05741174519062042\n",
      "Training batch 539 last loss: 0.11820146441459656\n",
      "Training batch 540 last loss: 0.07510652393102646\n",
      "Training batch 541 last loss: 0.1612909585237503\n",
      "Training batch 542 last loss: 0.04772848263382912\n",
      "Training batch 543 last loss: 0.18742655217647552\n",
      "Training batch 544 last loss: 0.12247543036937714\n",
      "Training batch 545 last loss: 0.013725325465202332\n",
      "Training batch 546 last loss: 0.19085606932640076\n",
      "Training batch 547 last loss: 0.010093804448843002\n",
      "Training batch 548 last loss: 0.06758861988782883\n",
      "Training batch 549 last loss: 0.02003766968846321\n",
      "Training batch 550 last loss: 0.04675498977303505\n",
      "Training batch 551 last loss: 0.1102028489112854\n",
      "Training batch 552 last loss: 0.02414729818701744\n",
      "Training batch 553 last loss: 0.08216050267219543\n",
      "Training batch 554 last loss: 0.014000632800161839\n",
      "Training batch 555 last loss: 0.07898502796888351\n",
      "Training batch 556 last loss: 0.1843661218881607\n",
      "Training batch 557 last loss: 0.31362730264663696\n",
      "Training batch 558 last loss: 0.13407552242279053\n",
      "Training batch 559 last loss: 0.10526998341083527\n",
      "Training batch 560 last loss: 0.025566887110471725\n",
      "Training batch 561 last loss: 0.12132283300161362\n",
      "Training batch 562 last loss: 0.059485599398612976\n",
      "Training batch 563 last loss: 0.20043662190437317\n",
      "Training batch 564 last loss: 0.24299725890159607\n",
      "Training batch 565 last loss: 0.05457250028848648\n",
      "Training batch 566 last loss: 0.2031976878643036\n",
      "Training batch 567 last loss: 0.07832807302474976\n",
      "Training batch 568 last loss: 0.13995127379894257\n",
      "Training batch 569 last loss: 0.1627538502216339\n",
      "Training batch 570 last loss: 0.030385930091142654\n",
      "Training batch 571 last loss: 0.10203956067562103\n",
      "Training batch 572 last loss: 0.04627944529056549\n",
      "Training batch 573 last loss: 0.1029883325099945\n",
      "Training batch 574 last loss: 0.06279520690441132\n",
      "Training batch 575 last loss: 0.031433992087841034\n",
      "Training batch 576 last loss: 0.0723414421081543\n",
      "Training batch 577 last loss: 0.04665794223546982\n",
      "Training batch 578 last loss: 0.08064097911119461\n",
      "Training batch 579 last loss: 0.043132416903972626\n",
      "Training batch 580 last loss: 0.021782908588647842\n",
      "Training batch 581 last loss: 0.1752239316701889\n",
      "Training batch 582 last loss: 0.02623915672302246\n",
      "Training batch 583 last loss: 0.06942053139209747\n",
      "Training batch 584 last loss: 0.1257365196943283\n",
      "Training batch 585 last loss: 0.0529121570289135\n",
      "Training batch 586 last loss: 0.27855366468429565\n",
      "Training batch 587 last loss: 0.03880619257688522\n",
      "Training batch 588 last loss: 0.15479686856269836\n",
      "Training batch 589 last loss: 0.062179289758205414\n",
      "Training batch 590 last loss: 0.23582953214645386\n",
      "Training batch 591 last loss: 0.019070381298661232\n",
      "Training batch 592 last loss: 0.012030857615172863\n",
      "Training batch 593 last loss: 0.05266660824418068\n",
      "Training batch 594 last loss: 0.059133145958185196\n",
      "Training batch 595 last loss: 0.10655723512172699\n",
      "Training batch 596 last loss: 0.0939786285161972\n",
      "Training batch 597 last loss: 0.03596005216240883\n",
      "Training batch 598 last loss: 0.10825338959693909\n",
      "Training batch 599 last loss: 0.023028664290905\n",
      "Training batch 600 last loss: 0.08239681273698807\n",
      "Training batch 601 last loss: 0.017154620960354805\n",
      "Training batch 602 last loss: 0.13909469544887543\n",
      "Training batch 603 last loss: 0.2086862027645111\n",
      "Training batch 604 last loss: 0.1280595064163208\n",
      "Training batch 605 last loss: 0.15565966069698334\n",
      "Training batch 606 last loss: 0.1534976065158844\n",
      "Training batch 607 last loss: 0.03844499960541725\n",
      "Training batch 608 last loss: 0.2041098028421402\n",
      "Training batch 609 last loss: 0.22164149582386017\n",
      "Training batch 610 last loss: 0.07872416824102402\n",
      "Training batch 611 last loss: 0.15870419144630432\n",
      "Training batch 612 last loss: 0.10852278769016266\n",
      "Training batch 613 last loss: 0.06671889126300812\n",
      "Training batch 614 last loss: 0.014036297798156738\n",
      "Training batch 615 last loss: 0.04679571092128754\n",
      "Training batch 616 last loss: 0.11305347084999084\n",
      "Training batch 617 last loss: 0.012160982936620712\n",
      "Training batch 618 last loss: 0.06478340923786163\n",
      "Training batch 619 last loss: 0.07651180773973465\n",
      "Training batch 620 last loss: 0.07494958490133286\n",
      "Training batch 621 last loss: 0.05587487667798996\n",
      "Training batch 622 last loss: 0.16338083148002625\n",
      "Training batch 623 last loss: 0.08732616901397705\n",
      "Training batch 624 last loss: 0.0630338042974472\n",
      "Training batch 625 last loss: 0.0207127146422863\n",
      "Training batch 626 last loss: 0.08944177627563477\n",
      "Training batch 627 last loss: 0.05566646531224251\n",
      "Training batch 628 last loss: 0.10216254740953445\n",
      "Training batch 629 last loss: 0.03614701330661774\n",
      "Training batch 630 last loss: 0.20122943818569183\n",
      "Training batch 631 last loss: 0.012751934118568897\n",
      "Training batch 632 last loss: 0.054292310029268265\n",
      "Training batch 633 last loss: 0.05666300281882286\n",
      "Training batch 634 last loss: 0.050852980464696884\n",
      "Training batch 635 last loss: 0.046748626977205276\n",
      "Training batch 636 last loss: 0.24428139626979828\n",
      "Training batch 637 last loss: 0.14070996642112732\n",
      "Training batch 638 last loss: 0.050040923058986664\n",
      "Training batch 639 last loss: 0.12459629774093628\n",
      "Training batch 640 last loss: 0.05648237466812134\n",
      "Training batch 641 last loss: 0.0329236201941967\n",
      "Training batch 642 last loss: 0.049959395080804825\n",
      "Training batch 643 last loss: 0.03500939905643463\n",
      "Training batch 644 last loss: 0.0663604736328125\n",
      "Training batch 645 last loss: 0.011468745768070221\n",
      "Training batch 646 last loss: 0.032674890011548996\n",
      "Training batch 647 last loss: 0.03440021350979805\n",
      "Training batch 648 last loss: 0.1426302194595337\n",
      "Training batch 649 last loss: 0.13652364909648895\n",
      "Training batch 650 last loss: 0.04344538226723671\n",
      "Training batch 651 last loss: 0.13258139789104462\n",
      "Training batch 652 last loss: 0.11100588738918304\n",
      "Training batch 653 last loss: 0.1335265338420868\n",
      "Training batch 654 last loss: 0.08550423383712769\n",
      "Training batch 655 last loss: 0.11012734472751617\n",
      "Training batch 656 last loss: 0.08320099115371704\n",
      "Training batch 657 last loss: 0.1119467094540596\n",
      "Training batch 658 last loss: 0.06149231642484665\n",
      "Training batch 659 last loss: 0.06773292273283005\n",
      "Training batch 660 last loss: 0.04905939847230911\n",
      "Training batch 661 last loss: 0.13162192702293396\n",
      "Training batch 662 last loss: 0.05103188008069992\n",
      "Training batch 663 last loss: 0.09290911257266998\n",
      "Training batch 664 last loss: 0.09211412072181702\n",
      "Training batch 665 last loss: 0.0655650645494461\n",
      "Training batch 666 last loss: 0.07898807525634766\n",
      "Training batch 667 last loss: 0.037917360663414\n",
      "Training batch 668 last loss: 0.08166450262069702\n",
      "Training batch 669 last loss: 0.09542322158813477\n",
      "Training batch 670 last loss: 0.06350255012512207\n",
      "Training batch 671 last loss: 0.015925511717796326\n",
      "Training batch 672 last loss: 0.035647571086883545\n",
      "Training batch 673 last loss: 0.06590819358825684\n",
      "Training batch 674 last loss: 0.03076332062482834\n",
      "Training batch 675 last loss: 0.1897858828306198\n",
      "Training batch 676 last loss: 0.2734261155128479\n",
      "Training batch 677 last loss: 0.07040601968765259\n",
      "Training batch 678 last loss: 0.11957088112831116\n",
      "Training batch 679 last loss: 0.03354310989379883\n",
      "Training batch 680 last loss: 0.1780644804239273\n",
      "Training batch 681 last loss: 0.07320292294025421\n",
      "Training batch 682 last loss: 0.03435007110238075\n",
      "Training batch 683 last loss: 0.15387001633644104\n",
      "Training batch 684 last loss: 0.20879203081130981\n",
      "Training batch 685 last loss: 0.05574288219213486\n",
      "Training batch 686 last loss: 0.14480295777320862\n",
      "Training batch 687 last loss: 0.10278748720884323\n",
      "Training batch 688 last loss: 0.10516970604658127\n",
      "Training batch 689 last loss: 0.03881913051009178\n",
      "Training batch 690 last loss: 0.058898553252220154\n",
      "Training batch 691 last loss: 0.12967148423194885\n",
      "Training batch 692 last loss: 0.02548978477716446\n",
      "Training batch 693 last loss: 0.0850066989660263\n",
      "Training batch 694 last loss: 0.031941451132297516\n",
      "Training batch 695 last loss: 0.10800234973430634\n",
      "Training batch 696 last loss: 0.06570818275213242\n",
      "Training batch 697 last loss: 0.13204213976860046\n",
      "Training batch 698 last loss: 0.0917653813958168\n",
      "Training batch 699 last loss: 0.23516029119491577\n",
      "Training batch 700 last loss: 0.08324837684631348\n",
      "Training batch 701 last loss: 0.04981725662946701\n",
      "Training batch 702 last loss: 0.009142112918198109\n",
      "Training batch 703 last loss: 0.10873106122016907\n",
      "Training batch 704 last loss: 0.04791417717933655\n",
      "Training batch 705 last loss: 0.13179990649223328\n",
      "Training batch 706 last loss: 0.12351884692907333\n",
      "Training batch 707 last loss: 0.2487097829580307\n",
      "Training batch 708 last loss: 0.07106277346611023\n",
      "Training batch 709 last loss: 0.09839817881584167\n",
      "Training batch 710 last loss: 0.09542027860879898\n",
      "Training batch 711 last loss: 0.06908299773931503\n",
      "Training batch 712 last loss: 0.17333155870437622\n",
      "Training batch 713 last loss: 0.13618530333042145\n",
      "Training batch 714 last loss: 0.06934580951929092\n",
      "Training batch 715 last loss: 0.022715827450156212\n",
      "Training batch 716 last loss: 0.02994842641055584\n",
      "Training batch 717 last loss: 0.11581148952245712\n",
      "Training batch 718 last loss: 0.01073683425784111\n",
      "Training batch 719 last loss: 0.1398124098777771\n",
      "Training batch 720 last loss: 0.028406504541635513\n",
      "Training batch 721 last loss: 0.11882230639457703\n",
      "Training batch 722 last loss: 0.14892993867397308\n",
      "Training batch 723 last loss: 0.049437299370765686\n",
      "Training batch 724 last loss: 0.030225854367017746\n",
      "Training batch 725 last loss: 0.12654787302017212\n",
      "Training batch 726 last loss: 0.02774645760655403\n",
      "Training batch 727 last loss: 0.15558165311813354\n",
      "Training batch 728 last loss: 0.06467720866203308\n",
      "Training batch 729 last loss: 0.06350239366292953\n",
      "Training batch 730 last loss: 0.08310683816671371\n",
      "Training batch 731 last loss: 0.010100308805704117\n",
      "Training batch 732 last loss: 0.07329689711332321\n",
      "Training batch 733 last loss: 0.014829268679022789\n",
      "Training batch 734 last loss: 0.05908370018005371\n",
      "Training batch 735 last loss: 0.05740416422486305\n",
      "Training batch 736 last loss: 0.17575618624687195\n",
      "Training batch 737 last loss: 0.3290936052799225\n",
      "Training batch 738 last loss: 0.09481649845838547\n",
      "Training batch 739 last loss: 0.12375827133655548\n",
      "Training batch 740 last loss: 0.2731451094150543\n",
      "Training batch 741 last loss: 0.025590423494577408\n",
      "Training batch 742 last loss: 0.15860795974731445\n",
      "Training batch 743 last loss: 0.02725031226873398\n",
      "Training batch 744 last loss: 0.06088286638259888\n",
      "Training batch 745 last loss: 0.0410752072930336\n",
      "Training batch 746 last loss: 0.03173992410302162\n",
      "Training batch 747 last loss: 0.13383862376213074\n",
      "Training batch 748 last loss: 0.030470391735434532\n",
      "Training batch 749 last loss: 0.059582848101854324\n",
      "Training batch 750 last loss: 0.014583854004740715\n",
      "Training batch 751 last loss: 0.20243239402770996\n",
      "Training batch 752 last loss: 0.021216951310634613\n",
      "Training batch 753 last loss: 0.12204916030168533\n",
      "Training batch 754 last loss: 0.060767244547605515\n",
      "Training batch 755 last loss: 0.08312457799911499\n",
      "Training batch 756 last loss: 0.051360078155994415\n",
      "Training batch 757 last loss: 0.2969890832901001\n",
      "Training batch 758 last loss: 0.04778575152158737\n",
      "Training batch 759 last loss: 0.018783217296004295\n",
      "Training batch 760 last loss: 0.05895574390888214\n",
      "Training batch 761 last loss: 0.0641300231218338\n",
      "Training batch 762 last loss: 0.3748513460159302\n",
      "Training batch 763 last loss: 0.046201616525650024\n",
      "Training batch 764 last loss: 0.09404247254133224\n",
      "Training batch 765 last loss: 0.06397586315870285\n",
      "Training batch 766 last loss: 0.20025181770324707\n",
      "Training batch 767 last loss: 0.17451439797878265\n",
      "Training batch 768 last loss: 0.012332931160926819\n",
      "Training batch 769 last loss: 0.04987255856394768\n",
      "Training batch 770 last loss: 0.11747744679450989\n",
      "Training batch 771 last loss: 0.19591021537780762\n",
      "Training batch 772 last loss: 0.12494874745607376\n",
      "Training batch 773 last loss: 0.05551695078611374\n",
      "Training batch 774 last loss: 0.06753893196582794\n",
      "Training batch 775 last loss: 0.039925623685121536\n",
      "Training batch 776 last loss: 0.068553626537323\n",
      "Training batch 777 last loss: 0.10389238595962524\n",
      "Training batch 778 last loss: 0.08690106123685837\n",
      "Training batch 779 last loss: 0.01869525946676731\n",
      "Training batch 780 last loss: 0.050922367721796036\n",
      "Training batch 781 last loss: 0.0858476310968399\n",
      "Training batch 782 last loss: 0.10608554631471634\n",
      "Training batch 783 last loss: 0.11061643064022064\n",
      "Training batch 784 last loss: 0.10929840058088303\n",
      "Training batch 785 last loss: 0.10091812908649445\n",
      "Training batch 786 last loss: 0.14672596752643585\n",
      "Training batch 787 last loss: 0.053099073469638824\n",
      "Training batch 788 last loss: 0.054896287620067596\n",
      "Training batch 789 last loss: 0.10666590929031372\n",
      "Training batch 790 last loss: 0.06966103613376617\n",
      "Training batch 791 last loss: 0.05960273742675781\n",
      "Training batch 792 last loss: 0.04632733017206192\n",
      "Training batch 793 last loss: 0.14057067036628723\n",
      "Training batch 794 last loss: 0.04108055680990219\n",
      "Training batch 795 last loss: 0.011252053081989288\n",
      "Training batch 796 last loss: 0.05095230042934418\n",
      "Training batch 797 last loss: 0.08728610724210739\n",
      "Training batch 798 last loss: 0.09414803981781006\n",
      "Training batch 799 last loss: 0.1318359524011612\n",
      "Training batch 800 last loss: 0.1791796237230301\n",
      "Training batch 801 last loss: 0.16123205423355103\n",
      "Training batch 802 last loss: 0.11008507013320923\n",
      "Training batch 803 last loss: 0.17608928680419922\n",
      "Training batch 804 last loss: 0.2105277180671692\n",
      "Training batch 805 last loss: 0.026078596711158752\n",
      "Training batch 806 last loss: 0.07026564329862595\n",
      "Training batch 807 last loss: 0.018774237483739853\n",
      "Training batch 808 last loss: 0.03817097470164299\n",
      "Training batch 809 last loss: 0.012267248705029488\n",
      "Training batch 810 last loss: 0.14224107563495636\n",
      "Training batch 811 last loss: 0.05874844640493393\n",
      "Training batch 812 last loss: 0.26480191946029663\n",
      "Training batch 813 last loss: 0.1720385104417801\n",
      "Training batch 814 last loss: 0.036572542041540146\n",
      "Training batch 815 last loss: 0.05799129605293274\n",
      "Training batch 816 last loss: 0.25890564918518066\n",
      "Training batch 817 last loss: 0.0332433246076107\n",
      "Training batch 818 last loss: 0.060697004199028015\n",
      "Training batch 819 last loss: 0.06709419935941696\n",
      "Training batch 820 last loss: 0.10010512173175812\n",
      "Training batch 821 last loss: 0.05977524816989899\n",
      "Training batch 822 last loss: 0.05463029071688652\n",
      "Training batch 823 last loss: 0.04873514547944069\n",
      "Training batch 824 last loss: 0.12202601879835129\n",
      "Training batch 825 last loss: 0.20132246613502502\n",
      "Training batch 826 last loss: 0.08195877820253372\n",
      "Training batch 827 last loss: 0.059731464833021164\n",
      "Training batch 828 last loss: 0.08245903253555298\n",
      "Training batch 829 last loss: 0.09118863940238953\n",
      "Training batch 830 last loss: 0.06268877536058426\n",
      "Training batch 831 last loss: 0.131705179810524\n",
      "Training batch 832 last loss: 0.3347143232822418\n",
      "Training batch 833 last loss: 0.1188979372382164\n",
      "Training batch 834 last loss: 0.09054849296808243\n",
      "Training batch 835 last loss: 0.03961138799786568\n",
      "Training batch 836 last loss: 0.047773294150829315\n",
      "Training batch 837 last loss: 0.09881139546632767\n",
      "Training batch 838 last loss: 0.04550702124834061\n",
      "Training batch 839 last loss: 0.16823358833789825\n",
      "Training batch 840 last loss: 0.09729917347431183\n",
      "Training batch 841 last loss: 0.0054850406013429165\n",
      "Training batch 842 last loss: 0.08370287716388702\n",
      "Training batch 843 last loss: 0.08393476158380508\n",
      "Training batch 844 last loss: 0.04920777305960655\n",
      "Training batch 845 last loss: 0.17034564912319183\n",
      "Training batch 846 last loss: 0.020630231127142906\n",
      "Training batch 847 last loss: 0.1799737513065338\n",
      "Training batch 848 last loss: 0.07680783420801163\n",
      "Training batch 849 last loss: 0.039321716874837875\n",
      "Training batch 850 last loss: 0.0855896919965744\n",
      "Training batch 851 last loss: 0.09815999120473862\n",
      "Training batch 852 last loss: 0.05886591225862503\n",
      "Training batch 853 last loss: 0.01170941349118948\n",
      "Training batch 854 last loss: 0.09307260811328888\n",
      "Training batch 855 last loss: 0.10665824264287949\n",
      "Training batch 856 last loss: 0.013076981529593468\n",
      "Training batch 857 last loss: 0.028443962335586548\n",
      "Training batch 858 last loss: 0.3400202989578247\n",
      "Training batch 859 last loss: 0.21237583458423615\n",
      "Training batch 860 last loss: 0.008866528049111366\n",
      "Training batch 861 last loss: 0.08052771538496017\n",
      "Training batch 862 last loss: 0.0153407147154212\n",
      "Training batch 863 last loss: 0.16509945690631866\n",
      "Training batch 864 last loss: 0.10639527440071106\n",
      "Training batch 865 last loss: 0.038398049771785736\n",
      "Training batch 866 last loss: 0.046974726021289825\n",
      "Training batch 867 last loss: 0.08384519815444946\n",
      "Training batch 868 last loss: 0.023970000445842743\n",
      "Training batch 869 last loss: 0.06450466066598892\n",
      "Training batch 870 last loss: 0.016296962276101112\n",
      "Training batch 871 last loss: 0.12110017240047455\n",
      "Training batch 872 last loss: 0.04226941987872124\n",
      "Training batch 873 last loss: 0.14489799737930298\n",
      "Training batch 874 last loss: 0.08687019348144531\n",
      "Training batch 875 last loss: 0.008347079157829285\n",
      "Training batch 876 last loss: 0.034772537648677826\n",
      "Training batch 877 last loss: 0.09608378261327744\n",
      "Training batch 878 last loss: 0.32707756757736206\n",
      "Training batch 879 last loss: 0.040158264338970184\n",
      "Training batch 880 last loss: 0.019650302827358246\n",
      "Training batch 881 last loss: 0.039316680282354355\n",
      "Training batch 882 last loss: 0.06359585374593735\n",
      "Training batch 883 last loss: 0.006607397925108671\n",
      "Training batch 884 last loss: 0.02491704560816288\n",
      "Training batch 885 last loss: 0.055345162749290466\n",
      "Training batch 886 last loss: 0.1802626997232437\n",
      "Training batch 887 last loss: 0.01132740918546915\n",
      "Training batch 888 last loss: 0.023801114410161972\n",
      "Training batch 889 last loss: 0.02106873132288456\n",
      "Training batch 890 last loss: 0.07126034051179886\n",
      "Training batch 891 last loss: 0.03052535280585289\n",
      "Training batch 892 last loss: 0.033613890409469604\n",
      "Training batch 893 last loss: 0.06854237616062164\n",
      "Training batch 894 last loss: 0.02145148627460003\n",
      "Training batch 895 last loss: 0.027462154626846313\n",
      "Training batch 896 last loss: 0.0453658252954483\n",
      "Training batch 897 last loss: 0.19789201021194458\n",
      "Training batch 898 last loss: 0.01030814554542303\n",
      "Training batch 899 last loss: 0.04284106194972992\n",
      "Training batch 900 last loss: 0.009670982137322426\n",
      "Training batch 901 last loss: 0.15338368713855743\n",
      "Training batch 902 last loss: 0.13145655393600464\n",
      "Training batch 903 last loss: 0.0338834710419178\n",
      "Training batch 904 last loss: 0.13441036641597748\n",
      "Training batch 905 last loss: 0.006424190476536751\n",
      "Training batch 906 last loss: 0.08655698597431183\n",
      "Training batch 907 last loss: 0.1946423053741455\n",
      "Training batch 908 last loss: 0.03261096403002739\n",
      "Training batch 909 last loss: 0.018599970266222954\n",
      "Training batch 910 last loss: 0.08096654713153839\n",
      "Training batch 911 last loss: 0.16938979923725128\n",
      "Training batch 912 last loss: 0.1344483196735382\n",
      "Training batch 913 last loss: 0.16113980114459991\n",
      "Training batch 914 last loss: 0.13214315474033356\n",
      "Training batch 915 last loss: 0.16319169104099274\n",
      "Training batch 916 last loss: 0.1887241005897522\n",
      "Training batch 917 last loss: 0.09866963326931\n",
      "Training batch 918 last loss: 0.008702296763658524\n",
      "Training batch 919 last loss: 0.022105829790234566\n",
      "Training batch 920 last loss: 0.040554773062467575\n",
      "Training batch 921 last loss: 0.0507664680480957\n",
      "Training batch 922 last loss: 0.14323930442333221\n",
      "Training batch 923 last loss: 0.07900959253311157\n",
      "Training batch 924 last loss: 0.14424565434455872\n",
      "Training batch 925 last loss: 0.07580897957086563\n",
      "Training batch 926 last loss: 0.016872452571988106\n",
      "Training batch 927 last loss: 0.104696124792099\n",
      "Training batch 928 last loss: 0.06000639870762825\n",
      "Training batch 929 last loss: 0.11339278519153595\n",
      "Training batch 930 last loss: 0.1190916895866394\n",
      "Training batch 931 last loss: 0.04978855699300766\n",
      "Training batch 932 last loss: 0.009089944884181023\n",
      "Training batch 933 last loss: 0.21407586336135864\n",
      "Training batch 934 last loss: 0.025498375296592712\n",
      "Training batch 935 last loss: 0.047907937318086624\n",
      "Training batch 936 last loss: 0.19733326137065887\n",
      "Training batch 937 last loss: 0.01115052867680788\n",
      "Training batch 938 last loss: 0.0774112343788147\n",
      "Training batch 939 last loss: 0.06460428982973099\n",
      "Training batch 940 last loss: 0.009614063426852226\n",
      "Training batch 941 last loss: 0.11956624686717987\n",
      "Training batch 942 last loss: 0.009644362144172192\n",
      "Training batch 943 last loss: 0.0463603176176548\n",
      "Training batch 944 last loss: 0.12200088798999786\n",
      "Training batch 945 last loss: 0.09927242994308472\n",
      "Training batch 946 last loss: 0.03713482618331909\n",
      "Training batch 947 last loss: 0.13175435364246368\n",
      "Training batch 948 last loss: 0.00860320869833231\n",
      "Training batch 949 last loss: 0.01579222083091736\n",
      "Training batch 950 last loss: 0.03475124388933182\n",
      "Training batch 951 last loss: 0.069832943379879\n",
      "Training batch 952 last loss: 0.141524538397789\n",
      "Training batch 953 last loss: 0.1684987097978592\n",
      "Training batch 954 last loss: 0.13838882744312286\n",
      "Training batch 955 last loss: 0.09359650313854218\n",
      "Training batch 956 last loss: 0.07514247298240662\n",
      "Training batch 957 last loss: 0.14124774932861328\n",
      "Training batch 958 last loss: 0.04286753758788109\n",
      "Training batch 959 last loss: 0.20132976770401\n",
      "Training batch 960 last loss: 0.09975989907979965\n",
      "Training batch 961 last loss: 0.07037509977817535\n",
      "Training batch 962 last loss: 0.1950381100177765\n",
      "Training batch 963 last loss: 0.06158588081598282\n",
      "Training batch 964 last loss: 0.1655842661857605\n",
      "Training batch 965 last loss: 0.15772730112075806\n",
      "Training batch 966 last loss: 0.250301331281662\n",
      "Training batch 967 last loss: 0.09618759900331497\n",
      "Training batch 968 last loss: 0.1330040991306305\n",
      "Training batch 969 last loss: 0.15203753113746643\n",
      "Training batch 970 last loss: 0.026017263531684875\n",
      "Training batch 971 last loss: 0.24934913218021393\n",
      "Training batch 972 last loss: 0.04116128012537956\n",
      "Training batch 973 last loss: 0.1243094801902771\n",
      "Training batch 974 last loss: 0.037295155227184296\n",
      "Training batch 975 last loss: 0.020403772592544556\n",
      "Training batch 976 last loss: 0.11740405112504959\n",
      "Training batch 977 last loss: 0.027251116931438446\n",
      "Training batch 978 last loss: 0.10385625064373016\n",
      "Training batch 979 last loss: 0.08531394600868225\n",
      "Training batch 980 last loss: 0.06421759724617004\n",
      "Training batch 981 last loss: 0.015789585188031197\n",
      "Training batch 982 last loss: 0.05144811049103737\n",
      "Training batch 983 last loss: 0.16509100794792175\n",
      "Training batch 984 last loss: 0.1301054060459137\n",
      "Training batch 985 last loss: 0.06069394201040268\n",
      "Training batch 986 last loss: 0.0807635486125946\n",
      "Training batch 987 last loss: 0.014134795404970646\n",
      "Training batch 988 last loss: 0.16442309319972992\n",
      "Training batch 989 last loss: 0.07136200368404388\n",
      "Training batch 990 last loss: 0.03750963881611824\n",
      "Training batch 991 last loss: 0.04305541515350342\n",
      "Training batch 992 last loss: 0.08884235471487045\n",
      "Training batch 993 last loss: 0.05294842645525932\n",
      "Training batch 994 last loss: 0.07484494894742966\n",
      "Training batch 995 last loss: 0.08771370351314545\n",
      "Training batch 996 last loss: 0.011930732987821102\n",
      "Training batch 997 last loss: 0.07230360060930252\n",
      "Training batch 998 last loss: 0.07154667377471924\n",
      "Training batch 999 last loss: 0.03550398349761963\n",
      "Training batch 1000 last loss: 0.11006671190261841\n",
      "Training batch 1001 last loss: 0.06759828329086304\n",
      "Training batch 1002 last loss: 0.008106105960905552\n",
      "Training batch 1003 last loss: 0.12604652345180511\n",
      "Training batch 1004 last loss: 0.02424159273505211\n",
      "Training batch 1005 last loss: 0.006028088741004467\n",
      "Training batch 1006 last loss: 0.017503444105386734\n",
      "Training batch 1007 last loss: 0.20212455093860626\n",
      "Training batch 1008 last loss: 0.11508587002754211\n",
      "Training batch 1009 last loss: 0.012328527867794037\n",
      "Training batch 1010 last loss: 0.015557542443275452\n",
      "Training batch 1011 last loss: 0.08340741693973541\n",
      "Training batch 1012 last loss: 0.01899217814207077\n",
      "Training batch 1013 last loss: 0.09535320848226547\n",
      "Training batch 1014 last loss: 0.2437642365694046\n",
      "Training batch 1015 last loss: 0.2502180337905884\n",
      "Training batch 1016 last loss: 0.13075006008148193\n",
      "Training batch 1017 last loss: 0.12614315748214722\n",
      "Training batch 1018 last loss: 0.061816468834877014\n",
      "Training batch 1019 last loss: 0.06371390074491501\n",
      "Training batch 1020 last loss: 0.1187501773238182\n",
      "Training batch 1021 last loss: 0.09674286842346191\n",
      "Training batch 1022 last loss: 0.13195070624351501\n",
      "Training batch 1023 last loss: 0.2900611460208893\n",
      "Training batch 1024 last loss: 0.45518559217453003\n",
      "Training batch 1025 last loss: 0.12894201278686523\n",
      "Training batch 1026 last loss: 0.12362869083881378\n",
      "Training batch 1027 last loss: 0.04091453552246094\n",
      "Training batch 1028 last loss: 0.030229244381189346\n",
      "Training batch 1029 last loss: 0.030662288889288902\n",
      "Training batch 1030 last loss: 0.04985632002353668\n",
      "Training batch 1031 last loss: 0.08087440580129623\n",
      "Training batch 1032 last loss: 0.02041926607489586\n",
      "Training batch 1033 last loss: 0.07038439810276031\n",
      "Training batch 1034 last loss: 0.13045796751976013\n",
      "Training batch 1035 last loss: 0.11121566593647003\n",
      "Training batch 1036 last loss: 0.025968879461288452\n",
      "Training batch 1037 last loss: 0.029944108799099922\n",
      "Training batch 1038 last loss: 0.062003690749406815\n",
      "Training batch 1039 last loss: 0.13974714279174805\n",
      "Training batch 1040 last loss: 0.08118043094873428\n",
      "Training batch 1041 last loss: 0.08772189170122147\n",
      "Training batch 1042 last loss: 0.09119433909654617\n",
      "Training batch 1043 last loss: 0.06605662405490875\n",
      "Training batch 1044 last loss: 0.04030575603246689\n",
      "Training batch 1045 last loss: 0.048552677035331726\n",
      "Training batch 1046 last loss: 0.02465740777552128\n",
      "Training batch 1047 last loss: 0.025545377284288406\n",
      "Training batch 1048 last loss: 0.10806345194578171\n",
      "Training batch 1049 last loss: 0.1310313642024994\n",
      "Training batch 1050 last loss: 0.028302635997533798\n",
      "Training batch 1051 last loss: 0.06523316353559494\n",
      "Training batch 1052 last loss: 0.008065024390816689\n",
      "Training batch 1053 last loss: 0.03569825366139412\n",
      "Training batch 1054 last loss: 0.1926524043083191\n",
      "Training batch 1055 last loss: 0.12822607159614563\n",
      "Training batch 1056 last loss: 0.22353976964950562\n",
      "Training batch 1057 last loss: 0.04179088771343231\n",
      "Training batch 1058 last loss: 0.05921155959367752\n",
      "Training batch 1059 last loss: 0.1358981728553772\n",
      "Training batch 1060 last loss: 0.12909948825836182\n",
      "Training batch 1061 last loss: 0.14560723304748535\n",
      "Training batch 1062 last loss: 0.02462581917643547\n",
      "Training batch 1063 last loss: 0.05411022901535034\n",
      "Training batch 1064 last loss: 0.005211591254919767\n",
      "Training batch 1065 last loss: 0.08668924123048782\n",
      "Training batch 1066 last loss: 0.0073614767752587795\n",
      "Training batch 1067 last loss: 0.05071185529232025\n",
      "Training batch 1068 last loss: 0.06367579102516174\n",
      "Training batch 1069 last loss: 0.02049309015274048\n",
      "Training batch 1070 last loss: 0.10360358655452728\n",
      "Training batch 1071 last loss: 0.046768832951784134\n",
      "Training batch 1072 last loss: 0.01373573113232851\n",
      "Training batch 1073 last loss: 0.036726005375385284\n",
      "Training batch 1074 last loss: 0.008590457029640675\n",
      "Training batch 1075 last loss: 0.034322094172239304\n",
      "Training batch 1076 last loss: 0.03188101947307587\n",
      "Training batch 1077 last loss: 0.028263695538043976\n",
      "Training batch 1078 last loss: 0.06740392744541168\n",
      "Training batch 1079 last loss: 0.04938960820436478\n",
      "Training batch 1080 last loss: 0.008058587089180946\n",
      "Training batch 1081 last loss: 0.10280434787273407\n",
      "Training batch 1082 last loss: 0.05695326626300812\n",
      "Training batch 1083 last loss: 0.13600952923297882\n",
      "Training batch 1084 last loss: 0.07018360495567322\n",
      "Training batch 1085 last loss: 0.07002334296703339\n",
      "Training batch 1086 last loss: 0.27525174617767334\n",
      "Training batch 1087 last loss: 0.16298723220825195\n",
      "Training batch 1088 last loss: 0.027377665042877197\n",
      "Training batch 1089 last loss: 0.008733006194233894\n",
      "Training batch 1090 last loss: 0.012384193018078804\n",
      "Training batch 1091 last loss: 0.12822633981704712\n",
      "Training batch 1092 last loss: 0.023529309779405594\n",
      "Training batch 1093 last loss: 0.07977695763111115\n",
      "Training batch 1094 last loss: 0.06189188361167908\n",
      "Training batch 1095 last loss: 0.17859433591365814\n",
      "Training batch 1096 last loss: 0.13170947134494781\n",
      "Training batch 1097 last loss: 0.0957995131611824\n",
      "Training batch 1098 last loss: 0.06059375777840614\n",
      "Training batch 1099 last loss: 0.04265813156962395\n",
      "Training batch 1100 last loss: 0.04327341914176941\n",
      "Training batch 1101 last loss: 0.10366999357938766\n",
      "Training batch 1102 last loss: 0.059199705719947815\n",
      "Training batch 1103 last loss: 0.05558600276708603\n",
      "Training batch 1104 last loss: 0.05981139466166496\n",
      "Training batch 1105 last loss: 0.09053163230419159\n",
      "Training batch 1106 last loss: 0.19409224390983582\n",
      "Training batch 1107 last loss: 0.27993887662887573\n",
      "Training batch 1108 last loss: 0.03607885539531708\n",
      "Training batch 1109 last loss: 0.3193638324737549\n",
      "Training batch 1110 last loss: 0.08128024637699127\n",
      "Training batch 1111 last loss: 0.040096573531627655\n",
      "Training batch 1112 last loss: 0.09563449770212173\n",
      "Training batch 1113 last loss: 0.0260135717689991\n",
      "Training batch 1114 last loss: 0.14946295320987701\n",
      "Training batch 1115 last loss: 0.07562574744224548\n",
      "Training batch 1116 last loss: 0.039520952850580215\n",
      "Training batch 1117 last loss: 0.31415316462516785\n",
      "Training batch 1118 last loss: 0.08208329975605011\n",
      "Training batch 1119 last loss: 0.1526629626750946\n",
      "Training batch 1120 last loss: 0.08915586769580841\n",
      "Training batch 1121 last loss: 0.07831589132547379\n",
      "Training batch 1122 last loss: 0.01244339533150196\n",
      "Training batch 1123 last loss: 0.026779906824231148\n",
      "Training batch 1124 last loss: 0.06353824585676193\n",
      "Training batch 1125 last loss: 0.05872233211994171\n",
      "Training batch 1126 last loss: 0.025609876960515976\n",
      "Training batch 1127 last loss: 0.014323869720101357\n",
      "Training batch 1128 last loss: 0.04644551873207092\n",
      "Training batch 1129 last loss: 0.12703031301498413\n",
      "Training batch 1130 last loss: 0.010285377502441406\n",
      "Training batch 1131 last loss: 0.06125560402870178\n",
      "Training batch 1132 last loss: 0.03448754921555519\n",
      "Training batch 1133 last loss: 0.1409071683883667\n",
      "Training batch 1134 last loss: 0.049683522433042526\n",
      "Training batch 1135 last loss: 0.037722617387771606\n",
      "Training batch 1136 last loss: 0.037926752120256424\n",
      "Training batch 1137 last loss: 0.012023502960801125\n",
      "Training batch 1138 last loss: 0.015377063304185867\n",
      "Training batch 1139 last loss: 0.25105100870132446\n",
      "Training batch 1140 last loss: 0.0763457641005516\n",
      "Training batch 1141 last loss: 0.13097035884857178\n",
      "Training batch 1142 last loss: 0.015799954533576965\n",
      "Training batch 1143 last loss: 0.014327079057693481\n",
      "Training batch 1144 last loss: 0.06326199322938919\n",
      "Training batch 1145 last loss: 0.056063372641801834\n",
      "Training batch 1146 last loss: 0.060080014169216156\n",
      "Training batch 1147 last loss: 0.07537908107042313\n",
      "Training batch 1148 last loss: 0.054610442370176315\n",
      "Training batch 1149 last loss: 0.011075781658291817\n",
      "Training batch 1150 last loss: 0.03082415461540222\n",
      "Training batch 1151 last loss: 0.023656221106648445\n",
      "Training batch 1152 last loss: 0.03599069267511368\n",
      "Training batch 1153 last loss: 0.10463403165340424\n",
      "Training batch 1154 last loss: 0.019360831007361412\n",
      "Training batch 1155 last loss: 0.033130351454019547\n",
      "Training batch 1156 last loss: 0.013988101854920387\n",
      "Training batch 1157 last loss: 0.03314598649740219\n",
      "Training batch 1158 last loss: 0.03396441787481308\n",
      "Training batch 1159 last loss: 0.052091799676418304\n",
      "Training batch 1160 last loss: 0.29746294021606445\n",
      "Training batch 1161 last loss: 0.10663233697414398\n",
      "Training batch 1162 last loss: 0.08395034074783325\n",
      "Training batch 1163 last loss: 0.2825872302055359\n",
      "Training batch 1164 last loss: 0.11970493942499161\n",
      "Training batch 1165 last loss: 0.026385128498077393\n",
      "Training batch 1166 last loss: 0.16878041625022888\n",
      "Training batch 1167 last loss: 0.04885111004114151\n",
      "Training batch 1168 last loss: 0.10583268105983734\n",
      "Training batch 1169 last loss: 0.11703649163246155\n",
      "Training batch 1170 last loss: 0.1953505426645279\n",
      "Training batch 1171 last loss: 0.011871669441461563\n",
      "Training batch 1172 last loss: 0.010584965348243713\n",
      "Training batch 1173 last loss: 0.032412201166152954\n",
      "Training batch 1174 last loss: 0.01678912155330181\n",
      "Training batch 1175 last loss: 0.12678775191307068\n",
      "Training batch 1176 last loss: 0.042715828865766525\n",
      "Training batch 1177 last loss: 0.04039574787020683\n",
      "Training batch 1178 last loss: 0.043381959199905396\n",
      "Training batch 1179 last loss: 0.15119990706443787\n",
      "Training batch 1180 last loss: 0.007409942336380482\n",
      "Training batch 1181 last loss: 0.2472250908613205\n",
      "Training batch 1182 last loss: 0.030535154044628143\n",
      "Training batch 1183 last loss: 0.049800608307123184\n",
      "Training batch 1184 last loss: 0.06114698573946953\n",
      "Training batch 1185 last loss: 0.02934897318482399\n",
      "Training batch 1186 last loss: 0.11518728733062744\n",
      "Training batch 1187 last loss: 0.0734316036105156\n",
      "Training batch 1188 last loss: 0.11767662316560745\n",
      "Training batch 1189 last loss: 0.017595507204532623\n",
      "Training batch 1190 last loss: 0.11944708973169327\n",
      "Training batch 1191 last loss: 0.07025554776191711\n",
      "Training batch 1192 last loss: 0.04088761657476425\n",
      "Training batch 1193 last loss: 0.08556848019361496\n",
      "Training batch 1194 last loss: 0.056301504373550415\n",
      "Training batch 1195 last loss: 0.20837357640266418\n",
      "Training batch 1196 last loss: 0.02420622669160366\n",
      "Training batch 1197 last loss: 0.027091870084404945\n",
      "Training batch 1198 last loss: 0.027369998395442963\n",
      "Training batch 1199 last loss: 0.12547366321086884\n",
      "Training batch 1200 last loss: 0.25969693064689636\n",
      "Training batch 1201 last loss: 0.16547513008117676\n",
      "Training batch 1202 last loss: 0.008896863088011742\n",
      "Training batch 1203 last loss: 0.010408768430352211\n",
      "Training batch 1204 last loss: 0.08505145460367203\n",
      "Training batch 1205 last loss: 0.1826236993074417\n",
      "Training batch 1206 last loss: 0.13240405917167664\n",
      "Training batch 1207 last loss: 0.00545947439968586\n",
      "Training batch 1208 last loss: 0.14811159670352936\n",
      "Training batch 1209 last loss: 0.006077771075069904\n",
      "Training batch 1210 last loss: 0.08959686756134033\n",
      "Training batch 1211 last loss: 0.1952071487903595\n",
      "Training batch 1212 last loss: 0.1556645929813385\n",
      "Training batch 1213 last loss: 0.12279640138149261\n",
      "Training batch 1214 last loss: 0.006271803751587868\n",
      "Training batch 1215 last loss: 0.0768747329711914\n",
      "Training batch 1216 last loss: 0.04422610253095627\n",
      "Training batch 1217 last loss: 0.26904764771461487\n",
      "Training batch 1218 last loss: 0.1253558248281479\n",
      "Training batch 1219 last loss: 0.03232470154762268\n",
      "Training batch 1220 last loss: 0.014180966652929783\n",
      "Training batch 1221 last loss: 0.05815812945365906\n",
      "Training batch 1222 last loss: 0.0086284838616848\n",
      "Training batch 1223 last loss: 0.016495663672685623\n",
      "Training batch 1224 last loss: 0.044704537838697433\n",
      "Training batch 1225 last loss: 0.33537745475769043\n",
      "Training batch 1226 last loss: 0.12665115296840668\n",
      "Training batch 1227 last loss: 0.05393953621387482\n",
      "Training batch 1228 last loss: 0.011582808569073677\n",
      "Training batch 1229 last loss: 0.025178346782922745\n",
      "Training batch 1230 last loss: 0.016338009387254715\n",
      "Training batch 1231 last loss: 0.014948900789022446\n",
      "Training batch 1232 last loss: 0.027213726192712784\n",
      "Training batch 1233 last loss: 0.19986297190189362\n",
      "Training batch 1234 last loss: 0.05708501487970352\n",
      "Training batch 1235 last loss: 0.17055410146713257\n",
      "Training batch 1236 last loss: 0.12797614932060242\n",
      "Training batch 1237 last loss: 0.06337530165910721\n",
      "Training batch 1238 last loss: 0.03774285316467285\n",
      "Training batch 1239 last loss: 0.006325512193143368\n",
      "Training batch 1240 last loss: 0.04692812263965607\n",
      "Training batch 1241 last loss: 0.10405907034873962\n",
      "Training batch 1242 last loss: 0.052138738334178925\n",
      "Training batch 1243 last loss: 0.11504008620977402\n",
      "Training batch 1244 last loss: 0.12252717465162277\n",
      "Training batch 1245 last loss: 0.07718624174594879\n",
      "Training batch 1246 last loss: 0.012649333104491234\n",
      "Training batch 1247 last loss: 0.056890565901994705\n",
      "Training batch 1248 last loss: 0.10697529464960098\n",
      "Training batch 1249 last loss: 0.00742674432694912\n",
      "Training batch 1250 last loss: 0.00984085351228714\n",
      "Training batch 1251 last loss: 0.06813438981771469\n",
      "Training batch 1252 last loss: 0.1290203034877777\n",
      "Training batch 1253 last loss: 0.03831882029771805\n",
      "Training batch 1254 last loss: 0.04371108114719391\n",
      "Training batch 1255 last loss: 0.09831514954566956\n",
      "Training batch 1256 last loss: 0.1019081175327301\n",
      "Training batch 1257 last loss: 0.029038123786449432\n",
      "Training batch 1258 last loss: 0.11386039853096008\n",
      "Training batch 1259 last loss: 0.21985377371311188\n",
      "Training batch 1260 last loss: 0.04955555498600006\n",
      "Training batch 1261 last loss: 0.028315911069512367\n",
      "Training batch 1262 last loss: 0.022408481687307358\n",
      "Training batch 1263 last loss: 0.12470114976167679\n",
      "Training batch 1264 last loss: 0.035081688314676285\n",
      "Training batch 1265 last loss: 0.12563031911849976\n",
      "Training batch 1266 last loss: 0.22867357730865479\n",
      "Training batch 1267 last loss: 0.13177746534347534\n",
      "Training batch 1268 last loss: 0.022704478353261948\n",
      "Training batch 1269 last loss: 0.05815383046865463\n",
      "Training batch 1270 last loss: 0.0974782258272171\n",
      "Training batch 1271 last loss: 0.034071747213602066\n",
      "Training batch 1272 last loss: 0.01096879132091999\n",
      "Training batch 1273 last loss: 0.0340290367603302\n",
      "Training batch 1274 last loss: 0.040792807936668396\n",
      "Training batch 1275 last loss: 0.07829899340867996\n",
      "Training batch 1276 last loss: 0.07269343733787537\n",
      "Training batch 1277 last loss: 0.11786096543073654\n",
      "Training batch 1278 last loss: 0.020056746900081635\n",
      "Training batch 1279 last loss: 0.009245719760656357\n",
      "Training batch 1280 last loss: 0.12616944313049316\n",
      "Training batch 1281 last loss: 0.14566227793693542\n",
      "Training batch 1282 last loss: 0.0123558659106493\n",
      "Training batch 1283 last loss: 0.013073013164103031\n",
      "Training batch 1284 last loss: 0.004047977272421122\n",
      "Training batch 1285 last loss: 0.08778554201126099\n",
      "Training batch 1286 last loss: 0.03634670004248619\n",
      "Training batch 1287 last loss: 0.012663926929235458\n",
      "Training batch 1288 last loss: 0.006365491077303886\n",
      "Training batch 1289 last loss: 0.08555446565151215\n",
      "Training batch 1290 last loss: 0.040024273097515106\n",
      "Training batch 1291 last loss: 0.073292076587677\n",
      "Training batch 1292 last loss: 0.02463824674487114\n",
      "Training batch 1293 last loss: 0.05433565750718117\n",
      "Training batch 1294 last loss: 0.018408168107271194\n",
      "Training batch 1295 last loss: 0.0460716113448143\n",
      "Training batch 1296 last loss: 0.020514028146862984\n",
      "Training batch 1297 last loss: 0.19855068624019623\n",
      "Training batch 1298 last loss: 0.0639619454741478\n",
      "Training batch 1299 last loss: 0.005195586942136288\n",
      "Training batch 1300 last loss: 0.23229600489139557\n",
      "Training batch 1301 last loss: 0.050029441714286804\n",
      "Training batch 1302 last loss: 0.027927018702030182\n",
      "Training batch 1303 last loss: 0.05280259624123573\n",
      "Training batch 1304 last loss: 0.1008940041065216\n",
      "Training batch 1305 last loss: 0.08086510002613068\n",
      "Training batch 1306 last loss: 0.03177992254495621\n",
      "Training batch 1307 last loss: 0.03624510020017624\n",
      "Training batch 1308 last loss: 0.24838492274284363\n",
      "Training batch 1309 last loss: 0.10823372006416321\n",
      "Training batch 1310 last loss: 0.20350059866905212\n",
      "Training batch 1311 last loss: 0.03452887013554573\n",
      "Training batch 1312 last loss: 0.08067665994167328\n",
      "Training batch 1313 last loss: 0.05478516221046448\n",
      "Training batch 1314 last loss: 0.04619595408439636\n",
      "Training batch 1315 last loss: 0.06084635108709335\n",
      "Training batch 1316 last loss: 0.05318087711930275\n",
      "Training batch 1317 last loss: 0.032487768679857254\n",
      "Training batch 1318 last loss: 0.02651314064860344\n",
      "Training batch 1319 last loss: 0.005457127466797829\n",
      "Training batch 1320 last loss: 0.027451373636722565\n",
      "Training batch 1321 last loss: 0.05050681158900261\n",
      "Training batch 1322 last loss: 0.06931846588850021\n",
      "Training batch 1323 last loss: 0.06034485995769501\n",
      "Training batch 1324 last loss: 0.07377548515796661\n",
      "Training batch 1325 last loss: 0.09235751628875732\n",
      "Training batch 1326 last loss: 0.008346706628799438\n",
      "Training batch 1327 last loss: 0.04489583522081375\n",
      "Training batch 1328 last loss: 0.027045385912060738\n",
      "Training batch 1329 last loss: 0.0037362249568104744\n",
      "Training batch 1330 last loss: 0.05025111138820648\n",
      "Training batch 1331 last loss: 0.15779712796211243\n",
      "Training batch 1332 last loss: 0.07267796993255615\n",
      "Training batch 1333 last loss: 0.05588838458061218\n",
      "Training batch 1334 last loss: 0.09837619960308075\n",
      "Training batch 1335 last loss: 0.004388521425426006\n",
      "Training batch 1336 last loss: 0.009487118571996689\n",
      "Training batch 1337 last loss: 0.014987478032708168\n",
      "Training batch 1338 last loss: 0.004877615720033646\n",
      "Training batch 1339 last loss: 0.138932466506958\n",
      "Training batch 1340 last loss: 0.033665407449007034\n",
      "Training batch 1341 last loss: 0.06399380415678024\n",
      "Training batch 1342 last loss: 0.07845263183116913\n",
      "Training batch 1343 last loss: 0.008969835937023163\n",
      "Training batch 1344 last loss: 0.1605244278907776\n",
      "Training batch 1345 last loss: 0.04494095593690872\n",
      "Training batch 1346 last loss: 0.014128342270851135\n",
      "Training batch 1347 last loss: 0.08305723965167999\n",
      "Training batch 1348 last loss: 0.018545933067798615\n",
      "Training batch 1349 last loss: 0.0843258872628212\n",
      "Training batch 1350 last loss: 0.18480153381824493\n",
      "Training batch 1351 last loss: 0.10676898807287216\n",
      "Training batch 1352 last loss: 0.08349961787462234\n",
      "Training batch 1353 last loss: 0.22875238955020905\n",
      "Training batch 1354 last loss: 0.07409783452749252\n",
      "Training batch 1355 last loss: 0.3697827458381653\n",
      "Training batch 1356 last loss: 0.05136600136756897\n",
      "Training batch 1357 last loss: 0.04596453160047531\n",
      "Training batch 1358 last loss: 0.04757276177406311\n",
      "Training batch 1359 last loss: 0.01988046243786812\n",
      "Training batch 1360 last loss: 0.023265432566404343\n",
      "Training batch 1361 last loss: 0.007439460139721632\n",
      "Training batch 1362 last loss: 0.017523039132356644\n",
      "Training batch 1363 last loss: 0.12442819774150848\n",
      "Training batch 1364 last loss: 0.17816844582557678\n",
      "Training batch 1365 last loss: 0.0368175208568573\n",
      "Training batch 1366 last loss: 0.013003939762711525\n",
      "Training batch 1367 last loss: 0.09338828921318054\n",
      "Training batch 1368 last loss: 0.08084067702293396\n",
      "Training batch 1369 last loss: 0.05038157477974892\n",
      "Training batch 1370 last loss: 0.0785030722618103\n",
      "Training batch 1371 last loss: 0.12593147158622742\n",
      "Training batch 1372 last loss: 0.0951918363571167\n",
      "Training batch 1373 last loss: 0.11181198805570602\n",
      "Training batch 1374 last loss: 0.0838937908411026\n",
      "Training batch 1375 last loss: 0.031512871384620667\n",
      "Training batch 1376 last loss: 0.08325327932834625\n",
      "Training batch 1377 last loss: 0.25012266635894775\n",
      "Training batch 1378 last loss: 0.010838843882083893\n",
      "Training batch 1379 last loss: 0.05644262209534645\n",
      "Training batch 1380 last loss: 0.036779142916202545\n",
      "Training batch 1381 last loss: 0.08947690576314926\n",
      "Training batch 1382 last loss: 0.09325624257326126\n",
      "Training batch 1383 last loss: 0.11012782901525497\n",
      "Training batch 1384 last loss: 0.09564017504453659\n",
      "Training batch 1385 last loss: 0.05007406324148178\n",
      "Training batch 1386 last loss: 0.006981110665947199\n",
      "Training batch 1387 last loss: 0.04150636866688728\n",
      "Training batch 1388 last loss: 0.009350682608783245\n",
      "Training batch 1389 last loss: 0.03674883395433426\n",
      "Training batch 1390 last loss: 0.0611959844827652\n",
      "Training batch 1391 last loss: 0.07920647412538528\n",
      "Training batch 1392 last loss: 0.11676275730133057\n",
      "Training batch 1393 last loss: 0.06090632826089859\n",
      "Training batch 1394 last loss: 0.016411196440458298\n",
      "Training batch 1395 last loss: 0.013345163315534592\n",
      "Training batch 1396 last loss: 0.016064602881669998\n",
      "Training batch 1397 last loss: 0.10824413597583771\n",
      "Training batch 1398 last loss: 0.06124843657016754\n",
      "Training batch 1399 last loss: 0.06947203725576401\n",
      "Training batch 1400 last loss: 0.14224961400032043\n",
      "Training batch 1401 last loss: 0.15108828246593475\n",
      "Training batch 1402 last loss: 0.0966845229268074\n",
      "Training batch 1403 last loss: 0.02415652759373188\n",
      "Training batch 1404 last loss: 0.03887631744146347\n",
      "Training batch 1405 last loss: 0.11119605600833893\n",
      "Training batch 1406 last loss: 0.008322044275701046\n",
      "Training batch 1407 last loss: 0.01821528933942318\n",
      "Training batch 1408 last loss: 0.08170118927955627\n",
      "Training batch 1409 last loss: 0.004507927224040031\n",
      "Training batch 1410 last loss: 0.010596201755106449\n",
      "Training batch 1411 last loss: 0.05591887608170509\n",
      "Training batch 1412 last loss: 0.03818410634994507\n",
      "Training batch 1413 last loss: 0.0028280140832066536\n",
      "Training batch 1414 last loss: 0.032410118728876114\n",
      "Training batch 1415 last loss: 0.017269838601350784\n",
      "Training batch 1416 last loss: 0.0467345230281353\n",
      "Training batch 1417 last loss: 0.017373720183968544\n",
      "Training batch 1418 last loss: 0.2773773670196533\n",
      "Training batch 1419 last loss: 0.01638926938176155\n",
      "Training batch 1420 last loss: 0.011038834229111671\n",
      "Training batch 1421 last loss: 0.037771932780742645\n",
      "Training batch 1422 last loss: 0.07398883253335953\n",
      "Training batch 1423 last loss: 0.09609714150428772\n",
      "Training batch 1424 last loss: 0.05626855790615082\n",
      "Training batch 1425 last loss: 0.025772448629140854\n",
      "Training batch 1426 last loss: 0.0432242825627327\n",
      "Training batch 1427 last loss: 0.07918057590723038\n",
      "Training batch 1428 last loss: 0.034246016293764114\n",
      "Training batch 1429 last loss: 0.08283840864896774\n",
      "Training batch 1430 last loss: 0.09107379615306854\n",
      "Training batch 1431 last loss: 0.002092062495648861\n",
      "Training batch 1432 last loss: 0.0864010825753212\n",
      "Training batch 1433 last loss: 0.06577935814857483\n",
      "Training batch 1434 last loss: 0.03286100551486015\n",
      "Training batch 1435 last loss: 0.14872673153877258\n",
      "Training batch 1436 last loss: 0.056643079966306686\n",
      "Training batch 1437 last loss: 0.012068228796124458\n",
      "Training batch 1438 last loss: 0.10558302700519562\n",
      "Training batch 1439 last loss: 0.09710992127656937\n",
      "Training batch 1440 last loss: 0.03418104350566864\n",
      "Training batch 1441 last loss: 0.06203019991517067\n",
      "Training batch 1442 last loss: 0.048571549355983734\n",
      "Training batch 1443 last loss: 0.050702620297670364\n",
      "Training batch 1444 last loss: 0.018094148486852646\n",
      "Training batch 1445 last loss: 0.07700254023075104\n",
      "Training batch 1446 last loss: 0.18275773525238037\n",
      "Training batch 1447 last loss: 0.12523694336414337\n",
      "Training batch 1448 last loss: 0.008830184116959572\n",
      "Training batch 1449 last loss: 0.024257425218820572\n",
      "Training batch 1450 last loss: 0.25529399514198303\n",
      "Training batch 1451 last loss: 0.031816642731428146\n",
      "Training batch 1452 last loss: 0.47721773386001587\n",
      "Training batch 1453 last loss: 0.010103438049554825\n",
      "Training batch 1454 last loss: 0.007254064083099365\n",
      "Training batch 1455 last loss: 0.06610661745071411\n",
      "Training batch 1456 last loss: 0.1165652871131897\n",
      "Training batch 1457 last loss: 0.2030046582221985\n",
      "Training batch 1458 last loss: 0.07249148935079575\n",
      "Training batch 1459 last loss: 0.16864201426506042\n",
      "Training batch 1460 last loss: 0.03320794180035591\n",
      "Training batch 1461 last loss: 0.03627549111843109\n",
      "Training batch 1462 last loss: 0.07912037521600723\n",
      "Training batch 1463 last loss: 0.06060958653688431\n",
      "Training batch 1464 last loss: 0.1973201483488083\n",
      "Training batch 1465 last loss: 0.053929999470710754\n",
      "Training batch 1466 last loss: 0.02006165310740471\n",
      "Training batch 1467 last loss: 0.0833892673254013\n",
      "Training batch 1468 last loss: 0.03050047531723976\n",
      "Training batch 1469 last loss: 0.026333466172218323\n",
      "Training batch 1470 last loss: 0.058709241449832916\n",
      "Training batch 1471 last loss: 0.08614553511142731\n",
      "Training batch 1472 last loss: 0.1067429855465889\n",
      "Training batch 1473 last loss: 0.024189546704292297\n",
      "Training batch 1474 last loss: 0.06658702343702316\n",
      "Training batch 1475 last loss: 0.06435559689998627\n",
      "Training batch 1476 last loss: 0.06606150418519974\n",
      "Training batch 1477 last loss: 0.02752058207988739\n",
      "Training batch 1478 last loss: 0.03856682404875755\n",
      "Training batch 1479 last loss: 0.08781233429908752\n",
      "Training batch 1480 last loss: 0.02252946048974991\n",
      "Training batch 1481 last loss: 0.040662966668605804\n",
      "Training batch 1482 last loss: 0.04476556181907654\n",
      "Training batch 1483 last loss: 0.04528876394033432\n",
      "Training batch 1484 last loss: 0.04983983561396599\n",
      "Training batch 1485 last loss: 0.03728180378675461\n",
      "Training batch 1486 last loss: 0.02441207319498062\n",
      "Training batch 1487 last loss: 0.2827525734901428\n",
      "Training batch 1488 last loss: 0.06373141705989838\n",
      "Training batch 1489 last loss: 0.15027260780334473\n",
      "Training batch 1490 last loss: 0.0325113944709301\n",
      "Training batch 1491 last loss: 0.07084192335605621\n",
      "Training batch 1492 last loss: 0.00307575985789299\n",
      "Training batch 1493 last loss: 0.23234198987483978\n",
      "Training batch 1494 last loss: 0.1475386768579483\n",
      "Training batch 1495 last loss: 0.0030989772640168667\n",
      "Training batch 1496 last loss: 0.016464106738567352\n",
      "Training batch 1497 last loss: 0.08781641721725464\n",
      "Training batch 1498 last loss: 0.09394945949316025\n",
      "Training batch 1499 last loss: 0.015971258282661438\n",
      "Training batch 1500 last loss: 0.007183092180639505\n",
      "Training batch 1501 last loss: 0.06356947869062424\n",
      "Training batch 1502 last loss: 0.006395394913852215\n",
      "Training batch 1503 last loss: 0.12088119983673096\n",
      "Training batch 1504 last loss: 0.08418584614992142\n",
      "Training batch 1505 last loss: 0.1408921480178833\n",
      "Training batch 1506 last loss: 0.054181501269340515\n",
      "Training batch 1507 last loss: 0.1052452027797699\n",
      "Training batch 1508 last loss: 0.07257027924060822\n",
      "Training batch 1509 last loss: 0.05587232857942581\n",
      "Training batch 1510 last loss: 0.022160178050398827\n",
      "Training batch 1511 last loss: 0.005039151757955551\n",
      "Training batch 1512 last loss: 0.035488348454236984\n",
      "Training batch 1513 last loss: 0.0077090756967663765\n",
      "Training batch 1514 last loss: 0.005654331296682358\n",
      "Training batch 1515 last loss: 0.124719999730587\n",
      "Training batch 1516 last loss: 0.06745056062936783\n",
      "Training batch 1517 last loss: 0.006696983706206083\n",
      "Training batch 1518 last loss: 0.10196012258529663\n",
      "Training batch 1519 last loss: 0.006002805661410093\n",
      "Training batch 1520 last loss: 0.04522441327571869\n",
      "Training batch 1521 last loss: 0.009201768785715103\n",
      "Training batch 1522 last loss: 0.006931249983608723\n",
      "Training batch 1523 last loss: 0.055208951234817505\n",
      "Training batch 1524 last loss: 0.029429469257593155\n",
      "Training batch 1525 last loss: 0.10438080132007599\n",
      "Training batch 1526 last loss: 0.227474644780159\n",
      "Training batch 1527 last loss: 0.14409491419792175\n",
      "Training batch 1528 last loss: 0.27475664019584656\n",
      "Training batch 1529 last loss: 0.06725383549928665\n",
      "Training batch 1530 last loss: 0.016317207366228104\n",
      "Training batch 1531 last loss: 0.19367380440235138\n",
      "Training batch 1532 last loss: 0.032920848578214645\n",
      "Training batch 1533 last loss: 0.2316516637802124\n",
      "Training batch 1534 last loss: 0.09215596318244934\n",
      "Training batch 1535 last loss: 0.1912243366241455\n",
      "Training batch 1536 last loss: 0.1157345250248909\n",
      "Training batch 1537 last loss: 0.09375601261854172\n",
      "Training batch 1538 last loss: 0.08442343026399612\n",
      "Training batch 1539 last loss: 0.11550009250640869\n",
      "Training batch 1540 last loss: 0.05230215936899185\n",
      "Training batch 1541 last loss: 0.02401026338338852\n",
      "Training batch 1542 last loss: 0.09494449198246002\n",
      "Training batch 1543 last loss: 0.053644903004169464\n",
      "Training batch 1544 last loss: 0.11039821058511734\n",
      "Training batch 1545 last loss: 0.046072445809841156\n",
      "Training batch 1546 last loss: 0.1021307110786438\n",
      "Training batch 1547 last loss: 0.008743442595005035\n",
      "Training batch 1548 last loss: 0.18208560347557068\n",
      "Training batch 1549 last loss: 0.012151750735938549\n",
      "Training batch 1550 last loss: 0.03317876160144806\n",
      "Training batch 1551 last loss: 0.046350374817848206\n",
      "Training batch 1552 last loss: 0.03378770127892494\n",
      "Training batch 1553 last loss: 0.09469107538461685\n",
      "Training batch 1554 last loss: 0.20396296679973602\n",
      "Training batch 1555 last loss: 0.2325633317232132\n",
      "Training batch 1556 last loss: 0.07169965654611588\n",
      "Training batch 1557 last loss: 0.026261940598487854\n",
      "Training batch 1558 last loss: 0.057458892464637756\n",
      "Training batch 1559 last loss: 0.097243532538414\n",
      "Training batch 1560 last loss: 0.08233720064163208\n",
      "Training batch 1561 last loss: 0.14002075791358948\n",
      "Training batch 1562 last loss: 0.11522834002971649\n",
      "Training batch 1563 last loss: 0.17606496810913086\n",
      "Training batch 1564 last loss: 0.1326882690191269\n",
      "Training batch 1565 last loss: 0.09747207164764404\n",
      "Training batch 1566 last loss: 0.1284160017967224\n",
      "Training batch 1567 last loss: 0.02392125129699707\n",
      "Training batch 1568 last loss: 0.04256530851125717\n",
      "Training batch 1569 last loss: 0.08539661765098572\n",
      "Training batch 1570 last loss: 0.09294719994068146\n",
      "Training batch 1571 last loss: 0.045056503266096115\n",
      "Training batch 1572 last loss: 0.055249981582164764\n",
      "Training batch 1573 last loss: 0.059011928737163544\n",
      "Training batch 1574 last loss: 0.09233148396015167\n",
      "Training batch 1575 last loss: 0.0381825789809227\n",
      "Training batch 1576 last loss: 0.050432659685611725\n",
      "Training batch 1577 last loss: 0.13307835161685944\n",
      "Training batch 1578 last loss: 0.01029343530535698\n",
      "Training batch 1579 last loss: 0.010305617935955524\n",
      "Training batch 1580 last loss: 0.06177680194377899\n",
      "Training batch 1581 last loss: 0.03394578769803047\n",
      "Training batch 1582 last loss: 0.04433835670351982\n",
      "Training batch 1583 last loss: 0.09745088964700699\n",
      "Training batch 1584 last loss: 0.10314742475748062\n",
      "Training batch 1585 last loss: 0.16053920984268188\n",
      "Training batch 1586 last loss: 0.004177525639533997\n",
      "Training batch 1587 last loss: 0.019823111593723297\n",
      "Training batch 1588 last loss: 0.0033395879436284304\n",
      "Training batch 1589 last loss: 0.0668649673461914\n",
      "Training batch 1590 last loss: 0.056010302156209946\n",
      "Training batch 1591 last loss: 0.20447245240211487\n",
      "Training batch 1592 last loss: 0.08949341624975204\n",
      "Training batch 1593 last loss: 0.11391868442296982\n",
      "Training batch 1594 last loss: 0.02564043551683426\n",
      "Training batch 1595 last loss: 0.0989157110452652\n",
      "Training batch 1596 last loss: 0.21318568289279938\n",
      "Training batch 1597 last loss: 0.06779523193836212\n",
      "Training batch 1598 last loss: 0.0032916232012212276\n",
      "Training batch 1599 last loss: 0.007505867630243301\n",
      "Training batch 1600 last loss: 0.008470378816127777\n",
      "Training batch 1601 last loss: 0.18936948478221893\n",
      "Training batch 1602 last loss: 0.14331462979316711\n",
      "Training batch 1603 last loss: 0.0019782979506999254\n",
      "Training batch 1604 last loss: 0.07318756729364395\n",
      "Training batch 1605 last loss: 0.11958499252796173\n",
      "Training batch 1606 last loss: 0.05326763913035393\n",
      "Training batch 1607 last loss: 0.07368899881839752\n",
      "Training batch 1608 last loss: 0.10947132110595703\n",
      "Training batch 1609 last loss: 0.09539429843425751\n",
      "Training batch 1610 last loss: 0.07520965486764908\n",
      "Training batch 1611 last loss: 0.002745749894529581\n",
      "Training batch 1612 last loss: 0.008247734978795052\n",
      "Training batch 1613 last loss: 0.1977252960205078\n",
      "Training batch 1614 last loss: 0.06254793703556061\n",
      "Training batch 1615 last loss: 0.18905316293239594\n",
      "Training batch 1616 last loss: 0.08942751586437225\n",
      "Training batch 1617 last loss: 0.047770921140909195\n",
      "Training batch 1618 last loss: 0.05541684105992317\n",
      "Training batch 1619 last loss: 0.06537062674760818\n",
      "Training batch 1620 last loss: 0.029032710939645767\n",
      "Training batch 1621 last loss: 0.0052544004283845425\n",
      "Training batch 1622 last loss: 0.06540725380182266\n",
      "Training batch 1623 last loss: 0.13558435440063477\n",
      "Training batch 1624 last loss: 0.1694483458995819\n",
      "Training batch 1625 last loss: 0.09318380802869797\n",
      "Training batch 1626 last loss: 0.1225254237651825\n",
      "Training batch 1627 last loss: 0.02898312360048294\n",
      "Training batch 1628 last loss: 0.0497988685965538\n",
      "Training batch 1629 last loss: 0.19359450042247772\n",
      "Training batch 1630 last loss: 0.10693225264549255\n",
      "Training batch 1631 last loss: 0.1745515763759613\n",
      "Training batch 1632 last loss: 0.024643750861287117\n",
      "Training batch 1633 last loss: 0.04433450847864151\n",
      "Training batch 1634 last loss: 0.08409237116575241\n",
      "Training batch 1635 last loss: 0.014088887721300125\n",
      "Training batch 1636 last loss: 0.03473770618438721\n",
      "Training batch 1637 last loss: 0.0253205057233572\n",
      "Training batch 1638 last loss: 0.062487050890922546\n",
      "Training batch 1639 last loss: 0.03232688084244728\n",
      "Training batch 1640 last loss: 0.07815714180469513\n",
      "Training batch 1641 last loss: 0.07426616549491882\n",
      "Training batch 1642 last loss: 0.13756397366523743\n",
      "Training batch 1643 last loss: 0.1880624145269394\n",
      "Training batch 1644 last loss: 0.07298828661441803\n",
      "Training batch 1645 last loss: 0.10893795639276505\n",
      "Training batch 1646 last loss: 0.3698325455188751\n",
      "Training batch 1647 last loss: 0.008260248228907585\n",
      "Training batch 1648 last loss: 0.08888309448957443\n",
      "Training batch 1649 last loss: 0.05164933204650879\n",
      "Training batch 1650 last loss: 0.07719532400369644\n",
      "Training batch 1651 last loss: 0.05635509267449379\n",
      "Training batch 1652 last loss: 0.036271169781684875\n",
      "Training batch 1653 last loss: 0.11063671112060547\n",
      "Training batch 1654 last loss: 0.013838201761245728\n",
      "Training batch 1655 last loss: 0.05659455806016922\n",
      "Training batch 1656 last loss: 0.02397037297487259\n",
      "Training batch 1657 last loss: 0.03100292757153511\n",
      "Training batch 1658 last loss: 0.009650925174355507\n",
      "Training batch 1659 last loss: 0.0549614280462265\n",
      "Training batch 1660 last loss: 0.04236004874110222\n",
      "Training batch 1661 last loss: 0.0682903304696083\n",
      "Training batch 1662 last loss: 0.005787537433207035\n",
      "Training batch 1663 last loss: 0.09333652257919312\n",
      "Training batch 1664 last loss: 0.017228741198778152\n",
      "Training batch 1665 last loss: 0.012437703087925911\n",
      "Training batch 1666 last loss: 0.08961808681488037\n",
      "Training batch 1667 last loss: 0.05344095453619957\n",
      "Training batch 1668 last loss: 0.04548146575689316\n",
      "Training batch 1669 last loss: 0.07361111044883728\n",
      "Training batch 1670 last loss: 0.14617644250392914\n",
      "Training batch 1671 last loss: 0.05083596706390381\n",
      "Training batch 1672 last loss: 0.02642560750246048\n",
      "Training batch 1673 last loss: 0.06028028577566147\n",
      "Training batch 1674 last loss: 0.09340254962444305\n",
      "Training batch 1675 last loss: 0.11489409953355789\n",
      "Training batch 1676 last loss: 0.13252536952495575\n",
      "Training batch 1677 last loss: 0.022707559168338776\n",
      "Training batch 1678 last loss: 0.17525245249271393\n",
      "Training batch 1679 last loss: 0.07186311483383179\n",
      "Training batch 1680 last loss: 0.03142252191901207\n",
      "Training batch 1681 last loss: 0.0380566343665123\n",
      "Training batch 1682 last loss: 0.023089680820703506\n",
      "Training batch 1683 last loss: 0.18136996030807495\n",
      "Training batch 1684 last loss: 0.05930262804031372\n",
      "Training batch 1685 last loss: 0.33114093542099\n",
      "Training batch 1686 last loss: 0.1626303493976593\n",
      "Training batch 1687 last loss: 0.16669350862503052\n",
      "Training batch 1688 last loss: 0.03223288059234619\n",
      "Training batch 1689 last loss: 0.1701083779335022\n",
      "Training batch 1690 last loss: 0.09185127913951874\n",
      "Training batch 1691 last loss: 0.13052166998386383\n",
      "Training batch 1692 last loss: 0.041431255638599396\n",
      "Training batch 1693 last loss: 0.05442286282777786\n",
      "Training batch 1694 last loss: 0.007375674322247505\n",
      "Training batch 1695 last loss: 0.02959585003554821\n",
      "Training batch 1696 last loss: 0.03183896467089653\n",
      "Training batch 1697 last loss: 0.013848094269633293\n",
      "Training batch 1698 last loss: 0.15690481662750244\n",
      "Training batch 1699 last loss: 0.06754934042692184\n",
      "Training batch 1700 last loss: 0.18444308638572693\n",
      "Training batch 1701 last loss: 0.11348754167556763\n",
      "Training batch 1702 last loss: 0.10632810741662979\n",
      "Training batch 1703 last loss: 0.036571040749549866\n",
      "Training batch 1704 last loss: 0.04826044663786888\n",
      "Training batch 1705 last loss: 0.007984680123627186\n",
      "Training batch 1706 last loss: 0.12768980860710144\n",
      "Training batch 1707 last loss: 0.2092263400554657\n",
      "Training batch 1708 last loss: 0.006129582412540913\n",
      "Training batch 1709 last loss: 0.0563710518181324\n",
      "Training batch 1710 last loss: 0.03158865496516228\n",
      "Training batch 1711 last loss: 0.03858660161495209\n",
      "Training batch 1712 last loss: 0.028609080240130424\n",
      "Training batch 1713 last loss: 0.004665683023631573\n",
      "Training batch 1714 last loss: 0.1194317489862442\n",
      "Training batch 1715 last loss: 0.2636808156967163\n",
      "Training batch 1716 last loss: 0.06267763674259186\n",
      "Training batch 1717 last loss: 0.06343294680118561\n",
      "Training batch 1718 last loss: 0.05283072218298912\n",
      "Training batch 1719 last loss: 0.10348661243915558\n",
      "Training batch 1720 last loss: 0.09836649149656296\n",
      "Training batch 1721 last loss: 0.07956165075302124\n",
      "Training batch 1722 last loss: 0.09275670349597931\n",
      "Training batch 1723 last loss: 0.14666499197483063\n",
      "Training batch 1724 last loss: 0.05373857542872429\n",
      "Training batch 1725 last loss: 0.005067847669124603\n",
      "Training batch 1726 last loss: 0.14302493631839752\n",
      "Training batch 1727 last loss: 0.07713039964437485\n",
      "Training batch 1728 last loss: 0.02546750381588936\n",
      "Training batch 1729 last loss: 0.020907802507281303\n",
      "Training batch 1730 last loss: 0.06687088310718536\n",
      "Training batch 1731 last loss: 0.0800459235906601\n",
      "Training batch 1732 last loss: 0.026719041168689728\n",
      "Training batch 1733 last loss: 0.1718416064977646\n",
      "Training batch 1734 last loss: 0.06839678436517715\n",
      "Training batch 1735 last loss: 0.05758940428495407\n",
      "Training batch 1736 last loss: 0.010470342822372913\n",
      "Training batch 1737 last loss: 0.08114096522331238\n",
      "Training batch 1738 last loss: 0.03022937662899494\n",
      "Training batch 1739 last loss: 0.1343337893486023\n",
      "Training batch 1740 last loss: 0.08429867029190063\n",
      "Training batch 1741 last loss: 0.1282467395067215\n",
      "Training batch 1742 last loss: 0.07447944581508636\n",
      "Training batch 1743 last loss: 0.054493505507707596\n",
      "Training batch 1744 last loss: 0.09973341971635818\n",
      "Training batch 1745 last loss: 0.0476091243326664\n",
      "Training batch 1746 last loss: 0.03216368705034256\n",
      "Training batch 1747 last loss: 0.03133171424269676\n",
      "Training batch 1748 last loss: 0.17176119983196259\n",
      "Training batch 1749 last loss: 0.12861251831054688\n",
      "Training batch 1750 last loss: 0.05286930128931999\n",
      "\n",
      "Training epoch 1 loss:  0.05286930128931999\n",
      "Testing batch 1 loss: 0.0030373665504157543\n",
      "Testing accuracy:  1.0\n",
      "Testing batch 2 loss: 0.004916331730782986\n",
      "Testing accuracy:  1.0\n",
      "Testing batch 3 loss: 0.06857093423604965\n",
      "Testing accuracy:  0.9166666666666666\n",
      "Testing batch 4 loss: 0.00657701538875699\n",
      "Testing accuracy:  0.9375\n",
      "Testing batch 5 loss: 0.21736067533493042\n",
      "Testing accuracy:  0.85\n",
      "Testing batch 6 loss: 0.020555272698402405\n",
      "Testing accuracy:  0.875\n",
      "Testing batch 7 loss: 0.040645673871040344\n",
      "Testing accuracy:  0.8928571428571429\n",
      "Testing batch 8 loss: 0.0047362432815134525\n",
      "Testing accuracy:  0.90625\n",
      "Testing batch 9 loss: 0.06490544974803925\n",
      "Testing accuracy:  0.9166666666666666\n",
      "Testing batch 10 loss: 0.34678399562835693\n",
      "Testing accuracy:  0.85\n",
      "Testing batch 11 loss: 0.0045877606607973576\n",
      "Testing accuracy:  0.8636363636363636\n",
      "Testing batch 12 loss: 0.007330008782446384\n",
      "Testing accuracy:  0.875\n",
      "Testing batch 13 loss: 0.07593218237161636\n",
      "Testing accuracy:  0.8653846153846154\n",
      "Testing batch 14 loss: 0.21581010520458221\n",
      "Testing accuracy:  0.8571428571428571\n",
      "Testing batch 15 loss: 0.014479005709290504\n",
      "Testing accuracy:  0.8666666666666667\n",
      "Testing batch 16 loss: 0.12999781966209412\n",
      "Testing accuracy:  0.859375\n",
      "Testing batch 17 loss: 0.09196991473436356\n",
      "Testing accuracy:  0.8529411764705882\n",
      "Testing batch 18 loss: 0.2554331123828888\n",
      "Testing accuracy:  0.8333333333333334\n",
      "Testing batch 19 loss: 0.04069599509239197\n",
      "Testing accuracy:  0.8421052631578947\n",
      "Testing batch 20 loss: 0.18529143929481506\n",
      "Testing accuracy:  0.825\n",
      "Testing batch 21 loss: 0.20121243596076965\n",
      "Testing accuracy:  0.8095238095238095\n",
      "Testing batch 22 loss: 0.010374157689511776\n",
      "Testing accuracy:  0.8181818181818182\n",
      "Testing batch 23 loss: 0.015395404770970345\n",
      "Testing accuracy:  0.8260869565217391\n",
      "Testing batch 24 loss: 0.15591669082641602\n",
      "Testing accuracy:  0.8125\n",
      "Testing batch 25 loss: 0.15733742713928223\n",
      "Testing accuracy:  0.8\n",
      "Testing batch 26 loss: 0.0025448438245803118\n",
      "Testing accuracy:  0.8076923076923077\n",
      "Testing batch 27 loss: 0.10175837576389313\n",
      "Testing accuracy:  0.8055555555555556\n",
      "Testing batch 28 loss: 0.25754043459892273\n",
      "Testing accuracy:  0.7946428571428571\n",
      "Testing batch 29 loss: 0.07639877498149872\n",
      "Testing accuracy:  0.7931034482758621\n",
      "Testing batch 30 loss: 0.06270074099302292\n",
      "Testing accuracy:  0.8\n",
      "Testing batch 31 loss: 0.0019868239760398865\n",
      "Testing accuracy:  0.8064516129032258\n",
      "Testing batch 32 loss: 0.002121069235727191\n",
      "Testing accuracy:  0.8125\n",
      "Testing batch 33 loss: 0.025824621319770813\n",
      "Testing accuracy:  0.8181818181818182\n",
      "Testing batch 34 loss: 0.17547869682312012\n",
      "Testing accuracy:  0.8161764705882353\n",
      "Testing batch 35 loss: 0.014062834903597832\n",
      "Testing accuracy:  0.8214285714285714\n",
      "Testing batch 36 loss: 0.1067606657743454\n",
      "Testing accuracy:  0.8194444444444444\n",
      "Testing batch 37 loss: 0.07645714282989502\n",
      "Testing accuracy:  0.8175675675675675\n",
      "Testing batch 38 loss: 0.05679548159241676\n",
      "Testing accuracy:  0.8223684210526315\n",
      "Testing batch 39 loss: 0.10657109320163727\n",
      "Testing accuracy:  0.8205128205128205\n",
      "Testing batch 40 loss: 0.11307016015052795\n",
      "Testing accuracy:  0.81875\n",
      "Testing batch 41 loss: 0.04174842685461044\n",
      "Testing accuracy:  0.823170731707317\n",
      "Testing batch 42 loss: 0.16765019297599792\n",
      "Testing accuracy:  0.8154761904761905\n",
      "Testing batch 43 loss: 0.0244265329092741\n",
      "Testing accuracy:  0.8197674418604651\n",
      "Testing batch 44 loss: 0.0709846019744873\n",
      "Testing accuracy:  0.8181818181818182\n",
      "Testing batch 45 loss: 0.15626011788845062\n",
      "Testing accuracy:  0.8166666666666667\n",
      "Testing batch 46 loss: 0.09868117421865463\n",
      "Testing accuracy:  0.8152173913043478\n",
      "Testing batch 47 loss: 0.31202232837677\n",
      "Testing accuracy:  0.8085106382978723\n",
      "Testing batch 48 loss: 0.012931851670145988\n",
      "Testing accuracy:  0.8125\n",
      "Testing batch 49 loss: 0.009713497012853622\n",
      "Testing accuracy:  0.8163265306122449\n",
      "Testing batch 50 loss: 0.013750514015555382\n",
      "Testing accuracy:  0.82\n",
      "Testing batch 51 loss: 0.12536558508872986\n",
      "Testing accuracy:  0.8186274509803921\n",
      "Testing batch 52 loss: 0.01851492002606392\n",
      "Testing accuracy:  0.8221153846153846\n",
      "Testing batch 53 loss: 0.07548400014638901\n",
      "Testing accuracy:  0.8207547169811321\n",
      "Testing batch 54 loss: 0.010364262387156487\n",
      "Testing accuracy:  0.8240740740740741\n",
      "Testing batch 55 loss: 0.16877080500125885\n",
      "Testing accuracy:  0.8227272727272728\n",
      "Testing batch 56 loss: 0.15408013761043549\n",
      "Testing accuracy:  0.8214285714285714\n",
      "Testing batch 57 loss: 0.10506000369787216\n",
      "Testing accuracy:  0.8201754385964912\n",
      "Testing batch 58 loss: 0.0027885716408491135\n",
      "Testing accuracy:  0.8232758620689655\n",
      "Testing batch 59 loss: 0.0065365927293896675\n",
      "Testing accuracy:  0.826271186440678\n",
      "Testing batch 60 loss: 0.3523593544960022\n",
      "Testing accuracy:  0.8208333333333333\n",
      "Testing batch 61 loss: 0.176417738199234\n",
      "Testing accuracy:  0.819672131147541\n",
      "Testing batch 62 loss: 0.13836269080638885\n",
      "Testing accuracy:  0.8185483870967742\n",
      "Testing batch 63 loss: 0.15940548479557037\n",
      "Testing accuracy:  0.8174603174603174\n",
      "Testing batch 64 loss: 0.32932525873184204\n",
      "Testing accuracy:  0.80859375\n",
      "Testing batch 65 loss: 0.2542838752269745\n",
      "Testing accuracy:  0.8038461538461539\n",
      "Testing batch 66 loss: 0.06471869349479675\n",
      "Testing accuracy:  0.803030303030303\n",
      "Testing batch 67 loss: 0.1236846074461937\n",
      "Testing accuracy:  0.8022388059701493\n",
      "Testing batch 68 loss: 0.21669591963291168\n",
      "Testing accuracy:  0.7977941176470589\n",
      "Testing batch 69 loss: 0.010686146095395088\n",
      "Testing accuracy:  0.8007246376811594\n",
      "Testing batch 70 loss: 0.08201886713504791\n",
      "Testing accuracy:  0.8\n",
      "Testing batch 71 loss: 0.00277864933013916\n",
      "Testing accuracy:  0.8028169014084507\n",
      "Testing batch 72 loss: 0.17087431252002716\n",
      "Testing accuracy:  0.7986111111111112\n",
      "Testing batch 73 loss: 0.04928049072623253\n",
      "Testing accuracy:  0.797945205479452\n",
      "Testing batch 74 loss: 0.19035601615905762\n",
      "Testing accuracy:  0.793918918918919\n",
      "Testing batch 75 loss: 0.2360515147447586\n",
      "Testing accuracy:  0.79\n",
      "Testing batch 76 loss: 0.01774466224014759\n",
      "Testing accuracy:  0.7927631578947368\n",
      "Testing batch 77 loss: 0.1218309998512268\n",
      "Testing accuracy:  0.7922077922077922\n",
      "Testing batch 78 loss: 0.15953293442726135\n",
      "Testing accuracy:  0.7884615384615384\n",
      "Testing batch 79 loss: 0.11027873307466507\n",
      "Testing accuracy:  0.7879746835443038\n",
      "Testing batch 80 loss: 0.0030008922331035137\n",
      "Testing accuracy:  0.790625\n",
      "Testing batch 81 loss: 0.018205303698778152\n",
      "Testing accuracy:  0.7932098765432098\n",
      "Testing batch 82 loss: 0.23551180958747864\n",
      "Testing accuracy:  0.7896341463414634\n",
      "Testing batch 83 loss: 0.07288917154073715\n",
      "Testing accuracy:  0.7891566265060241\n",
      "Testing batch 84 loss: 0.12149061262607574\n",
      "Testing accuracy:  0.7886904761904762\n",
      "Testing batch 85 loss: 0.003437702776864171\n",
      "Testing accuracy:  0.7911764705882353\n",
      "Testing batch 86 loss: 0.03405432030558586\n",
      "Testing accuracy:  0.7936046511627907\n",
      "Testing batch 87 loss: 0.22081483900547028\n",
      "Testing accuracy:  0.7902298850574713\n",
      "Testing batch 88 loss: 0.0017221738817170262\n",
      "Testing accuracy:  0.7926136363636364\n",
      "Testing batch 89 loss: 0.012676956132054329\n",
      "Testing accuracy:  0.7949438202247191\n",
      "Testing batch 90 loss: 0.058232784271240234\n",
      "Testing accuracy:  0.7972222222222223\n",
      "Testing batch 91 loss: 0.030307842418551445\n",
      "Testing accuracy:  0.7994505494505495\n",
      "Testing batch 92 loss: 0.1754857897758484\n",
      "Testing accuracy:  0.7989130434782609\n",
      "Testing batch 93 loss: 0.16520340740680695\n",
      "Testing accuracy:  0.7956989247311828\n",
      "Testing batch 94 loss: 0.15223510563373566\n",
      "Testing accuracy:  0.7952127659574468\n",
      "Testing batch 95 loss: 0.041360221803188324\n",
      "Testing accuracy:  0.7973684210526316\n",
      "Testing batch 96 loss: 0.008730852045118809\n",
      "Testing accuracy:  0.7994791666666666\n",
      "Testing batch 97 loss: 0.07181724160909653\n",
      "Testing accuracy:  0.7989690721649485\n",
      "Testing batch 98 loss: 0.00825379230082035\n",
      "Testing accuracy:  0.8010204081632653\n",
      "Testing batch 99 loss: 0.27853676676750183\n",
      "Testing accuracy:  0.797979797979798\n",
      "Testing batch 100 loss: 0.07023773342370987\n",
      "Testing accuracy:  0.7975\n",
      "Testing batch 101 loss: 0.21803928911685944\n",
      "Testing accuracy:  0.7945544554455446\n",
      "Testing batch 102 loss: 0.165522962808609\n",
      "Testing accuracy:  0.7941176470588235\n",
      "Testing batch 103 loss: 0.06165695562958717\n",
      "Testing accuracy:  0.7936893203883495\n",
      "Testing batch 104 loss: 0.15006035566329956\n",
      "Testing accuracy:  0.7908653846153846\n",
      "Testing batch 105 loss: 0.031147420406341553\n",
      "Testing accuracy:  0.7928571428571428\n",
      "Testing batch 106 loss: 0.016483794897794724\n",
      "Testing accuracy:  0.7948113207547169\n",
      "Testing batch 107 loss: 0.3495029807090759\n",
      "Testing accuracy:  0.7920560747663551\n",
      "Testing batch 108 loss: 0.02348954603075981\n",
      "Testing accuracy:  0.7939814814814815\n",
      "Testing batch 109 loss: 0.01044068019837141\n",
      "Testing accuracy:  0.7958715596330275\n",
      "Testing batch 110 loss: 0.0023093558847904205\n",
      "Testing accuracy:  0.7977272727272727\n",
      "Testing batch 111 loss: 0.0982113629579544\n",
      "Testing accuracy:  0.7972972972972973\n",
      "Testing batch 112 loss: 0.04345256835222244\n",
      "Testing accuracy:  0.7991071428571429\n",
      "Testing batch 113 loss: 0.0938991904258728\n",
      "Testing accuracy:  0.7986725663716814\n",
      "Testing batch 114 loss: 0.17210206389427185\n",
      "Testing accuracy:  0.7960526315789473\n",
      "Testing batch 115 loss: 0.009298723191022873\n",
      "Testing accuracy:  0.7978260869565217\n",
      "Testing batch 116 loss: 0.03190435469150543\n",
      "Testing accuracy:  0.7995689655172413\n",
      "Testing batch 117 loss: 0.21571733057498932\n",
      "Testing accuracy:  0.7970085470085471\n",
      "Testing batch 118 loss: 0.05718248337507248\n",
      "Testing accuracy:  0.798728813559322\n",
      "Testing batch 119 loss: 0.04600422829389572\n",
      "Testing accuracy:  0.8004201680672269\n",
      "Testing batch 120 loss: 0.007848347537219524\n",
      "Testing accuracy:  0.8020833333333334\n",
      "Testing batch 121 loss: 0.034764546900987625\n",
      "Testing accuracy:  0.8037190082644629\n",
      "Testing batch 122 loss: 0.10206252336502075\n",
      "Testing accuracy:  0.8032786885245902\n",
      "Testing batch 123 loss: 0.1418728530406952\n",
      "Testing accuracy:  0.8008130081300813\n",
      "Testing batch 124 loss: 0.2714722156524658\n",
      "Testing accuracy:  0.7983870967741935\n",
      "Testing batch 125 loss: 0.10891743004322052\n",
      "Testing accuracy:  0.798\n",
      "Testing batch 126 loss: 0.2806563079357147\n",
      "Testing accuracy:  0.7956349206349206\n",
      "Testing batch 127 loss: 0.2253546267747879\n",
      "Testing accuracy:  0.7933070866141733\n",
      "Testing batch 128 loss: 0.07246620953083038\n",
      "Testing accuracy:  0.79296875\n",
      "Testing batch 129 loss: 0.05075272545218468\n",
      "Testing accuracy:  0.7945736434108527\n",
      "Testing batch 130 loss: 0.0024932525120675564\n",
      "Testing accuracy:  0.7961538461538461\n",
      "Testing batch 131 loss: 0.03498588874936104\n",
      "Testing accuracy:  0.7977099236641222\n",
      "Testing batch 132 loss: 0.01025298610329628\n",
      "Testing accuracy:  0.7992424242424242\n",
      "Testing batch 133 loss: 0.026140984147787094\n",
      "Testing accuracy:  0.8007518796992481\n",
      "Testing batch 134 loss: 0.1843586564064026\n",
      "Testing accuracy:  0.7966417910447762\n",
      "Testing batch 135 loss: 0.017981311306357384\n",
      "Testing accuracy:  0.7981481481481482\n",
      "Testing batch 136 loss: 0.01643054187297821\n",
      "Testing accuracy:  0.7996323529411765\n",
      "Testing batch 137 loss: 0.17871633172035217\n",
      "Testing accuracy:  0.7992700729927007\n",
      "Testing batch 138 loss: 0.008148930966854095\n",
      "Testing accuracy:  0.8007246376811594\n",
      "Testing batch 139 loss: 0.09446043521165848\n",
      "Testing accuracy:  0.8003597122302158\n",
      "Testing batch 140 loss: 0.2041677087545395\n",
      "Testing accuracy:  0.7982142857142858\n",
      "Testing batch 141 loss: 0.16134727001190186\n",
      "Testing accuracy:  0.7960992907801419\n",
      "Testing batch 142 loss: 0.10130727291107178\n",
      "Testing accuracy:  0.795774647887324\n",
      "Testing batch 143 loss: 0.07289442420005798\n",
      "Testing accuracy:  0.7954545454545454\n",
      "Testing batch 144 loss: 0.0018822663696482778\n",
      "Testing accuracy:  0.796875\n",
      "Testing batch 145 loss: 0.002587784081697464\n",
      "Testing accuracy:  0.7982758620689655\n",
      "Testing batch 146 loss: 0.004367677494883537\n",
      "Testing accuracy:  0.7996575342465754\n",
      "Testing batch 147 loss: 0.0030334140174090862\n",
      "Testing accuracy:  0.8010204081632653\n",
      "Testing batch 148 loss: 0.005144295282661915\n",
      "Testing accuracy:  0.8023648648648649\n",
      "Testing batch 149 loss: 0.07969844341278076\n",
      "Testing accuracy:  0.802013422818792\n",
      "Testing batch 150 loss: 0.0032336299773305655\n",
      "Testing accuracy:  0.8033333333333333\n",
      "Testing batch 151 loss: 0.18160046637058258\n",
      "Testing accuracy:  0.8029801324503312\n",
      "Testing batch 152 loss: 0.14517277479171753\n",
      "Testing accuracy:  0.8009868421052632\n",
      "Testing batch 153 loss: 0.0768100693821907\n",
      "Testing accuracy:  0.8006535947712419\n",
      "Testing batch 154 loss: 0.1780262440443039\n",
      "Testing accuracy:  0.7987012987012987\n",
      "Testing batch 155 loss: 0.09426259994506836\n",
      "Testing accuracy:  0.7983870967741935\n",
      "Testing batch 156 loss: 0.20539139211177826\n",
      "Testing accuracy:  0.7980769230769231\n",
      "Testing batch 157 loss: 0.12244996428489685\n",
      "Testing accuracy:  0.7977707006369427\n",
      "Testing batch 158 loss: 0.10083639621734619\n",
      "Testing accuracy:  0.7974683544303798\n",
      "Testing batch 159 loss: 0.10132913291454315\n",
      "Testing accuracy:  0.7971698113207547\n",
      "Testing batch 160 loss: 0.08583162724971771\n",
      "Testing accuracy:  0.796875\n",
      "Testing batch 161 loss: 0.12686775624752045\n",
      "Testing accuracy:  0.796583850931677\n",
      "Testing batch 162 loss: 0.0022844080813229084\n",
      "Testing accuracy:  0.7978395061728395\n",
      "Testing batch 163 loss: 0.05660563334822655\n",
      "Testing accuracy:  0.7975460122699386\n",
      "Testing batch 164 loss: 0.1806577742099762\n",
      "Testing accuracy:  0.7957317073170732\n",
      "Testing batch 165 loss: 0.026860574260354042\n",
      "Testing accuracy:  0.796969696969697\n",
      "Testing batch 166 loss: 0.2117677628993988\n",
      "Testing accuracy:  0.7951807228915663\n",
      "Testing batch 167 loss: 0.021824005991220474\n",
      "Testing accuracy:  0.7964071856287425\n",
      "Testing batch 168 loss: 0.029149817302823067\n",
      "Testing accuracy:  0.7976190476190477\n",
      "Testing batch 169 loss: 0.1053708866238594\n",
      "Testing accuracy:  0.7973372781065089\n",
      "Testing batch 170 loss: 0.4715813398361206\n",
      "Testing accuracy:  0.7941176470588235\n",
      "Testing batch 171 loss: 0.30230581760406494\n",
      "Testing accuracy:  0.793859649122807\n",
      "Testing batch 172 loss: 0.3406949043273926\n",
      "Testing accuracy:  0.7921511627906976\n",
      "Testing batch 173 loss: 0.1070621982216835\n",
      "Testing accuracy:  0.791907514450867\n",
      "Testing batch 174 loss: 0.48409727215766907\n",
      "Testing accuracy:  0.7902298850574713\n",
      "Testing batch 175 loss: 0.0680576041340828\n",
      "Testing accuracy:  0.79\n",
      "Testing batch 176 loss: 0.01996786706149578\n",
      "Testing accuracy:  0.7911931818181818\n",
      "Testing batch 177 loss: 0.005939204711467028\n",
      "Testing accuracy:  0.7923728813559322\n",
      "Testing batch 178 loss: 0.0021609854884445667\n",
      "Testing accuracy:  0.7935393258426966\n",
      "Testing batch 179 loss: 0.14565448462963104\n",
      "Testing accuracy:  0.7932960893854749\n",
      "Testing batch 180 loss: 0.2695390582084656\n",
      "Testing accuracy:  0.7916666666666666\n",
      "Testing batch 181 loss: 0.02619960531592369\n",
      "Testing accuracy:  0.7928176795580111\n",
      "Testing batch 182 loss: 0.16370457410812378\n",
      "Testing accuracy:  0.7925824175824175\n",
      "Testing batch 183 loss: 0.08810354024171829\n",
      "Testing accuracy:  0.7923497267759563\n",
      "Testing batch 184 loss: 0.05712328106164932\n",
      "Testing accuracy:  0.7934782608695652\n",
      "Testing batch 185 loss: 0.041867855936288834\n",
      "Testing accuracy:  0.7945945945945946\n",
      "Testing batch 186 loss: 0.06139399856328964\n",
      "Testing accuracy:  0.7956989247311828\n",
      "Testing batch 187 loss: 0.011987650766968727\n",
      "Testing accuracy:  0.7967914438502673\n",
      "Testing batch 188 loss: 0.004199865739792585\n",
      "Testing accuracy:  0.7978723404255319\n",
      "Testing batch 189 loss: 0.001857866533100605\n",
      "Testing accuracy:  0.798941798941799\n",
      "Testing batch 190 loss: 0.1220472902059555\n",
      "Testing accuracy:  0.7986842105263158\n",
      "Testing batch 191 loss: 0.024827253073453903\n",
      "Testing accuracy:  0.7997382198952879\n",
      "Testing batch 192 loss: 0.00905076414346695\n",
      "Testing accuracy:  0.80078125\n",
      "Testing batch 193 loss: 0.2492060363292694\n",
      "Testing accuracy:  0.7992227979274611\n",
      "Testing batch 194 loss: 0.11740142107009888\n",
      "Testing accuracy:  0.7989690721649485\n",
      "Testing batch 195 loss: 0.06417356431484222\n",
      "Testing accuracy:  0.7987179487179488\n",
      "Testing batch 196 loss: 0.005110682919621468\n",
      "Testing accuracy:  0.7997448979591837\n",
      "Testing batch 197 loss: 0.36069053411483765\n",
      "Testing accuracy:  0.7982233502538071\n",
      "Testing batch 198 loss: 0.25015461444854736\n",
      "Testing accuracy:  0.7967171717171717\n",
      "Testing batch 199 loss: 0.16767433285713196\n",
      "Testing accuracy:  0.7964824120603015\n",
      "Testing batch 200 loss: 0.0045082103461027145\n",
      "Testing accuracy:  0.7975\n",
      "Testing batch 201 loss: 0.02975897490978241\n",
      "Testing accuracy:  0.7985074626865671\n",
      "Testing batch 202 loss: 0.002469171304255724\n",
      "Testing accuracy:  0.7995049504950495\n",
      "Testing batch 203 loss: 0.006500495597720146\n",
      "Testing accuracy:  0.8004926108374384\n",
      "Testing batch 204 loss: 0.13105198740959167\n",
      "Testing accuracy:  0.8002450980392157\n",
      "Testing batch 205 loss: 0.024510400369763374\n",
      "Testing accuracy:  0.801219512195122\n",
      "Testing batch 206 loss: 0.23786437511444092\n",
      "Testing accuracy:  0.7997572815533981\n",
      "Testing batch 207 loss: 0.1943242847919464\n",
      "Testing accuracy:  0.7983091787439613\n",
      "Testing batch 208 loss: 0.12392346560955048\n",
      "Testing accuracy:  0.7980769230769231\n",
      "Testing batch 209 loss: 0.13505135476589203\n",
      "Testing accuracy:  0.7978468899521531\n",
      "Testing batch 210 loss: 0.01239883154630661\n",
      "Testing accuracy:  0.7988095238095239\n",
      "Testing batch 211 loss: 0.10241684317588806\n",
      "Testing accuracy:  0.7985781990521327\n",
      "Testing batch 212 loss: 0.12892134487628937\n",
      "Testing accuracy:  0.7983490566037735\n",
      "Testing batch 213 loss: 0.22649091482162476\n",
      "Testing accuracy:  0.7981220657276995\n",
      "Testing batch 214 loss: 0.031175795942544937\n",
      "Testing accuracy:  0.7990654205607477\n",
      "Testing batch 215 loss: 0.009932558983564377\n",
      "Testing accuracy:  0.8\n",
      "Testing batch 216 loss: 0.07254179567098618\n",
      "Testing accuracy:  0.7997685185185185\n",
      "Testing batch 217 loss: 0.05610247328877449\n",
      "Testing accuracy:  0.7995391705069125\n",
      "Testing batch 218 loss: 0.025156598538160324\n",
      "Testing accuracy:  0.8004587155963303\n",
      "Testing batch 219 loss: 0.1060410737991333\n",
      "Testing accuracy:  0.8002283105022832\n",
      "Testing batch 220 loss: 0.18021570146083832\n",
      "Testing accuracy:  0.7988636363636363\n",
      "Testing batch 221 loss: 0.09850522130727768\n",
      "Testing accuracy:  0.7986425339366516\n",
      "Testing batch 222 loss: 0.028493376448750496\n",
      "Testing accuracy:  0.7995495495495496\n",
      "Testing batch 223 loss: 0.18535840511322021\n",
      "Testing accuracy:  0.7993273542600897\n",
      "Testing batch 224 loss: 0.13277968764305115\n",
      "Testing accuracy:  0.7979910714285714\n",
      "Testing batch 225 loss: 0.03516439348459244\n",
      "Testing accuracy:  0.7988888888888889\n",
      "Testing batch 226 loss: 0.09916012734174728\n",
      "Testing accuracy:  0.7986725663716814\n",
      "Testing batch 227 loss: 0.02512391097843647\n",
      "Testing accuracy:  0.7995594713656388\n",
      "Testing batch 228 loss: 0.46799367666244507\n",
      "Testing accuracy:  0.7971491228070176\n",
      "Testing batch 229 loss: 0.07998586446046829\n",
      "Testing accuracy:  0.7969432314410481\n",
      "Testing batch 230 loss: 0.0017434835899621248\n",
      "Testing accuracy:  0.7978260869565217\n",
      "Testing batch 231 loss: 0.24182823300361633\n",
      "Testing accuracy:  0.7976190476190477\n",
      "Testing batch 232 loss: 0.17615708708763123\n",
      "Testing accuracy:  0.7974137931034483\n",
      "Testing batch 233 loss: 0.1140231192111969\n",
      "Testing accuracy:  0.7972103004291845\n",
      "Testing batch 234 loss: 0.06531206518411636\n",
      "Testing accuracy:  0.7970085470085471\n",
      "Testing batch 235 loss: 0.165558323264122\n",
      "Testing accuracy:  0.7957446808510639\n",
      "Testing batch 236 loss: 0.3212483525276184\n",
      "Testing accuracy:  0.7944915254237288\n",
      "Testing batch 237 loss: 0.13871067762374878\n",
      "Testing accuracy:  0.7943037974683544\n",
      "Testing batch 238 loss: 0.09209632128477097\n",
      "Testing accuracy:  0.7941176470588235\n",
      "Testing batch 239 loss: 0.026679811999201775\n",
      "Testing accuracy:  0.7949790794979079\n",
      "Testing batch 240 loss: 0.045415326952934265\n",
      "Testing accuracy:  0.7958333333333333\n",
      "Testing batch 241 loss: 0.11828725785017014\n",
      "Testing accuracy:  0.7956431535269709\n",
      "Testing batch 242 loss: 0.13218113780021667\n",
      "Testing accuracy:  0.7954545454545454\n",
      "Testing batch 243 loss: 0.005905799102038145\n",
      "Testing accuracy:  0.7962962962962963\n",
      "Testing batch 244 loss: 0.0038188102189451456\n",
      "Testing accuracy:  0.7971311475409836\n",
      "Testing batch 245 loss: 0.26752185821533203\n",
      "Testing accuracy:  0.7959183673469388\n",
      "Testing batch 246 loss: 0.04024825617671013\n",
      "Testing accuracy:  0.7967479674796748\n",
      "Testing batch 247 loss: 0.042337287217378616\n",
      "Testing accuracy:  0.7975708502024291\n",
      "Testing batch 248 loss: 0.06416477262973785\n",
      "Testing accuracy:  0.7973790322580645\n",
      "Testing batch 249 loss: 0.0024117138236761093\n",
      "Testing accuracy:  0.7981927710843374\n",
      "Testing batch 250 loss: 0.11741053313016891\n",
      "Testing accuracy:  0.798\n",
      "Testing batch 251 loss: 0.09565549343824387\n",
      "Testing accuracy:  0.797808764940239\n",
      "Testing batch 252 loss: 0.0021756310015916824\n",
      "Testing accuracy:  0.7986111111111112\n",
      "Testing batch 253 loss: 0.6401251554489136\n",
      "Testing accuracy:  0.7954545454545454\n",
      "Testing batch 254 loss: 0.18176041543483734\n",
      "Testing accuracy:  0.7942913385826772\n",
      "Testing batch 255 loss: 0.011173032224178314\n",
      "Testing accuracy:  0.7950980392156862\n",
      "Testing batch 256 loss: 0.178503155708313\n",
      "Testing accuracy:  0.794921875\n",
      "Testing batch 257 loss: 0.13599196076393127\n",
      "Testing accuracy:  0.7947470817120622\n",
      "Testing batch 258 loss: 0.09116892516613007\n",
      "Testing accuracy:  0.7945736434108527\n",
      "Testing batch 259 loss: 0.11913061141967773\n",
      "Testing accuracy:  0.7944015444015444\n",
      "Testing batch 260 loss: 0.12672269344329834\n",
      "Testing accuracy:  0.7932692307692307\n",
      "Testing batch 261 loss: 0.030983055010437965\n",
      "Testing accuracy:  0.7940613026819924\n",
      "Testing batch 262 loss: 0.006465267855674028\n",
      "Testing accuracy:  0.7948473282442748\n",
      "Testing batch 263 loss: 0.09662319719791412\n",
      "Testing accuracy:  0.7946768060836502\n",
      "Testing batch 264 loss: 0.18913432955741882\n",
      "Testing accuracy:  0.7935606060606061\n",
      "Testing batch 265 loss: 0.041231025010347366\n",
      "Testing accuracy:  0.7943396226415095\n",
      "Testing batch 266 loss: 0.3232954740524292\n",
      "Testing accuracy:  0.7922932330827067\n",
      "Testing batch 267 loss: 0.34973564743995667\n",
      "Testing accuracy:  0.7902621722846442\n",
      "Testing batch 268 loss: 0.19549967348575592\n",
      "Testing accuracy:  0.789179104477612\n",
      "Testing batch 269 loss: 0.10265125334262848\n",
      "Testing accuracy:  0.7881040892193308\n",
      "Testing batch 270 loss: 0.2526991069316864\n",
      "Testing accuracy:  0.7870370370370371\n",
      "Testing batch 271 loss: 0.08179470896720886\n",
      "Testing accuracy:  0.7869003690036901\n",
      "Testing batch 272 loss: 0.10660304874181747\n",
      "Testing accuracy:  0.7867647058823529\n",
      "Testing batch 273 loss: 0.053736478090286255\n",
      "Testing accuracy:  0.7875457875457875\n",
      "Testing batch 274 loss: 0.13190847635269165\n",
      "Testing accuracy:  0.7874087591240876\n",
      "Testing batch 275 loss: 0.0016751482617110014\n",
      "Testing accuracy:  0.7881818181818182\n",
      "Testing batch 276 loss: 0.03438087925314903\n",
      "Testing accuracy:  0.7889492753623188\n",
      "Testing batch 277 loss: 0.0030774225015193224\n",
      "Testing accuracy:  0.7897111913357401\n",
      "Testing batch 278 loss: 0.031358953565359116\n",
      "Testing accuracy:  0.7904676258992805\n",
      "Testing batch 279 loss: 0.014898279681801796\n",
      "Testing accuracy:  0.7912186379928315\n",
      "Testing batch 280 loss: 0.2116387039422989\n",
      "Testing accuracy:  0.7910714285714285\n",
      "Testing batch 281 loss: 0.06785809993743896\n",
      "Testing accuracy:  0.791814946619217\n",
      "Testing batch 282 loss: 0.10595566034317017\n",
      "Testing accuracy:  0.7916666666666666\n",
      "Testing batch 283 loss: 0.000889673363417387\n",
      "Testing accuracy:  0.7924028268551236\n",
      "Testing batch 284 loss: 0.16692106425762177\n",
      "Testing accuracy:  0.7922535211267606\n",
      "Testing batch 285 loss: 0.24174126982688904\n",
      "Testing accuracy:  0.7921052631578948\n",
      "Testing batch 286 loss: 0.02812066674232483\n",
      "Testing accuracy:  0.7928321678321678\n",
      "Testing batch 287 loss: 0.170978382229805\n",
      "Testing accuracy:  0.7926829268292683\n",
      "Testing batch 288 loss: 0.20367096364498138\n",
      "Testing accuracy:  0.7925347222222222\n",
      "Testing batch 289 loss: 0.08308692276477814\n",
      "Testing accuracy:  0.7923875432525952\n",
      "Testing batch 290 loss: 0.10590265691280365\n",
      "Testing accuracy:  0.7922413793103448\n",
      "Testing batch 291 loss: 0.003928374964743853\n",
      "Testing accuracy:  0.7929553264604811\n",
      "Testing batch 292 loss: 0.16445350646972656\n",
      "Testing accuracy:  0.7928082191780822\n",
      "Testing batch 293 loss: 0.00757663743570447\n",
      "Testing accuracy:  0.7935153583617748\n",
      "Testing batch 294 loss: 0.06835886090993881\n",
      "Testing accuracy:  0.7933673469387755\n",
      "Testing batch 295 loss: 0.08348824083805084\n",
      "Testing accuracy:  0.7932203389830509\n",
      "Testing batch 296 loss: 0.0017915407661348581\n",
      "Testing accuracy:  0.793918918918919\n",
      "Testing batch 297 loss: 0.18221940100193024\n",
      "Testing accuracy:  0.7937710437710438\n",
      "Testing batch 298 loss: 0.12069311738014221\n",
      "Testing accuracy:  0.7936241610738255\n",
      "Testing batch 299 loss: 0.08795700967311859\n",
      "Testing accuracy:  0.7934782608695652\n",
      "Testing batch 300 loss: 0.10013153403997421\n",
      "Testing accuracy:  0.7933333333333333\n",
      "Testing batch 301 loss: 0.1181018278002739\n",
      "Testing accuracy:  0.7931893687707641\n",
      "Testing batch 302 loss: 0.00492621585726738\n",
      "Testing accuracy:  0.7938741721854304\n",
      "Testing batch 303 loss: 0.0075681465677917\n",
      "Testing accuracy:  0.7945544554455446\n",
      "Testing batch 304 loss: 0.012636043131351471\n",
      "Testing accuracy:  0.7952302631578947\n",
      "Testing batch 305 loss: 0.21405869722366333\n",
      "Testing accuracy:  0.7934426229508197\n",
      "Testing batch 306 loss: 0.0403454527258873\n",
      "Testing accuracy:  0.7941176470588235\n",
      "Testing batch 307 loss: 0.1473124623298645\n",
      "Testing accuracy:  0.7939739413680782\n",
      "Testing batch 308 loss: 0.19999423623085022\n",
      "Testing accuracy:  0.7938311688311688\n",
      "Testing batch 309 loss: 0.22241854667663574\n",
      "Testing accuracy:  0.7928802588996764\n",
      "Testing batch 310 loss: 0.023289093747735023\n",
      "Testing accuracy:  0.7935483870967742\n",
      "Testing batch 311 loss: 0.00524046178907156\n",
      "Testing accuracy:  0.7942122186495176\n",
      "Testing batch 312 loss: 0.1490251123905182\n",
      "Testing accuracy:  0.7940705128205128\n",
      "Testing batch 313 loss: 0.08454034477472305\n",
      "Testing accuracy:  0.7939297124600639\n",
      "Testing batch 314 loss: 0.11389125138521194\n",
      "Testing accuracy:  0.7937898089171974\n",
      "Testing batch 315 loss: 0.0954148918390274\n",
      "Testing accuracy:  0.7936507936507936\n",
      "Testing batch 316 loss: 0.060910843312740326\n",
      "Testing accuracy:  0.7935126582278481\n",
      "Testing batch 317 loss: 0.23426207900047302\n",
      "Testing accuracy:  0.7925867507886435\n",
      "Testing batch 318 loss: 0.006673399358987808\n",
      "Testing accuracy:  0.7932389937106918\n",
      "Testing batch 319 loss: 0.08851517736911774\n",
      "Testing accuracy:  0.7931034482758621\n",
      "Testing batch 320 loss: 0.3455982804298401\n",
      "Testing accuracy:  0.7921875\n",
      "Testing batch 321 loss: 0.17686492204666138\n",
      "Testing accuracy:  0.7920560747663551\n",
      "Testing batch 322 loss: 0.1935240924358368\n",
      "Testing accuracy:  0.7919254658385093\n",
      "Testing batch 323 loss: 0.02727827988564968\n",
      "Testing accuracy:  0.7925696594427245\n",
      "Testing batch 324 loss: 0.13574406504631042\n",
      "Testing accuracy:  0.7924382716049383\n",
      "Testing batch 325 loss: 0.030663814395666122\n",
      "Testing accuracy:  0.7930769230769231\n",
      "Testing batch 326 loss: 0.16531400382518768\n",
      "Testing accuracy:  0.7929447852760736\n",
      "Testing batch 327 loss: 0.02309543639421463\n",
      "Testing accuracy:  0.7935779816513762\n",
      "Testing batch 328 loss: 0.0938812792301178\n",
      "Testing accuracy:  0.7934451219512195\n",
      "Testing batch 329 loss: 0.2745186686515808\n",
      "Testing accuracy:  0.7925531914893617\n",
      "Testing batch 330 loss: 0.19719551503658295\n",
      "Testing accuracy:  0.7924242424242425\n",
      "Testing batch 331 loss: 0.21611908078193665\n",
      "Testing accuracy:  0.7922960725075529\n",
      "Testing batch 332 loss: 0.006068428047001362\n",
      "Testing accuracy:  0.7929216867469879\n",
      "Testing batch 333 loss: 0.08000364899635315\n",
      "Testing accuracy:  0.7927927927927928\n",
      "Testing batch 334 loss: 0.10334931313991547\n",
      "Testing accuracy:  0.7926646706586826\n",
      "Testing batch 335 loss: 0.03911364823579788\n",
      "Testing accuracy:  0.7932835820895522\n",
      "Testing batch 336 loss: 0.1475416123867035\n",
      "Testing accuracy:  0.7924107142857143\n",
      "Testing batch 337 loss: 0.12182135879993439\n",
      "Testing accuracy:  0.7922848664688428\n",
      "Testing batch 338 loss: 0.27231431007385254\n",
      "Testing accuracy:  0.7921597633136095\n",
      "Testing batch 339 loss: 0.1133008524775505\n",
      "Testing accuracy:  0.7920353982300885\n",
      "Testing batch 340 loss: 0.0029957564547657967\n",
      "Testing accuracy:  0.7926470588235294\n",
      "Testing batch 341 loss: 0.2068464308977127\n",
      "Testing accuracy:  0.7925219941348973\n",
      "Testing batch 342 loss: 0.013001692481338978\n",
      "Testing accuracy:  0.7931286549707602\n",
      "Testing batch 343 loss: 0.011757208965718746\n",
      "Testing accuracy:  0.793731778425656\n",
      "Testing batch 344 loss: 0.004441733937710524\n",
      "Testing accuracy:  0.7943313953488372\n",
      "Testing batch 345 loss: 0.2297934591770172\n",
      "Testing accuracy:  0.7934782608695652\n",
      "Testing batch 346 loss: 0.316113144159317\n",
      "Testing accuracy:  0.7926300578034682\n",
      "Testing batch 347 loss: 0.016966573894023895\n",
      "Testing accuracy:  0.7932276657060519\n",
      "Testing batch 348 loss: 0.03988463804125786\n",
      "Testing accuracy:  0.7938218390804598\n",
      "Testing batch 349 loss: 0.11593500524759293\n",
      "Testing accuracy:  0.7936962750716332\n",
      "Testing batch 350 loss: 0.3344835042953491\n",
      "Testing accuracy:  0.7928571428571428\n",
      "Testing batch 351 loss: 0.22952546179294586\n",
      "Testing accuracy:  0.792022792022792\n",
      "Testing batch 352 loss: 0.07091157138347626\n",
      "Testing accuracy:  0.7919034090909091\n",
      "Testing batch 353 loss: 0.11215793341398239\n",
      "Testing accuracy:  0.7917847025495751\n",
      "Testing batch 354 loss: 0.08730665594339371\n",
      "Testing accuracy:  0.7916666666666666\n",
      "Testing batch 355 loss: 0.18409425020217896\n",
      "Testing accuracy:  0.7915492957746478\n",
      "Testing batch 356 loss: 0.14394637942314148\n",
      "Testing accuracy:  0.7914325842696629\n",
      "Testing batch 357 loss: 0.07230321317911148\n",
      "Testing accuracy:  0.7913165266106442\n",
      "Testing batch 358 loss: 0.03360852599143982\n",
      "Testing accuracy:  0.7918994413407822\n",
      "Testing batch 359 loss: 0.004018766805529594\n",
      "Testing accuracy:  0.7924791086350975\n",
      "Testing batch 360 loss: 0.42332157492637634\n",
      "Testing accuracy:  0.7909722222222222\n",
      "Testing batch 361 loss: 0.15088607370853424\n",
      "Testing accuracy:  0.7908587257617729\n",
      "Testing batch 362 loss: 0.0015061532612890005\n",
      "Testing accuracy:  0.7914364640883977\n",
      "Testing batch 363 loss: 0.022018171846866608\n",
      "Testing accuracy:  0.7920110192837465\n",
      "Testing batch 364 loss: 0.008630642667412758\n",
      "Testing accuracy:  0.7925824175824175\n",
      "Testing batch 365 loss: 0.012615535408258438\n",
      "Testing accuracy:  0.7931506849315069\n",
      "Testing batch 366 loss: 0.08790198713541031\n",
      "Testing accuracy:  0.7930327868852459\n",
      "Testing batch 367 loss: 0.03334517031908035\n",
      "Testing accuracy:  0.7935967302452316\n",
      "Testing batch 368 loss: 0.26007819175720215\n",
      "Testing accuracy:  0.7921195652173914\n",
      "Testing batch 369 loss: 0.19332125782966614\n",
      "Testing accuracy:  0.7920054200542005\n",
      "Testing batch 370 loss: 0.14259563386440277\n",
      "Testing accuracy:  0.7918918918918919\n",
      "Testing batch 371 loss: 0.006457039620727301\n",
      "Testing accuracy:  0.7924528301886793\n",
      "Testing batch 372 loss: 0.056109555065631866\n",
      "Testing accuracy:  0.793010752688172\n",
      "Testing batch 373 loss: 0.26172327995300293\n",
      "Testing accuracy:  0.792225201072386\n",
      "Testing batch 374 loss: 0.06439832597970963\n",
      "Testing accuracy:  0.7927807486631016\n",
      "Testing batch 375 loss: 0.044847775250673294\n",
      "Testing accuracy:  0.7933333333333333\n",
      "Testing batch 376 loss: 0.029595468193292618\n",
      "Testing accuracy:  0.7938829787234043\n",
      "Testing batch 377 loss: 0.05367142707109451\n",
      "Testing accuracy:  0.7944297082228117\n",
      "Testing batch 378 loss: 0.026008732616901398\n",
      "Testing accuracy:  0.794973544973545\n",
      "Testing batch 379 loss: 0.034780748188495636\n",
      "Testing accuracy:  0.7955145118733509\n",
      "Testing batch 380 loss: 0.006760844495147467\n",
      "Testing accuracy:  0.7960526315789473\n",
      "Testing batch 381 loss: 0.16289861500263214\n",
      "Testing accuracy:  0.7959317585301837\n",
      "Testing batch 382 loss: 0.2553253769874573\n",
      "Testing accuracy:  0.7951570680628273\n",
      "Testing batch 383 loss: 0.05819489061832428\n",
      "Testing accuracy:  0.7956919060052219\n",
      "Testing batch 384 loss: 0.12631939351558685\n",
      "Testing accuracy:  0.7955729166666666\n",
      "Testing batch 385 loss: 0.1305365115404129\n",
      "Testing accuracy:  0.7954545454545454\n",
      "Testing batch 386 loss: 0.0720147043466568\n",
      "Testing accuracy:  0.7953367875647669\n",
      "Testing batch 387 loss: 0.010255038738250732\n",
      "Testing accuracy:  0.7958656330749354\n",
      "Testing batch 388 loss: 0.008576689288020134\n",
      "Testing accuracy:  0.7963917525773195\n",
      "Testing batch 389 loss: 0.06679210811853409\n",
      "Testing accuracy:  0.7969151670951157\n",
      "Testing batch 390 loss: 0.12955091893672943\n",
      "Testing accuracy:  0.7967948717948717\n",
      "Testing batch 391 loss: 0.07113705575466156\n",
      "Testing accuracy:  0.7966751918158568\n",
      "Testing batch 392 loss: 0.06484421342611313\n",
      "Testing accuracy:  0.7965561224489796\n",
      "Testing batch 393 loss: 0.008534523658454418\n",
      "Testing accuracy:  0.7970737913486005\n",
      "Testing batch 394 loss: 0.00902539025992155\n",
      "Testing accuracy:  0.7975888324873096\n",
      "Testing batch 395 loss: 0.2875998020172119\n",
      "Testing accuracy:  0.7962025316455696\n",
      "Testing batch 396 loss: 0.10337450355291367\n",
      "Testing accuracy:  0.7960858585858586\n",
      "Testing batch 397 loss: 0.12224218994379044\n",
      "Testing accuracy:  0.7953400503778337\n",
      "Testing batch 398 loss: 0.0864374190568924\n",
      "Testing accuracy:  0.7952261306532663\n",
      "Testing batch 399 loss: 0.051619723439216614\n",
      "Testing accuracy:  0.7951127819548872\n",
      "Testing batch 400 loss: 0.11575720459222794\n",
      "Testing accuracy:  0.795\n",
      "Testing batch 401 loss: 0.21968817710876465\n",
      "Testing accuracy:  0.7948877805486284\n",
      "Testing batch 402 loss: 0.1256670355796814\n",
      "Testing accuracy:  0.7947761194029851\n",
      "Testing batch 403 loss: 0.0685616284608841\n",
      "Testing accuracy:  0.7946650124069479\n",
      "Testing batch 404 loss: 0.17274440824985504\n",
      "Testing accuracy:  0.7945544554455446\n",
      "Testing batch 405 loss: 0.06720495223999023\n",
      "Testing accuracy:  0.7944444444444444\n",
      "Testing batch 406 loss: 0.018428606912493706\n",
      "Testing accuracy:  0.7949507389162561\n",
      "Testing batch 407 loss: 0.10239309817552567\n",
      "Testing accuracy:  0.7948402948402948\n",
      "Testing batch 408 loss: 0.08108890801668167\n",
      "Testing accuracy:  0.7947303921568627\n",
      "Testing batch 409 loss: 0.03903943672776222\n",
      "Testing accuracy:  0.7952322738386308\n",
      "Testing batch 410 loss: 0.0034168059937655926\n",
      "Testing accuracy:  0.7957317073170732\n",
      "Testing batch 411 loss: 0.006106369197368622\n",
      "Testing accuracy:  0.7962287104622872\n",
      "Testing batch 412 loss: 0.008368791081011295\n",
      "Testing accuracy:  0.7967233009708737\n",
      "Testing batch 413 loss: 0.16290462017059326\n",
      "Testing accuracy:  0.7966101694915254\n",
      "Testing batch 414 loss: 0.10910800099372864\n",
      "Testing accuracy:  0.7964975845410628\n",
      "Testing batch 415 loss: 0.006348734721541405\n",
      "Testing accuracy:  0.7969879518072289\n",
      "Testing batch 416 loss: 0.012963484972715378\n",
      "Testing accuracy:  0.7974759615384616\n",
      "Testing batch 417 loss: 0.21563172340393066\n",
      "Testing accuracy:  0.7967625899280576\n",
      "Testing batch 418 loss: 0.006789444945752621\n",
      "Testing accuracy:  0.7972488038277512\n",
      "Testing batch 419 loss: 0.005425110459327698\n",
      "Testing accuracy:  0.7977326968973747\n",
      "Testing batch 420 loss: 0.08656509965658188\n",
      "Testing accuracy:  0.7976190476190477\n",
      "Testing batch 421 loss: 0.16178731620311737\n",
      "Testing accuracy:  0.7975059382422803\n",
      "Testing batch 422 loss: 0.059106696397066116\n",
      "Testing accuracy:  0.79739336492891\n",
      "Testing batch 423 loss: 0.055805280804634094\n",
      "Testing accuracy:  0.7972813238770685\n",
      "Testing batch 424 loss: 0.09714227169752121\n",
      "Testing accuracy:  0.7971698113207547\n",
      "Testing batch 425 loss: 0.010156569071114063\n",
      "Testing accuracy:  0.7976470588235294\n",
      "Testing batch 426 loss: 0.010118268430233002\n",
      "Testing accuracy:  0.7981220657276995\n",
      "Testing batch 427 loss: 0.10918036848306656\n",
      "Testing accuracy:  0.7980093676814989\n",
      "Testing batch 428 loss: 0.08630320429801941\n",
      "Testing accuracy:  0.7978971962616822\n",
      "Testing batch 429 loss: 0.02010258287191391\n",
      "Testing accuracy:  0.7983682983682984\n",
      "Testing batch 430 loss: 0.1534915715456009\n",
      "Testing accuracy:  0.7982558139534883\n",
      "Testing batch 431 loss: 0.015068655833601952\n",
      "Testing accuracy:  0.798723897911833\n",
      "Testing batch 432 loss: 0.4119050204753876\n",
      "Testing accuracy:  0.7980324074074074\n",
      "Testing batch 433 loss: 0.3445013165473938\n",
      "Testing accuracy:  0.7979214780600462\n",
      "Testing batch 434 loss: 0.027149925008416176\n",
      "Testing accuracy:  0.7983870967741935\n",
      "Testing batch 435 loss: 0.006175092421472073\n",
      "Testing accuracy:  0.7988505747126436\n",
      "Testing batch 436 loss: 0.05450804904103279\n",
      "Testing accuracy:  0.7987385321100917\n",
      "Testing batch 437 loss: 0.023153379559516907\n",
      "Testing accuracy:  0.7991990846681922\n",
      "Testing batch 438 loss: 0.00689361710101366\n",
      "Testing accuracy:  0.7996575342465754\n",
      "Testing batch 439 loss: 0.10285989940166473\n",
      "Testing accuracy:  0.7995444191343963\n",
      "Testing batch 440 loss: 0.2383173257112503\n",
      "Testing accuracy:  0.7988636363636363\n",
      "Testing batch 441 loss: 0.021891707554459572\n",
      "Testing accuracy:  0.7993197278911565\n",
      "Testing batch 442 loss: 0.24654436111450195\n",
      "Testing accuracy:  0.7986425339366516\n",
      "Testing batch 443 loss: 0.0016980868531391025\n",
      "Testing accuracy:  0.7990970654627539\n",
      "Testing batch 444 loss: 0.03505920246243477\n",
      "Testing accuracy:  0.7995495495495496\n",
      "Testing batch 445 loss: 0.1453324407339096\n",
      "Testing accuracy:  0.799438202247191\n",
      "Testing batch 446 loss: 0.011319288052618504\n",
      "Testing accuracy:  0.7998878923766816\n",
      "Testing batch 447 loss: 0.0020999033004045486\n",
      "Testing accuracy:  0.8003355704697986\n",
      "Testing batch 448 loss: 0.39011454582214355\n",
      "Testing accuracy:  0.7996651785714286\n",
      "Testing batch 449 loss: 0.0011915700742974877\n",
      "Testing accuracy:  0.8001113585746102\n",
      "Testing batch 450 loss: 0.15672460198402405\n",
      "Testing accuracy:  0.7994444444444444\n",
      "Testing batch 451 loss: 0.2687395215034485\n",
      "Testing accuracy:  0.7987804878048781\n",
      "Testing batch 452 loss: 0.050217948853969574\n",
      "Testing accuracy:  0.7986725663716814\n",
      "Testing batch 453 loss: 0.04093591123819351\n",
      "Testing accuracy:  0.7991169977924945\n",
      "Testing batch 454 loss: 0.20828428864479065\n",
      "Testing accuracy:  0.7984581497797357\n",
      "Testing batch 455 loss: 0.010226083919405937\n",
      "Testing accuracy:  0.798901098901099\n",
      "Testing batch 456 loss: 0.12563253939151764\n",
      "Testing accuracy:  0.7987938596491229\n",
      "Testing batch 457 loss: 0.027072114869952202\n",
      "Testing accuracy:  0.7992341356673961\n",
      "Testing batch 458 loss: 0.07871054857969284\n",
      "Testing accuracy:  0.7991266375545851\n",
      "Testing batch 459 loss: 0.11583034694194794\n",
      "Testing accuracy:  0.7990196078431373\n",
      "Testing batch 460 loss: 0.03073626011610031\n",
      "Testing accuracy:  0.7994565217391304\n",
      "Testing batch 461 loss: 0.10060252249240875\n",
      "Testing accuracy:  0.7993492407809111\n",
      "Testing batch 462 loss: 0.34730327129364014\n",
      "Testing accuracy:  0.7992424242424242\n",
      "Testing batch 463 loss: 0.0583801195025444\n",
      "Testing accuracy:  0.7991360691144709\n",
      "Testing batch 464 loss: 0.1840839982032776\n",
      "Testing accuracy:  0.7990301724137931\n",
      "Testing batch 465 loss: 0.08588239550590515\n",
      "Testing accuracy:  0.7989247311827957\n",
      "Testing batch 466 loss: 0.0026124536525458097\n",
      "Testing accuracy:  0.7993562231759657\n",
      "Testing batch 467 loss: 0.033889561891555786\n",
      "Testing accuracy:  0.7997858672376874\n",
      "Testing batch 468 loss: 0.014038216322660446\n",
      "Testing accuracy:  0.8002136752136753\n",
      "Testing batch 469 loss: 0.002646230859681964\n",
      "Testing accuracy:  0.8006396588486141\n",
      "Testing batch 470 loss: 0.24825088679790497\n",
      "Testing accuracy:  0.8\n",
      "Testing batch 471 loss: 0.2237282693386078\n",
      "Testing accuracy:  0.7993630573248408\n",
      "Testing batch 472 loss: 0.18779197335243225\n",
      "Testing accuracy:  0.798728813559322\n",
      "Testing batch 473 loss: 0.07032255828380585\n",
      "Testing accuracy:  0.7986257928118393\n",
      "Testing batch 474 loss: 0.028033370152115822\n",
      "Testing accuracy:  0.7990506329113924\n",
      "Testing batch 475 loss: 0.06430698931217194\n",
      "Testing accuracy:  0.7989473684210526\n",
      "Testing batch 476 loss: 0.01776076853275299\n",
      "Testing accuracy:  0.7993697478991597\n",
      "Testing batch 477 loss: 0.07417256385087967\n",
      "Testing accuracy:  0.799266247379455\n",
      "Testing batch 478 loss: 0.15757127106189728\n",
      "Testing accuracy:  0.7986401673640168\n",
      "Testing batch 479 loss: 0.0046953968703746796\n",
      "Testing accuracy:  0.7990605427974948\n",
      "Testing batch 480 loss: 0.02641456574201584\n",
      "Testing accuracy:  0.7994791666666666\n",
      "Testing batch 481 loss: 0.004782389849424362\n",
      "Testing accuracy:  0.7998960498960499\n",
      "Testing batch 482 loss: 0.2795819640159607\n",
      "Testing accuracy:  0.7992738589211619\n",
      "Testing batch 483 loss: 0.11283940821886063\n",
      "Testing accuracy:  0.7991718426501035\n",
      "Testing batch 484 loss: 0.03180186077952385\n",
      "Testing accuracy:  0.7995867768595041\n",
      "Testing batch 485 loss: 0.22974833846092224\n",
      "Testing accuracy:  0.7989690721649485\n",
      "Testing batch 486 loss: 0.04243895784020424\n",
      "Testing accuracy:  0.7993827160493827\n",
      "Testing batch 487 loss: 0.08794715255498886\n",
      "Testing accuracy:  0.7992813141683778\n",
      "Testing batch 488 loss: 0.013627197593450546\n",
      "Testing accuracy:  0.7996926229508197\n",
      "Testing batch 489 loss: 0.08722401410341263\n",
      "Testing accuracy:  0.7995910020449898\n",
      "Testing batch 490 loss: 0.28761059045791626\n",
      "Testing accuracy:  0.7989795918367347\n",
      "Testing batch 491 loss: 0.12557709217071533\n",
      "Testing accuracy:  0.7988798370672098\n",
      "Testing batch 492 loss: 0.1008872240781784\n",
      "Testing accuracy:  0.7987804878048781\n",
      "Testing batch 493 loss: 0.18640683591365814\n",
      "Testing accuracy:  0.7981744421906694\n",
      "Testing batch 494 loss: 0.1270386129617691\n",
      "Testing accuracy:  0.7980769230769231\n",
      "Testing batch 495 loss: 0.14860303699970245\n",
      "Testing accuracy:  0.797979797979798\n",
      "Testing batch 496 loss: 0.020455215126276016\n",
      "Testing accuracy:  0.7983870967741935\n",
      "Testing batch 497 loss: 0.07486079633235931\n",
      "Testing accuracy:  0.7982897384305835\n",
      "Testing batch 498 loss: 0.0499684140086174\n",
      "Testing accuracy:  0.7986947791164659\n",
      "Testing batch 499 loss: 0.08194214850664139\n",
      "Testing accuracy:  0.7985971943887775\n",
      "Testing batch 500 loss: 0.08580401539802551\n",
      "Testing accuracy:  0.7985\n",
      "Testing batch 501 loss: 0.09037438035011292\n",
      "Testing accuracy:  0.7984031936127745\n",
      "Testing batch 502 loss: 0.022925028577446938\n",
      "Testing accuracy:  0.798804780876494\n",
      "Testing batch 503 loss: 0.09704678505659103\n",
      "Testing accuracy:  0.7987077534791253\n",
      "Testing batch 504 loss: 0.011412013322114944\n",
      "Testing accuracy:  0.7991071428571429\n",
      "Testing batch 505 loss: 0.002023332519456744\n",
      "Testing accuracy:  0.7995049504950495\n",
      "Testing batch 506 loss: 0.011261317878961563\n",
      "Testing accuracy:  0.799901185770751\n",
      "Testing batch 507 loss: 0.006538563407957554\n",
      "Testing accuracy:  0.8002958579881657\n",
      "Testing batch 508 loss: 0.35989657044410706\n",
      "Testing accuracy:  0.7992125984251969\n",
      "Testing batch 509 loss: 0.004913224373012781\n",
      "Testing accuracy:  0.7996070726915521\n",
      "Testing batch 510 loss: 0.31291258335113525\n",
      "Testing accuracy:  0.7985294117647059\n",
      "Testing batch 511 loss: 0.056383393704891205\n",
      "Testing accuracy:  0.7984344422700587\n",
      "Testing batch 512 loss: 0.23060697317123413\n",
      "Testing accuracy:  0.7978515625\n",
      "Testing batch 513 loss: 0.016374530270695686\n",
      "Testing accuracy:  0.7982456140350878\n",
      "Testing batch 514 loss: 0.08961305022239685\n",
      "Testing accuracy:  0.7981517509727627\n",
      "Testing batch 515 loss: 0.010194719769060612\n",
      "Testing accuracy:  0.7985436893203883\n",
      "Testing batch 516 loss: 0.11622161418199539\n",
      "Testing accuracy:  0.7984496124031008\n",
      "Testing batch 517 loss: 0.1364278346300125\n",
      "Testing accuracy:  0.7983558994197292\n",
      "Testing batch 518 loss: 0.08229782432317734\n",
      "Testing accuracy:  0.7982625482625483\n",
      "Testing batch 519 loss: 0.09567110240459442\n",
      "Testing accuracy:  0.7976878612716763\n",
      "Testing batch 520 loss: 0.1489170342683792\n",
      "Testing accuracy:  0.7975961538461539\n",
      "Testing batch 521 loss: 0.08168889582157135\n",
      "Testing accuracy:  0.7975047984644914\n",
      "Testing batch 522 loss: 0.09759753197431564\n",
      "Testing accuracy:  0.7974137931034483\n",
      "Testing batch 523 loss: 0.17047730088233948\n",
      "Testing accuracy:  0.7973231357552581\n",
      "Testing batch 524 loss: 0.07004579156637192\n",
      "Testing accuracy:  0.7972328244274809\n",
      "Testing batch 525 loss: 0.07145469635725021\n",
      "Testing accuracy:  0.7971428571428572\n",
      "Testing batch 526 loss: 0.002365787047892809\n",
      "Testing accuracy:  0.7975285171102662\n",
      "Testing batch 527 loss: 0.06033343821763992\n",
      "Testing accuracy:  0.797438330170778\n",
      "Testing batch 528 loss: 0.00451682647690177\n",
      "Testing accuracy:  0.7978219696969697\n",
      "Testing batch 529 loss: 0.19681836664676666\n",
      "Testing accuracy:  0.7977315689981096\n",
      "Testing batch 530 loss: 0.05188674479722977\n",
      "Testing accuracy:  0.7981132075471699\n",
      "Testing batch 531 loss: 0.020811259746551514\n",
      "Testing accuracy:  0.7984934086629002\n",
      "Testing batch 532 loss: 0.26051637530326843\n",
      "Testing accuracy:  0.7974624060150376\n",
      "Testing batch 533 loss: 0.003345772158354521\n",
      "Testing accuracy:  0.7978424015009381\n",
      "Testing batch 534 loss: 0.019828062504529953\n",
      "Testing accuracy:  0.7982209737827716\n",
      "Testing batch 535 loss: 0.13599690794944763\n",
      "Testing accuracy:  0.7981308411214953\n",
      "Testing batch 536 loss: 0.39013826847076416\n",
      "Testing accuracy:  0.7971082089552238\n",
      "Testing batch 537 loss: 0.1051793172955513\n",
      "Testing accuracy:  0.7970204841713222\n",
      "Testing batch 538 loss: 0.024902042001485825\n",
      "Testing accuracy:  0.7973977695167286\n",
      "Testing batch 539 loss: 0.06942855566740036\n",
      "Testing accuracy:  0.7973098330241187\n",
      "Testing batch 540 loss: 0.012851922772824764\n",
      "Testing accuracy:  0.7976851851851852\n",
      "Testing batch 541 loss: 0.01396919135004282\n",
      "Testing accuracy:  0.7980591497227357\n",
      "Testing batch 542 loss: 0.1604401171207428\n",
      "Testing accuracy:  0.7979704797047971\n",
      "Testing batch 543 loss: 0.01659807190299034\n",
      "Testing accuracy:  0.7983425414364641\n",
      "Testing batch 544 loss: 0.003933795262128115\n",
      "Testing accuracy:  0.7987132352941176\n",
      "Testing batch 545 loss: 0.11080411076545715\n",
      "Testing accuracy:  0.7986238532110091\n",
      "Testing batch 546 loss: 0.14437641203403473\n",
      "Testing accuracy:  0.7985347985347986\n",
      "Testing batch 547 loss: 0.024068141356110573\n",
      "Testing accuracy:  0.7989031078610603\n",
      "Testing batch 548 loss: 0.203815758228302\n",
      "Testing accuracy:  0.7983576642335767\n",
      "Testing batch 549 loss: 0.08791211992502213\n",
      "Testing accuracy:  0.7982695810564663\n",
      "Testing batch 550 loss: 0.4336884319782257\n",
      "Testing accuracy:  0.7977272727272727\n",
      "Testing batch 551 loss: 0.062066953629255295\n",
      "Testing accuracy:  0.7976406533575318\n",
      "Testing batch 552 loss: 0.16043435037136078\n",
      "Testing accuracy:  0.7971014492753623\n",
      "Testing batch 553 loss: 0.10940936207771301\n",
      "Testing accuracy:  0.7970162748643761\n",
      "Testing batch 554 loss: 0.04963018000125885\n",
      "Testing accuracy:  0.7973826714801444\n",
      "Testing batch 555 loss: 0.24017348885536194\n",
      "Testing accuracy:  0.7968468468468468\n",
      "Testing batch 556 loss: 0.09615454077720642\n",
      "Testing accuracy:  0.7967625899280576\n",
      "Testing batch 557 loss: 0.014241263270378113\n",
      "Testing accuracy:  0.7971274685816876\n",
      "Testing batch 558 loss: 0.0244155190885067\n",
      "Testing accuracy:  0.7974910394265233\n",
      "Testing batch 559 loss: 0.0022753276862204075\n",
      "Testing accuracy:  0.7978533094812165\n",
      "Testing batch 560 loss: 0.017159854993224144\n",
      "Testing accuracy:  0.7982142857142858\n",
      "Testing batch 561 loss: 0.08859902620315552\n",
      "Testing accuracy:  0.7981283422459893\n",
      "Testing batch 562 loss: 0.002596917562186718\n",
      "Testing accuracy:  0.7984875444839857\n",
      "Testing batch 563 loss: 0.05038084089756012\n",
      "Testing accuracy:  0.7984014209591475\n",
      "Testing batch 564 loss: 0.011084545403718948\n",
      "Testing accuracy:  0.7987588652482269\n",
      "Testing batch 565 loss: 0.0024050911888480186\n",
      "Testing accuracy:  0.7991150442477876\n",
      "Testing batch 566 loss: 0.16716191172599792\n",
      "Testing accuracy:  0.7990282685512368\n",
      "Testing batch 567 loss: 0.17583216726779938\n",
      "Testing accuracy:  0.7985008818342152\n",
      "Testing batch 568 loss: 0.018076613545417786\n",
      "Testing accuracy:  0.7988556338028169\n",
      "Testing batch 569 loss: 0.2743089497089386\n",
      "Testing accuracy:  0.7983304042179262\n",
      "Testing batch 570 loss: 0.01223398931324482\n",
      "Testing accuracy:  0.7986842105263158\n",
      "Testing batch 571 loss: 0.11577684432268143\n",
      "Testing accuracy:  0.7985989492119089\n",
      "Testing batch 572 loss: 0.06041298806667328\n",
      "Testing accuracy:  0.798951048951049\n",
      "Testing batch 573 loss: 0.021388374269008636\n",
      "Testing accuracy:  0.7993019197207679\n",
      "Testing batch 574 loss: 0.074058897793293\n",
      "Testing accuracy:  0.7992160278745645\n",
      "Testing batch 575 loss: 0.056153856217861176\n",
      "Testing accuracy:  0.7991304347826087\n",
      "Testing batch 576 loss: 0.13895930349826813\n",
      "Testing accuracy:  0.7986111111111112\n",
      "Testing batch 577 loss: 0.2370768040418625\n",
      "Testing accuracy:  0.7980935875216638\n",
      "Testing batch 578 loss: 0.02370172180235386\n",
      "Testing accuracy:  0.7984429065743944\n",
      "Testing batch 579 loss: 0.02765728160738945\n",
      "Testing accuracy:  0.7987910189982729\n",
      "Testing batch 580 loss: 0.0032648048363626003\n",
      "Testing accuracy:  0.7991379310344827\n",
      "Testing batch 581 loss: 0.09549792110919952\n",
      "Testing accuracy:  0.7990533562822719\n",
      "Testing batch 582 loss: 0.09734528511762619\n",
      "Testing accuracy:  0.7989690721649485\n",
      "Testing batch 583 loss: 0.04798072576522827\n",
      "Testing accuracy:  0.798885077186964\n",
      "Testing batch 584 loss: 0.0726449117064476\n",
      "Testing accuracy:  0.7988013698630136\n",
      "Testing batch 585 loss: 0.10368339717388153\n",
      "Testing accuracy:  0.7987179487179488\n",
      "Testing batch 586 loss: 0.16045372188091278\n",
      "Testing accuracy:  0.7986348122866894\n",
      "Testing batch 587 loss: 0.27832549810409546\n",
      "Testing accuracy:  0.7981260647359455\n",
      "Testing batch 588 loss: 0.2159813791513443\n",
      "Testing accuracy:  0.7980442176870748\n",
      "Testing batch 589 loss: 0.21150895953178406\n",
      "Testing accuracy:  0.7975382003395586\n",
      "Testing batch 590 loss: 0.032184261828660965\n",
      "Testing accuracy:  0.7978813559322034\n",
      "Testing batch 591 loss: 0.03483204543590546\n",
      "Testing accuracy:  0.7982233502538071\n",
      "Testing batch 592 loss: 0.02545982226729393\n",
      "Testing accuracy:  0.7985641891891891\n",
      "Testing batch 593 loss: 0.17737291753292084\n",
      "Testing accuracy:  0.7980607082630692\n",
      "Testing batch 594 loss: 0.12482685595750809\n",
      "Testing accuracy:  0.7975589225589226\n",
      "Testing batch 595 loss: 0.012266471050679684\n",
      "Testing accuracy:  0.7978991596638656\n",
      "Testing batch 596 loss: 0.10049890726804733\n",
      "Testing accuracy:  0.7978187919463087\n",
      "Testing batch 597 loss: 0.13751523196697235\n",
      "Testing accuracy:  0.7977386934673367\n",
      "Testing batch 598 loss: 0.008885195478796959\n",
      "Testing accuracy:  0.7980769230769231\n",
      "Testing batch 599 loss: 0.06769505888223648\n",
      "Testing accuracy:  0.7979966611018364\n",
      "Testing batch 600 loss: 0.17139789462089539\n",
      "Testing accuracy:  0.7979166666666667\n",
      "Testing batch 601 loss: 0.04577421769499779\n",
      "Testing accuracy:  0.7982529118136439\n",
      "Testing batch 602 loss: 0.0019087846158072352\n",
      "Testing accuracy:  0.7985880398671097\n",
      "Testing batch 603 loss: 0.18320123851299286\n",
      "Testing accuracy:  0.7980928689883914\n",
      "Testing batch 604 loss: 0.013390950858592987\n",
      "Testing accuracy:  0.7984271523178808\n",
      "Testing batch 605 loss: 0.049627989530563354\n",
      "Testing accuracy:  0.7983471074380165\n",
      "Testing batch 606 loss: 0.06999173760414124\n",
      "Testing accuracy:  0.7986798679867987\n",
      "Testing batch 607 loss: 0.2234390377998352\n",
      "Testing accuracy:  0.7981878088962109\n",
      "Testing batch 608 loss: 0.11211724579334259\n",
      "Testing accuracy:  0.798108552631579\n",
      "Testing batch 609 loss: 0.18691807985305786\n",
      "Testing accuracy:  0.7980295566502463\n",
      "Testing batch 610 loss: 0.29943567514419556\n",
      "Testing accuracy:  0.7975409836065573\n",
      "Testing batch 611 loss: 0.011641565710306168\n",
      "Testing accuracy:  0.7978723404255319\n",
      "Testing batch 612 loss: 0.01705014519393444\n",
      "Testing accuracy:  0.798202614379085\n",
      "Testing batch 613 loss: 0.07536794245243073\n",
      "Testing accuracy:  0.7981239804241436\n",
      "Testing batch 614 loss: 0.014897792600095272\n",
      "Testing accuracy:  0.7984527687296417\n",
      "Testing batch 615 loss: 0.23684754967689514\n",
      "Testing accuracy:  0.7979674796747968\n",
      "Testing batch 616 loss: 0.26023781299591064\n",
      "Testing accuracy:  0.7978896103896104\n",
      "Testing batch 617 loss: 0.12133624404668808\n",
      "Testing accuracy:  0.7978119935170178\n",
      "Testing batch 618 loss: 0.035896457731723785\n",
      "Testing accuracy:  0.7981391585760518\n",
      "Testing batch 619 loss: 0.0016631502658128738\n",
      "Testing accuracy:  0.7984652665589661\n",
      "Testing batch 620 loss: 0.0378510020673275\n",
      "Testing accuracy:  0.7987903225806452\n",
      "Testing batch 621 loss: 0.012289640493690968\n",
      "Testing accuracy:  0.7991143317230274\n",
      "Testing batch 622 loss: 0.05037381872534752\n",
      "Testing accuracy:  0.7994372990353698\n",
      "Testing batch 623 loss: 0.0237134899944067\n",
      "Testing accuracy:  0.7997592295345104\n",
      "Testing batch 624 loss: 0.0745052620768547\n",
      "Testing accuracy:  0.7996794871794872\n",
      "Testing batch 625 loss: 0.031222859397530556\n",
      "Testing accuracy:  0.8\n",
      "Testing batch 626 loss: 0.01023094542324543\n",
      "Testing accuracy:  0.8003194888178914\n",
      "Testing batch 627 loss: 0.01647690124809742\n",
      "Testing accuracy:  0.8006379585326954\n",
      "Testing batch 628 loss: 0.005015854723751545\n",
      "Testing accuracy:  0.8009554140127388\n",
      "Testing batch 629 loss: 0.003784863045439124\n",
      "Testing accuracy:  0.8012718600953895\n",
      "Testing batch 630 loss: 0.13554777204990387\n",
      "Testing accuracy:  0.8007936507936508\n",
      "Testing batch 631 loss: 0.002678521443158388\n",
      "Testing accuracy:  0.8011093502377179\n",
      "Testing batch 632 loss: 0.0560007244348526\n",
      "Testing accuracy:  0.8010284810126582\n",
      "Testing batch 633 loss: 0.1458466798067093\n",
      "Testing accuracy:  0.8009478672985783\n",
      "Testing batch 634 loss: 0.02746177464723587\n",
      "Testing accuracy:  0.8012618296529969\n",
      "Testing batch 635 loss: 0.16803857684135437\n",
      "Testing accuracy:  0.8011811023622047\n",
      "Testing batch 636 loss: 0.1230892762541771\n",
      "Testing accuracy:  0.8011006289308176\n",
      "Testing batch 637 loss: 0.3283064365386963\n",
      "Testing accuracy:  0.8006279434850864\n",
      "Testing batch 638 loss: 0.09475244581699371\n",
      "Testing accuracy:  0.8005485893416928\n",
      "Testing batch 639 loss: 0.026269350200891495\n",
      "Testing accuracy:  0.8008607198748043\n",
      "Testing batch 640 loss: 0.30939003825187683\n",
      "Testing accuracy:  0.800390625\n",
      "Testing batch 641 loss: 0.034090977162122726\n",
      "Testing accuracy:  0.8007020280811232\n",
      "Testing batch 642 loss: 0.09569427371025085\n",
      "Testing accuracy:  0.8006230529595015\n",
      "Testing batch 643 loss: 0.010074898600578308\n",
      "Testing accuracy:  0.8009331259720062\n",
      "Testing batch 644 loss: 0.12304392457008362\n",
      "Testing accuracy:  0.8008540372670807\n",
      "Testing batch 645 loss: 0.05438319221138954\n",
      "Testing accuracy:  0.8007751937984496\n",
      "Testing batch 646 loss: 0.19536778330802917\n",
      "Testing accuracy:  0.7999226006191951\n",
      "Testing batch 647 loss: 0.01917142979800701\n",
      "Testing accuracy:  0.8002318392581144\n",
      "Testing batch 648 loss: 0.014122306369245052\n",
      "Testing accuracy:  0.8005401234567902\n",
      "Testing batch 649 loss: 0.0433097705245018\n",
      "Testing accuracy:  0.8008474576271186\n",
      "Testing batch 650 loss: 0.11265771836042404\n",
      "Testing accuracy:  0.8007692307692308\n",
      "Testing batch 651 loss: 0.09897153079509735\n",
      "Testing accuracy:  0.8006912442396313\n",
      "Testing batch 652 loss: 0.045325033366680145\n",
      "Testing accuracy:  0.8009969325153374\n",
      "Testing batch 653 loss: 0.25753211975097656\n",
      "Testing accuracy:  0.8009188361408882\n",
      "Testing batch 654 loss: 0.006068832706660032\n",
      "Testing accuracy:  0.8012232415902141\n",
      "Testing batch 655 loss: 0.005158836022019386\n",
      "Testing accuracy:  0.8015267175572519\n",
      "Testing batch 656 loss: 0.1706055849790573\n",
      "Testing accuracy:  0.8014481707317073\n",
      "Testing batch 657 loss: 0.1218249574303627\n",
      "Testing accuracy:  0.8013698630136986\n",
      "Testing batch 658 loss: 0.018029551953077316\n",
      "Testing accuracy:  0.8016717325227963\n",
      "Testing batch 659 loss: 0.01628086157143116\n",
      "Testing accuracy:  0.8019726858877086\n",
      "Testing batch 660 loss: 0.016133036464452744\n",
      "Testing accuracy:  0.8022727272727272\n",
      "Testing batch 661 loss: 0.07054983079433441\n",
      "Testing accuracy:  0.8021936459909228\n",
      "Testing batch 662 loss: 0.2991163730621338\n",
      "Testing accuracy:  0.801737160120846\n",
      "Testing batch 663 loss: 0.006869295611977577\n",
      "Testing accuracy:  0.8020361990950227\n",
      "Testing batch 664 loss: 0.18606781959533691\n",
      "Testing accuracy:  0.8019578313253012\n",
      "Testing batch 665 loss: 0.003977298736572266\n",
      "Testing accuracy:  0.8022556390977443\n",
      "Testing batch 666 loss: 0.05641695857048035\n",
      "Testing accuracy:  0.8021771771771772\n",
      "Testing batch 667 loss: 0.015789266675710678\n",
      "Testing accuracy:  0.8024737631184408\n",
      "Testing batch 668 loss: 0.0032675217371433973\n",
      "Testing accuracy:  0.8027694610778443\n",
      "Testing batch 669 loss: 0.15194761753082275\n",
      "Testing accuracy:  0.8026905829596412\n",
      "Testing batch 670 loss: 0.002134326845407486\n",
      "Testing accuracy:  0.8029850746268656\n",
      "Testing batch 671 loss: 0.0019201315008103848\n",
      "Testing accuracy:  0.8032786885245902\n",
      "Testing batch 672 loss: 0.0020806894171983004\n",
      "Testing accuracy:  0.8035714285714286\n",
      "Testing batch 673 loss: 0.05536673218011856\n",
      "Testing accuracy:  0.8034918276374443\n",
      "Testing batch 674 loss: 0.3177449107170105\n",
      "Testing accuracy:  0.8026706231454006\n",
      "Testing batch 675 loss: 0.0835748091340065\n",
      "Testing accuracy:  0.8025925925925926\n",
      "Testing batch 676 loss: 0.12633228302001953\n",
      "Testing accuracy:  0.8021449704142012\n",
      "Testing batch 677 loss: 0.012349576689302921\n",
      "Testing accuracy:  0.802437223042836\n",
      "Testing batch 678 loss: 0.005610693246126175\n",
      "Testing accuracy:  0.8027286135693216\n",
      "Testing batch 679 loss: 0.07787053287029266\n",
      "Testing accuracy:  0.8026509572901326\n",
      "Testing batch 680 loss: 0.15899726748466492\n",
      "Testing accuracy:  0.8025735294117647\n",
      "Testing batch 681 loss: 0.15791530907154083\n",
      "Testing accuracy:  0.802496328928047\n",
      "Testing batch 682 loss: 0.20092958211898804\n",
      "Testing accuracy:  0.8024193548387096\n",
      "Testing batch 683 loss: 0.09552916139364243\n",
      "Testing accuracy:  0.8023426061493412\n",
      "Testing batch 684 loss: 0.06986721605062485\n",
      "Testing accuracy:  0.8022660818713451\n",
      "Testing batch 685 loss: 0.02722606062889099\n",
      "Testing accuracy:  0.8025547445255474\n",
      "Testing batch 686 loss: 0.1414892077445984\n",
      "Testing accuracy:  0.8024781341107872\n",
      "Testing batch 687 loss: 0.06396275013685226\n",
      "Testing accuracy:  0.8024017467248908\n",
      "Testing batch 688 loss: 0.015361426398158073\n",
      "Testing accuracy:  0.8026889534883721\n",
      "Testing batch 689 loss: 0.051976047456264496\n",
      "Testing accuracy:  0.8026124818577649\n",
      "Testing batch 690 loss: 0.06236150115728378\n",
      "Testing accuracy:  0.802536231884058\n",
      "Testing batch 691 loss: 0.017721980810165405\n",
      "Testing accuracy:  0.802821997105644\n",
      "Testing batch 692 loss: 0.18164607882499695\n",
      "Testing accuracy:  0.8023843930635838\n",
      "Testing batch 693 loss: 0.2692382037639618\n",
      "Testing accuracy:  0.801948051948052\n",
      "Testing batch 694 loss: 0.11349896341562271\n",
      "Testing accuracy:  0.8018731988472623\n",
      "Testing batch 695 loss: 0.010840775445103645\n",
      "Testing accuracy:  0.802158273381295\n",
      "Testing batch 696 loss: 0.008430778980255127\n",
      "Testing accuracy:  0.8024425287356322\n",
      "Testing batch 697 loss: 0.09910987317562103\n",
      "Testing accuracy:  0.8023672883787661\n",
      "Testing batch 698 loss: 0.08884508162736893\n",
      "Testing accuracy:  0.8022922636103151\n",
      "Testing batch 699 loss: 0.10366391390562057\n",
      "Testing accuracy:  0.8022174535050072\n",
      "Testing batch 700 loss: 0.08602410554885864\n",
      "Testing accuracy:  0.8021428571428572\n",
      "Testing batch 701 loss: 0.15110020339488983\n",
      "Testing accuracy:  0.8017118402282454\n",
      "Testing batch 702 loss: 0.05290064960718155\n",
      "Testing accuracy:  0.8016381766381766\n",
      "Testing batch 703 loss: 0.15686671435832977\n",
      "Testing accuracy:  0.8012091038406828\n",
      "Testing batch 704 loss: 0.026981273666024208\n",
      "Testing accuracy:  0.8014914772727273\n",
      "Testing batch 705 loss: 0.10935820639133453\n",
      "Testing accuracy:  0.8014184397163121\n",
      "Testing batch 706 loss: 0.30020278692245483\n",
      "Testing accuracy:  0.8009915014164306\n",
      "Testing batch 707 loss: 0.09836475551128387\n",
      "Testing accuracy:  0.8009193776520509\n",
      "Testing batch 708 loss: 0.03542821854352951\n",
      "Testing accuracy:  0.8012005649717514\n",
      "Testing batch 709 loss: 0.11461466550827026\n",
      "Testing accuracy:  0.8011283497884344\n",
      "Testing batch 710 loss: 0.2565135061740875\n",
      "Testing accuracy:  0.8003521126760563\n",
      "Testing batch 711 loss: 0.0016226121224462986\n",
      "Testing accuracy:  0.8006329113924051\n",
      "Testing batch 712 loss: 0.24721835553646088\n",
      "Testing accuracy:  0.800561797752809\n",
      "Testing batch 713 loss: 0.0277237705886364\n",
      "Testing accuracy:  0.8008415147265077\n",
      "Testing batch 714 loss: 0.022277668118476868\n",
      "Testing accuracy:  0.8011204481792717\n",
      "Testing batch 715 loss: 0.06592034548521042\n",
      "Testing accuracy:  0.801048951048951\n",
      "Testing batch 716 loss: 0.01575445383787155\n",
      "Testing accuracy:  0.8013268156424581\n",
      "Testing batch 717 loss: 0.0958280935883522\n",
      "Testing accuracy:  0.801255230125523\n",
      "Testing batch 718 loss: 0.16352418065071106\n",
      "Testing accuracy:  0.8011838440111421\n",
      "Testing batch 719 loss: 0.27491116523742676\n",
      "Testing accuracy:  0.8007649513212796\n",
      "Testing batch 720 loss: 0.00605996698141098\n",
      "Testing accuracy:  0.8010416666666667\n",
      "Testing batch 721 loss: 0.08387061953544617\n",
      "Testing accuracy:  0.8009708737864077\n",
      "Testing batch 722 loss: 0.002086259424686432\n",
      "Testing accuracy:  0.8012465373961218\n",
      "Testing batch 723 loss: 0.005208109971135855\n",
      "Testing accuracy:  0.8015214384508991\n",
      "Testing batch 724 loss: 0.18008333444595337\n",
      "Testing accuracy:  0.8011049723756906\n",
      "Testing batch 725 loss: 0.017170365899801254\n",
      "Testing accuracy:  0.8013793103448276\n",
      "Testing batch 726 loss: 0.010844660922884941\n",
      "Testing accuracy:  0.8016528925619835\n",
      "Testing batch 727 loss: 0.06511911004781723\n",
      "Testing accuracy:  0.8015818431911967\n",
      "Testing batch 728 loss: 0.015778206288814545\n",
      "Testing accuracy:  0.8018543956043956\n",
      "Testing batch 729 loss: 0.07602329552173615\n",
      "Testing accuracy:  0.8017832647462277\n",
      "Testing batch 730 loss: 0.06949524581432343\n",
      "Testing accuracy:  0.802054794520548\n",
      "Testing batch 731 loss: 0.1340959072113037\n",
      "Testing accuracy:  0.801641586867305\n",
      "Testing batch 732 loss: 0.08760471642017365\n",
      "Testing accuracy:  0.8015710382513661\n",
      "Testing batch 733 loss: 0.03893434256315231\n",
      "Testing accuracy:  0.8018417462482946\n",
      "Testing batch 734 loss: 0.006706442683935165\n",
      "Testing accuracy:  0.8021117166212534\n",
      "Testing batch 735 loss: 0.13041913509368896\n",
      "Testing accuracy:  0.8017006802721088\n",
      "Testing batch 736 loss: 0.0637628585100174\n",
      "Testing accuracy:  0.8016304347826086\n",
      "Testing batch 737 loss: 0.09533148258924484\n",
      "Testing accuracy:  0.8015603799185889\n",
      "Testing batch 738 loss: 0.11838719248771667\n",
      "Testing accuracy:  0.801490514905149\n",
      "Testing batch 739 loss: 0.33797505497932434\n",
      "Testing accuracy:  0.8014208389715832\n",
      "Testing batch 740 loss: 0.049461785703897476\n",
      "Testing accuracy:  0.8016891891891892\n",
      "Testing batch 741 loss: 0.02175603061914444\n",
      "Testing accuracy:  0.8019568151147098\n",
      "Testing batch 742 loss: 0.06434831768274307\n",
      "Testing accuracy:  0.8022237196765498\n",
      "Testing batch 743 loss: 0.00218428997322917\n",
      "Testing accuracy:  0.8024899057873486\n",
      "Testing batch 744 loss: 0.16940565407276154\n",
      "Testing accuracy:  0.8020833333333334\n",
      "Testing batch 745 loss: 0.007951086387038231\n",
      "Testing accuracy:  0.8023489932885906\n",
      "Testing batch 746 loss: 0.025700457394123077\n",
      "Testing accuracy:  0.8026139410187667\n",
      "Testing batch 747 loss: 0.0014212673995643854\n",
      "Testing accuracy:  0.8028781793842035\n",
      "Testing batch 748 loss: 0.19324269890785217\n",
      "Testing accuracy:  0.8024732620320856\n",
      "Testing batch 749 loss: 0.03500194847583771\n",
      "Testing accuracy:  0.8027369826435247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [08:44<00:00, 524.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing batch 750 loss: 0.02818208932876587\n",
      "Testing accuracy:  0.803\n",
      "\n",
      "Testing epoch 1 last loss:  0.02818208932876587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(num_epochs)):\n",
    "    print(\"Epoch: \",(epoch + 1))\n",
    "    # TRAINING BLOCK STARTS\n",
    "    bert_model.train()\n",
    "    for i,batch in enumerate(train_loader):    \n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Setting the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Passing the data to the model\n",
    "        outputs = bert_model(input_ids = batch['input_ids'], attention_mask = batch['attention_mask'])\n",
    "        \n",
    "        # The logits will be used for measuring the loss\n",
    "        pred = outputs.logits\n",
    "        loss = loss_fn(pred, batch['labels'])\n",
    "\n",
    "        # Calculating the gradient for the loss function\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimizing the parameters of the bert model\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculating the running loss for logging purposes\n",
    "        train_batch_loss = loss.item()\n",
    "        train_last_loss = train_batch_loss / batch_size\n",
    "\n",
    "        print('Training batch {} last loss: {}'.format(i + 1, train_last_loss))\n",
    "    # Logging epoch-wise training loss\n",
    "    print(f\"\\nTraining epoch {epoch + 1} loss: \",train_last_loss)\n",
    "    # TRAINING BLOCK ENDS \n",
    "\n",
    "    # TESTING BLOCK STARTS\n",
    "    bert_model.eval()\n",
    "    correct = 0\n",
    "    test_pred = []\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # We don't need gradients for testing\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(input_ids = batch['input_ids'], attention_mask = batch['attention_mask'])\n",
    "        \n",
    "        # Logits act as predictions\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Calculating total batch loss using the logits and labels\n",
    "        loss = loss_fn(logits, batch['labels'])\n",
    "        test_batch_loss = loss.item()\n",
    "        \n",
    "        # Calculating the mean batch loss\n",
    "        test_last_loss = test_batch_loss / batch_size\n",
    "        print('Testing batch {} loss: {}'.format(i + 1, test_last_loss))\n",
    "        \n",
    "        # Comparing the predicted target with the labels in the batch\n",
    "        correct += (logits.argmax(1) == batch['labels']).sum().item()\n",
    "        print(\"Testing accuracy: \",correct/((i + 1) * batch_size))\n",
    "    \n",
    "    print(f\"\\nTesting epoch {epoch + 1} last loss: \",test_last_loss)\n",
    "    # TESTING BLOCK ENDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d833960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bert_model.state_dict(), 'bert.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
