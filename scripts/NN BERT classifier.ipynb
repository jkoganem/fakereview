{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77449ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: transformers in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (4.29.2)\n",
      "Requirement already satisfied: filelock in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: pandas in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/salilsingh/anaconda3/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install pandas\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5ca90f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f25a6a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Model</th>\n",
       "      <th>Original dataset</th>\n",
       "      <th>Row in original dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>While driverless cars present many promising b...</td>\n",
       "      <td>Machine</td>\n",
       "      <td>darragh_claude_v7</td>\n",
       "      <td>essays</td>\n",
       "      <td>13355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Homework Clubs: The Key to Unlocking Academic ...</td>\n",
       "      <td>Machine</td>\n",
       "      <td>llama2_chat</td>\n",
       "      <td>essays</td>\n",
       "      <td>7249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"The legalization of marijuana has been a cont...</td>\n",
       "      <td>Machine</td>\n",
       "      <td>falcon_180b_v1</td>\n",
       "      <td>essays</td>\n",
       "      <td>2603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Taking the opportunity to learn new things can...</td>\n",
       "      <td>Machine</td>\n",
       "      <td>mistral7binstruct_v1</td>\n",
       "      <td>essays</td>\n",
       "      <td>3993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Working with a partner is an effective way fo...</td>\n",
       "      <td>Machine</td>\n",
       "      <td>mistral7binstruct_v2</td>\n",
       "      <td>essays</td>\n",
       "      <td>3773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text    Label  \\\n",
       "0  While driverless cars present many promising b...  Machine   \n",
       "1  Homework Clubs: The Key to Unlocking Academic ...  Machine   \n",
       "2  \"The legalization of marijuana has been a cont...  Machine   \n",
       "3  Taking the opportunity to learn new things can...  Machine   \n",
       "4   Working with a partner is an effective way fo...  Machine   \n",
       "\n",
       "                  Model Original dataset  Row in original dataset  \n",
       "0     darragh_claude_v7           essays                    13355  \n",
       "1           llama2_chat           essays                     7249  \n",
       "2        falcon_180b_v1           essays                     2603  \n",
       "3  mistral7binstruct_v1           essays                     3993  \n",
       "4  mistral7binstruct_v2           essays                     3773  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./combined_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c277f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Label'] = data['Label'].apply(lambda x: 0 if x == 'Human' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d8883d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = data.sample(n=200, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cffa203e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X shape: (140,)\n",
      "Test X shape: (60,)\n",
      "Train Y shape: (140,)\n",
      "Test Y shape: (60,)\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(\n",
    "    sampled_data['Text'], \n",
    "    sampled_data['Label'], \n",
    "    train_size=0.7, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Print the shapes to verify\n",
    "print(f\"Train X shape: {train_X.shape}\")\n",
    "print(f\"Test X shape: {test_X.shape}\")\n",
    "print(f\"Train Y shape: {train_Y.shape}\")\n",
    "print(f\"Test Y shape: {test_Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e39d4577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64fa63116c92438d9ea5f43e30d286fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d015ac6dd044daa1fd99e84beccbf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0703e3afd174593b0af4f5e647b8b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3bac408084c474c81ea6ebbd0ed9c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7278afbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = tokenizer(list(train_X), padding = True, truncation=True)\n",
    "test_tokens = tokenizer(list(test_X), padding = True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afec4909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fa57178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 6510, 1431, 1129, 1682, 1106, 1902, 1147, 1319, 2247, 1933, 1272, 117, 1122, 1660, 1172, 170, 2640, 1106, 1138, 1380, 1106, 1440, 1977, 1106, 1107, 1103, 2247, 119, 2907, 1122, 1156, 1494, 1172, 1243, 170, 2640, 1120, 13992, 1147, 1319, 17980, 119, 6510, 1156, 1129, 1682, 1106, 1138, 1115, 2296, 1115, 1152, 1132, 1682, 1106, 1321, 1113, 1103, 4812, 119, 1220, 1156, 1129, 1682, 1106, 3858, 1167, 1191, 1152, 1169, 2011, 1122, 1155, 1113, 1147, 1319, 119, 1370, 1538, 1651, 1152, 1274, 1204, 1541, 1440, 1977, 1106, 1833, 1933, 1133, 1191, 1128, 1660, 1172, 170, 2640, 1106, 1202, 1122, 1113, 1147, 1319, 1152, 1547, 1243, 1154, 1122, 119, 1752, 117, 13795, 1240, 1319, 1933, 1112, 170, 2377, 1122, 1660, 1128, 1380, 1106, 1440, 1977, 1106, 1107, 1103, 2247, 119, 2082, 1651, 2215, 1313, 1111, 1103, 2247, 1114, 1720, 1106, 1202, 119, 1573, 117, 1191, 1128, 1660, 170, 2377, 170, 2640, 1106, 1902, 1147, 1319, 1933, 1111, 1103, 2247, 1122, 1660, 1172, 170, 2640, 1106, 1202, 1380, 119, 6510, 1156, 1129, 1682, 1106, 8664, 1167, 1191, 1152, 1902, 1147, 1319, 1933, 119, 1556, 12138, 1147, 1136, 2807, 1120, 1313, 5497, 170, 3821, 1104, 13228, 2903, 22989, 1152, 1132, 2140, 1217, 2327, 1105, 2232, 1213, 119, 1966, 117, 1211, 4952, 6616, 1122, 1110, 170, 2213, 1911, 1111, 1651, 1106, 1902, 1152, 1319, 1933, 1272, 1152, 2059, 1152, 1209, 1136, 1321, 1122, 5536, 119, 1135, 2140, 1103, 3714, 1544, 1104, 1251, 2126, 1107, 1103, 1362, 1336, 1138, 1651, 1150, 1110, 1154, 1893, 1137, 1154, 13795, 1614, 119, 1135, 1156, 1660, 1651, 170, 2640, 1106, 6848, 1147, 1319, 17980, 119, 2082, 1651, 1567, 1103, 1354, 1104, 1543, 1147, 1319, 3203, 119, 1220, 1156, 3858, 1167, 1105, 7290, 1167, 1114, 1543, 1147, 1319, 1933, 1105, 13795, 1122, 1103, 1236, 1115, 1152, 9353, 1122, 1106, 1129, 119, 2907, 117, 5074, 1651, 1902, 1147, 1281, 1933, 1156, 1519, 1172, 7290, 1167, 119, 1556, 15137, 1152, 1132, 1682, 1106, 3858, 1167, 1105, 1267, 1645, 1115, 1152, 1309, 1562, 119, 16409, 26671, 1474, 1240, 1107, 1132, 1105, 1128, 1893, 3218, 1660, 1128, 170, 2247, 1933, 119, 1262, 1115, 1933, 1110, 1106, 1321, 170, 185, 1596, 1104, 1184, 1128, 1341, 18028, 1234, 1107, 1103, 1362, 119, 1192, 1156, 1138, 1106, 1243, 1149, 1240, 7238, 4834, 1106, 1202, 1115, 1176, 1280, 1796, 119, 1122, 1156, 1145, 1129, 170, 1618, 1236, 1106, 10621, 1114, 1234, 1106, 4361, 1869, 1111, 1240, 1933, 119, 7282, 117, 4952, 1156, 1328, 1111, 1147, 1651, 1106, 1321, 1113, 1115, 4812, 1106, 5424, 1115, 1152, 1132, 2407, 1106, 1250, 1111, 1147, 3654, 119, 1135, 1156, 1145, 1660, 4067, 1103, 2296, 1115, 1152, 1132, 4451, 1104, 1781, 1113, 1103, 4812, 1104, 15975, 1106, 1175, 3218, 1397, 1214, 1115, 1152, 1132, 2784, 119, 9904, 1138, 170, 1662, 1159, 4000, 1115, 1152, 1169, 1202, 4333, 1107, 170, 2784, 4758, 119, 1573, 117, 2368, 4067, 1115, 4812, 1104, 13795, 1147, 1933, 1122, 112, 188, 5074, 1172, 1221, 1115, 1122, 1110, 1147, 2261, 1106, 1294, 1612, 1152, 1138, 1917, 1152, 1444, 1111, 1103, 1933, 1105, 3650, 1115, 1152, 1132, 4451, 1104, 1833, 1115, 102]\n",
      "[CLS] Students should be able to design their own summer project because, it give them a chance to have something to look forward to in the summer. Also it would help them get a chance at experiencing their own creativity. Students would be able to have that feeling that they are able to take on the responsibility. They would be able to learn more if they can designed it all on their own. For must students they dont really look forward to doing project but if you give them a chance to do it on their own they might get into it. First, designing your own project as a student it give you something to look forward to in the summer. Most students stay home for the summer with nothing to do. So, if you give a student a chance to design their own project for the summer it give them a chance to do something. Students would be able to explore more if they design their own project. With exploring their not sitting at home eating a bag of chips watching Netflix they are actually being active and moving around. Although, most teachers believes it is a bad idea for students to design they own project because they believe they will not take it seriously. It actually the opposite half of any schools in the world may have students who is into art or into designing things. It would give students a chance to express their own creativity. Most students love the thought of making their own projects. They would learn more and discover more with making their own project and designing it the way that they prefer it to be. Also, letting students design their won project would let them discover more. With discovering they are able to learn more and see thing that they never seen. Example say your in are and you art teacher give you a summer project. And that project is to take a pic of what you think expresses people in the world. You would have to get out your comfort zone to do that like going outside. it would also be a better way to communicate with people to gain information for your project. Furthermore, teachers would want for their students to take on that responsibility to prove that they are ready to work for their grade. It would also give kids the feeling that they are capable of taking on the responsibility of proving to there teacher next year that they are responsible. Kids have a hard time showing that they can do stuff in a responsible manner. So, giving kids that responsibility of designing their project it's letting them know that it is their job to make sure they have everything they need for the project and knowing that they are capable of doing that [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(train_tokens['input_ids'][0])\n",
    "print(tokenizer.decode(train_tokens['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b80a80f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenData(Dataset):\n",
    "    def __init__(self, train = False):\n",
    "        if train:\n",
    "            self.text_data = train_X\n",
    "            self.tokens = train_tokens\n",
    "            self.labels = list(train_Y)\n",
    "        else:\n",
    "            self.text_data = test_X\n",
    "            self.tokens = test_tokens\n",
    "            self.labels = list(test_Y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        for k, v in self.tokens.items():\n",
    "            sample[k] = torch.tensor(v[idx])\n",
    "        sample['labels'] = torch.tensor(self.labels[idx])\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab6ba20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "train_dataset = TokenData(train = True)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_dataset = TokenData(train = False)\n",
    "test_loader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36f72ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c666b075cb74dacb442734476964cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-cased') # Pre-trained model\n",
    "optimizer = AdamW(bert_model.parameters(), lr=1e-5) # Optimization function\n",
    "loss_fn = torch.nn.CrossEntropyLoss() # Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e2f2256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "bert_model.to(device) # Transfer model to GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e823c785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch: \",(epoch + 1))\n",
    "    # TRAINING BLOCK STARTS\n",
    "    bert_model.train()\n",
    "    for i,batch in enumerate(train_loader):    \n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Setting the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Passing the data to the model\n",
    "        outputs = bert_model(input_ids = batch['input_ids'], attention_mask = batch['attention_mask'])\n",
    "        \n",
    "        # The logits will be used for measuring the loss\n",
    "        pred = outputs.logits\n",
    "        loss = loss_fn(pred, batch['labels'])\n",
    "\n",
    "        # Calculating the gradient for the loss function\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimizing the parameters of the bert model\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculating the running loss for logging purposes\n",
    "        train_batch_loss = loss.item()\n",
    "        train_last_loss = train_batch_loss / batch_size\n",
    "\n",
    "        print('Training batch {} last loss: {}'.format(i + 1, train_last_loss))\n",
    "    # Logging epoch-wise training loss\n",
    "    print(f\"\\nTraining epoch {epoch + 1} loss: \",train_last_loss)\n",
    "    # TRAINING BLOCK ENDS \n",
    "\n",
    "    # TESTING BLOCK STARTS\n",
    "    bert_model.eval()\n",
    "    correct = 0\n",
    "    test_pred = []\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # We don't need gradients for testing\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(input_ids = batch['input_ids'], attention_mask = batch['attention_mask'])\n",
    "        \n",
    "        # Logits act as predictions\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Calculating total batch loss using the logits and labels\n",
    "        loss = loss_fn(logits, batch['labels'])\n",
    "        test_batch_loss = loss.item()\n",
    "        \n",
    "        # Calculating the mean batch loss\n",
    "        test_last_loss = test_batch_loss / batch_size\n",
    "        print('Testing batch {} loss: {}'.format(i + 1, test_last_loss))\n",
    "        \n",
    "        # Comparing the predicted target with the labels in the batch\n",
    "        correct += (logits.argmax(1) == batch['labels']).sum().item()\n",
    "        print(\"Testing accuracy: \",correct/((i + 1) * batch_size))\n",
    "    \n",
    "    print(f\"\\nTesting epoch {epoch + 1} last loss: \",test_last_loss)\n",
    "    # TESTING BLOCK ENDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d833960a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
