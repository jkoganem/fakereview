# Detecting AI-generated text with deep learning

## Authors
[Amzi Jeffs](https://github.com/AmziJeffs)    
[Junichi Koganemaru](https://github.com/jkoganem)  
[Salil Singh](https://github.com/sllsnghlrns)  
[Ashwin Tarikere](https://github.com/ashwintan1)     


## Overview

With the explosion of LLMs and NLP methods, AI-generated text has become ubiquitous on the internet. This presents challenges across many contexts, ranging from plagiarism in the academic setting, to misinformation on social media and its consequences in electoral politics. With this in mind, we explore a range of classical statistical learning classifiers as well as deep learning based transformers to detect AI-generated text. 

## Structure of repository

- `raw data` contains a readme citing our data sources, and our exploratory data analysis notebooks. It does not contain the raw data itself, which was too large to add to the repository. Instead, the `processing.ipynb` notebook can be used to reconstruct our dataset from the source datasets that we link.
- `code` contains our Python scripts for XGBoost and fine-tuning, which are intended to run on a powerful computing cluster.
- `generalization experiments` contains experimental results quantifying how well our deep learning models generalize from one context to another. 

## Our dataset

We compiled 40000 human-generated and 40000 AI-generated text snippets sourced from various contexts and models. 

- Product Reviews generated by GPT-2 from a [Kaggle competition](https://www.kaggle.com/datasets/mexwell/fake-reviews-dataset
).
- ~~Essays generated by a range of models, also from a [Kaggle competition](https://www.kaggle.com/datasets/thedrcat/daigt-v2-train-dataset
).~~ Removed, see discussion below.
- News articles generated by [Grover](https://github.com/rowanz/grover/tree/master
).
- Wikipedia intros generated by GPT-3 Curie, from a Hugging Face [repo](https://huggingface.co/datasets/aadityaubhat/GPT-wiki-intro
). 
 
## Analysis
Our baseline approach was to use an SBERT model to embed our text samples in a high-dimensional vector space, then use classical techniques such as SVM and XGBoost. Our baselines gave unrealistically high performance on the essays dataset, and upon further inspection we discovered that this data set was generated in a narrow manner that led to artificial clustering in the SBERT embedding. Specifically, the essays data reused a small set of prompts when generating essays, causing repetitive outputs from the models that effectively created data leakage; see `raw data/embedding_eda.ipynb`). As such, we remove the essay data from our experiments, and restricted our analysis to the remaining three contexts.

To get the most out of our baseline, we extensively optimized the hyperparameters for XGBoost using Optuna, since XGBoost had performed the best among our baselines (`code/xgbboost_optimize.py`). XGBoost was highly sensitive to differences in the hyperparameters, with results ranging from 50% accuracy to 75% accuracy.

To move beyond the out-of-the-box SBERT, we attached a linear classification head and performed fine-tuning (`code/sbert_crossencoder_mpnet_v2.py`). With the SBERT model frozen, the linear classifier only achieved 69\% accuracy, but when fine-tuning the entire model we achieved 88\% validation accuracy after 20 epochs. Even better, the fine-tuned model achieved 92\% accuracy on our final holdout set. Our work confirms that fine-tuning existing language models is a valuable approach for detecting AI-generated text content. 

## Generalization experiments
A natural followup question arises from these results—is our model learning enough that it could be applied in truly novel contexts, or is it narrowly learning our specific dataset? To assess the extent to which our approach could generalize to new contexts, we ran a series of experiments with 10 epochs of training in restricted contexts, but validation accuracies computed in every context. Importantly, we found that 
 * training in one context *could* lead to higher performance in different ones, and
 * training in one context does not appear to harm performance in unseen contexts.

The table below has one row for each of seven experiments, corresponding to the nonempty subsets of our three contexts: Grover, Wikipedia intros, and reviews. For each experiment, we fine-tuned an `all-mpnet-base-v2` classifier for 10 epochs on the corresponding contexts, and computed final test accuracies on a held out set consisting of 20% of the data. 

| Training contexts <br> (10 epochs) | Grover accuracy | Wiki accuracy | Reviews accuracy |
|-------------------|-----------------|---------------|------------------|
| G | 0.6675	| 0.5245 |	0.5215 |
| W | 0.50475 | 0.977 | 0.5775 |
| R | 0.50125 |	0.54225 |	0.92525 |
| GW | 0.587 |	0.9645 |	0.5345 |
| GR | 0.6065 |	0.60725 |	0.85875 |
| WR | 0.5005 |	0.96625 |	0.8725 |
| GWR | 0.5705 |0.94325 |	0.78125 |

The results show stark differences in performance on each dataset. Grover is clearly the "hardest" to learn, and sees almost no improvement from training on different contexts. This perhaps indicates that this is the most robust dataset out of the three. On the other hand, the Wiki and review data see roughly similar benefits from training on different contexts; it is perhaps not surprising to see this correlation given that these datasets were generated with GPT-3 and GPT-2 respectively. 

## Acknowledgements 
We would like to thank the Erdös institute for providing the authors the opportunity to work on this project as part of the Erdös institute Deep Learning Bootcamp. We would also like to thank Nuno Chagas and the Department of Mathematical Sciences at Carnegie Mellon University for providing computing support for the project. 



