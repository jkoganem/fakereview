# Detecting AI reviews using deep learning architectures

## Authors
[Amzi Jeffs](https://github.com/AmziJeffs)    
[Junichi Koganemaru](https://github.com/jkoganem)  
[Salil Singh](https://github.com/sllsnghlrns)  
[Ashwin Tarikere](https://github.com/ashwintan1)     


## Overview

With the explosion of LLMs and NLP methods, AI-generated text has become ubiquitous on the internet. This presents several challenges across many contexts, ranging from plagiarism in the academic setting to misinformation on social media and its consequences in electoral politics. With this in mind, we explore a range of classical statistical learning classifiers as well as deep learning based transformers to detect AI-generated text. 

## Structure of repository

- `all scripts` folder contains scripts for all the architectures and preprocessing steps that we have been considering.  
- `data` folder contains the training metadata. .py scripts in the scripts folders can be ran locally after populating the `data` folder with the official training audio data. 
- `sample scripts` folder contains sample scripts showcasing our work. Included are 
    - [EDAbaseline.ipynb](/sample%20scripts/EDAbaseline.ipynb) for exploratory data analysis and a simple training pipeline.
    - [baseline.ipynb](/sample%20scripts/baseline.ipynb) and [baseline.py](/sample%20scripts/baseline.py) for a two-layer baseline model with minimal preprocessing. 
    - [6layerNN.py](/sample%20scripts/) for a six-layer CNN model with improved preprocessing.
    - [convnext.py](/sample%20scripts/convnext.py) and [resnet.py](/sample%20scripts/resnet.py) for transfer learning with ResNet and ConvNeXT. 

#### Remark
The scripts are written to be ran on a remote cluster with considerable computing power. As written, the training data is loaded into memory to speed up runtime performance, which requires significant RAM usage. Simple modifications can be made for the scripts to be utilized locally. 



## Our Dataset: 

Consisted of 10000 human-generated and 10000 AI-generated text snippets sourced from various contexts and models. 

- Product Reviews generated by GPT-2 from a [Kaggle competition](https://www.kaggle.com/datasets/mexwell/fake-reviews-dataset
).
- Essays generated by a range of models, also from a [Kaggle competition](https://www.kaggle.com/datasets/thedrcat/daigt-v2-train-dataset
).
- News articles generated by [Grover](https://github.com/rowanz/grover/tree/master
).
- Wikipedia intros generated by GPT-3 Curie, from a Hugging Face [repo](https://huggingface.co/datasets/aadityaubhat/GPT-wiki-intro
). 

 
## Evaluation metric



## Acknowledgements 

We would like to thank the Erdös institute for providing the authors the opportunity to work on this project as part of the Erdös institute Deep Learning Bootcamp. 

We would also like to thank Nuno Chagas and the Department of Mathematical Sciences at Carnegie Mellon University for providing computing support for the project. 



