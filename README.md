# Detecting AI reviews using deep learning architectures

## Authors
[Amzi Jeffs](https://github.com/AmziJeffs)    
[Junichi Koganemaru](https://github.com/jkoganem)  
[Salil Singh](https://github.com/sllsnghlrns)  
[Ashwin Tarikere](https://github.com/ashwintan1)     


## Overview

With the explosion of LLMs and NLP methods, AI-generated text has become ubiquitous on the internet. This presents several challenges across many contexts, ranging from plagiarism in the academic setting to misinformation on social media and its consequences in electoral politics. With this in mind, we explore a range of classical statistical learning classifiers as well as deep learning based transformers to detect AI-generated text. 

## Structure of repository

- `raw data` contains a readme citing our data sources, and our exploratory data analysis notebooks. It does not contain the raw data itself, which was too large to add to the repository. Instead, the `processing.ipynb` notebook can be used to reconstruct our dataset from the source datasets that we link.
- `code` contains our Python scripts for XGBoost and fine-tuning, which are intended to run on a powerful computing cluster.

## Our Dataset

We compiled 40000 human-generated and 40000 AI-generated text snippets sourced from various contexts and models. 

- Product Reviews generated by GPT-2 from a [Kaggle competition](https://www.kaggle.com/datasets/mexwell/fake-reviews-dataset
).
- ~~Essays generated by a range of models, also from a [Kaggle competition](https://www.kaggle.com/datasets/thedrcat/daigt-v2-train-dataset
).~~ Removed, see discussion below.
- News articles generated by [Grover](https://github.com/rowanz/grover/tree/master
).
- Wikipedia intros generated by GPT-3 Curie, from a Hugging Face [repo](https://huggingface.co/datasets/aadityaubhat/GPT-wiki-intro
). 

 
## Analysis
Our baseline approach was to use an SBERT model to embed our text samples in a high-dimensional vector space, then use classical techniques such as SVM and XGBoost. Our baselines gave unrealistically high performance on the essays dataset, and we found that this data set was generated in a narrow manner that led to artificial clustering in the SBERT embedding. Specifically, the essays data reused a small set of prompts when generating essays, causing repetitive outputs from the models that effectively created data leakage. As such, we did not use the essay data, and restricted our analysis to the remaining samples.

We extensively optimized the hyperparameters for XGBoost using Optuna, since XGBoost had performed the best among our baselines (`xgbboost_optimize.py`). XGBoost was highly sensitive to differences in the hyperparameters, with results ranging from 50% accuracy to 75% accuracy.

To move beyond the out-of-the-box SBERT, we attached a linear classification head and performed fine-tuning (`sbert_crossencoder_mpnet_v2.py`). With the SBERT model frozen, the linear classifier only achieved 69% accuracy, but when fine-tuning the entire model we achieved 88% validation accuracy after 20 epochs. Even better, the fine-tuned model achieved 92\% accuracy on our final holdout set. Our work confirms that fine-tuning existing language models is a valuable approach for detecting AI-generated text content. 


## Acknowledgements 
We would like to thank the Erdös institute for providing the authors the opportunity to work on this project as part of the Erdös institute Deep Learning Bootcamp. We would also like to thank Nuno Chagas and the Department of Mathematical Sciences at Carnegie Mellon University for providing computing support for the project. 



